<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
<title>gopher://hoi.st/1/posts</title>
<link>gopher://hoi.st/1/posts</link>

<item>
<link>gopher://hoi.st/1/posts/2024-01-22-lets-play-some-cards-in-awk.txt</link>
<title>(2024-01-22) Let's play some cards... in POSIX AWK</title>
<description><![CDATA[<pre>
(2024-01-22) Let&#x27;s play some cards... in POSIX AWK
--------------------------------------------------
Oh yeah. Some windozed morons kept saying something like &quot;Linux has no games&quot;
for a long time, and it was never true. But what about &quot;BusyBox has no 
games&quot;? Technically, that is true: if the system solely has a single BusyBox 
binary in addition to the kernel, there are no games built into it. However, 
it does have an AWK interpreter (unless specifically built without it), and 
that is just enough to game on. Yes, some time ago I did create VTL-2, 
CHIP-8, Subleq16 and TinyChoice engine ports in AWK that could run in the 
BusyBox version as well, but what about &quot;native&quot; AWK-based games? Are there 
any at all?

Well, it turned out that if we stay on the standard side of things and
exclude all gawk-specific shenanigans like Awkventure or Awkaster, then we 
cannot find anything particularly interesting. Of course we could write some 
interactive fiction (again, say hello to TinyChoice), text-based sims like 
Lunar Lander or challenge-response games like Bulls and Cows pretty easily, 
but all that stuff is something quite trivial to implement in any 
programming language and can get boring pretty quickly. And I totally did 
understand why that&#x27;s the case: AWK isn&#x27;t really suited for anything other 
than line-based I/O. But I really wanted to influence the situation. At this 
point, I basically had three choices:

1) write my own roguelike game being a simpler and standard-compliant
alternative to Awkventure,
2) write a port of Text Elite by Ian Bell ([1]) in addition to already
existing JS, Python, Go and even Erlang ports,
3) write something no one had thought of porting to such a language before.

Recently, I also have rediscovered the FreeCell solitaire game and started
learning its history and quirks, so that kinda hugely helped me to lean 
towards the choice number 3. Maybe, I&#x27;ll do a Text Elite port a bit later, 
who knows.

So, enter FreeCell. Despite being first popularized by M$, it still is an
awesome game by itself, and, in order to fully appreciate its awesomeness, I 
recommend you to read a long but comprehensive article at Solitaire 
Laboratory ([2]). To me, the most appealing part in comparison to most other 
patience/solitaire card games is randomness has almost no influence on the 
outcome and almost all deals are solvable (I think that eight impossible 
deals out of a million is a pretty good indication). How easily each of them 
is solvable is another story. Interestingly enough, there still is no 
mathematically proven method to detect whether a FreeCell deal is solvable 
or not, all the stats were found by human players and computer bruteforce 
simulations.

In case you still don&#x27;t know what this game is about, I could direct you to a
tutorial on that Solitaire Laboratory website, but let&#x27;s recap the basic 
FreeCell rules right here.

1. The game starts with four empty freecells (traditionally in the top left)
and four empty homecells aka foundations (traditionally in the top right). 
Below (or in between) them, all 52 cards (if your deck has jokers, they are 
not used) are dealt in 8 columns (aka tableaus) in the random order. The 
cards are dealt horizontally though in rows that contain 8 cards each. So, 
each of the first four columns will contain 7 cards and each of the next 
four columns will contain 6 cards.

2. The last card in the column (or an existing card in any freecell) can be
moved to a freecell. Only one card can be put into a freecell at a time. If 
the freecell is occupied by another card, you cannot put a new card there.

3. The last card from a column or a card from a freecell can be moved onto
the homecell/foundation using the following rule: aces go to any empty 
foundations, deuces go onto the aces of the same suit, threes go onto the 
deuces of the same suit, and so on up to the kings. Once in a homecell, the 
card cannot be moved anywhere else anymore.

4. The last card from a column or a card from a freecell can be moved onto
the end of another column if and only if the current last card in that 
column is a rank higher and of the opposite color. E.g. a ten of spades can 
be appended to the column that ends with a jack of hearts or diamonds, but 
not to the one that ends with a jack of clubs or spades. If the column is 
empty, any card can be moved onto it from a freecell or from the end of 
another column.

5. Technically, only one card can be moved across columns. However, almost
all FreeCell implementations account for the amount of freecells and empty 
columns to allow so-called supermoves. So, if you have a column ending with 
a valid stack of cards (alternating colors and decreasing ranks), the 
maximum amount of cards in the stack you&#x27;re allowed to move at once is 
determined by the formula (2^M) * (N + 1), where M is the amount of empty 
columns and N is the amount of empty freecells.

6. The goal of the game is to move all 52 cards to the homecells/foundations
using the above rules.

Sounds simple enough, right? Well, I thought so too. And started coding it up
in AWK. And quickly realized it wasn&#x27;t as easy as I thought.

The first challenge arose right away: how to represent the playfield and how
to render it. For the columns that have variable lengths, we cannot have a 
&quot;normal&quot; array, and the POSIX standard AWK doesn&#x27;t have true 
multidimensional arrays either, only their associative emulation. Well, 
that&#x27;s what I had to use anyway. For column rendering, I still had to assume 
the maximum amount of cards that a column could theoretically contain: 
imagine e.g. an untouched column 1 that ends with a king — that&#x27;s 7 cards, 
and then we can stack everything from queen to ace, but in practice an ace 
would never end up there — that&#x27;s 11 more. So, maximum column capacity is 18 
cards, this is our virtual second array dimension. In fact, my rendering 
method &quot;draws&quot; 20 rows into a string but then strips all trailing whitespace 
from the result. Due to the specifics of AWK&#x27;s &quot;multidimensionality&quot;, I also 
had to keep track of the column lengths in a separate array and update it 
accordingly on every move to keep things efficient.

Another challenge was a part of the mechanics that never got into any
official FreeCell ruleset but all existing implementations employ it: 
autohome aka autoplay. The simplest variant of it means automatically 
putting cards into the homecells/foundations as soon as there is a 
possibility to do so. Unfortunately, this means that one cannot reuse 
lower-ranked cards to give place to some even lower-ranked cards if there&#x27;s 
a need, and that rules out some deals that can only be solved with this. So, 
I settled on implementing what&#x27;s called &quot;safe autoplay&quot; and being used in M$ 
FreeCell, FreeCell Pro and, I believe, Aisleriot I play on my Arch as well. 
Quoting from the Solitaire Laboratory: &quot;Microsoft FreeCell or FreeCell Pro 
only plays an available card to its homecell automatically when all of the 
lower-ranked cards of the opposite color are already on the homecells 
(except that a two is played if the corresponding ace is on its homecell); 
aces are always played when available.&quot;

On top of that, what most implementations use but no one mentions is the
emulation of M$ FreeCell deal PRNG algorithm. It&#x27;s a simple LCG modulo 2^31, 
nothing fancy. But what also matters is the initial deck layout before 
running this algorithm. This is the reason I had to change the suit order to 
clubs-diamonds-hearts-spades instead of something more convenient for 
internal representation and color detection. Why is all this important and 
we cannot just use a standard AWK&#x27;s rand()? Well, we can but then we 
wouldn&#x27;t be able to reproduce some famous deals that are known by their 
seeds (&quot;game numbers&quot;) on teh interwebz and beyond. This is where I agreed 
to make the code more complex for the benefit of being able to unify the 
deal numbering.

The biggest challenge though, and that&#x27;s also related to unification and
text-based UI simplification, was to implement correct column-to-column 
stack movement that would allow to get rid of explicit stack length 
specification in the second parameter and also to support the &quot;standard 
notation&quot; accepted on a lot of FreeCell-related websites and programs. This 
involved some heuristics I had to change several times for it to work 
correctly, but when it worked, I even added the &quot;h&quot; letter as the third 
option to use when moving a card to the foundation, i.e. 4, 40, and 4h all 
mean &quot;move the last card from column 4 to the foundation&quot;. This was done 
because &quot;*h&quot; form is a part of that &quot;standard&quot; notation to move a card to 
its homecell. Right now, of course, you still have to press Enter/Return 
after entering each move, but, other than that, you can easily convert 
notation files to the command sequence.

The resulting piece of code I named nnfc (&quot;no-nonsense FreeCell&quot;, of course),
with all its features, still is under 300 SLOC of pure POSIX AWK. And I 
wrote it all on my tty-driven Alpine on that old A1370, in busybox vi. Wanna 
know how it all turned out? Well, you can clone my repo and see for 
yourselves:

git clone git://git.luxferre.top/nnfc.git

Make sure to read the README file to know how to run the game with the
options you need. For instance, on my Kindle, Kterm supports UTF-8 but 
doesn&#x27;t support colors, so I run it as awk -f nnfc.awk -v MONOCHROME=1 and 
it displays all cards with a single color.

So, here you are, my newly favorite card game is now playable on
BusyBox-based systems, as well as anything else that can run AWK scripts 
within the current standard. I hope I have made at least some contribution 
to the AWK gaming scene with this humble entry. And no, I wasn&#x27;t joking 
about porting Text Elite to POSIX AWK. I&#x27;m seriously considering it.

--- Luxferre ---

[1]: http://www.elitehomepage.org/text/index.htm
[2]: https://www.solitairelaboratory.com/fcfaq.html
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2024-01-15-state-of-a1370-so-far.txt</link>
<title>(2024-01-15) State of my A1370 (so far) and some minor news</title>
<description><![CDATA[<pre>
(2024-01-15) State of my A1370 (so far) and some minor news
-----------------------------------------------------------
I promised to tell you what has been set up on my old MacBook Air A1370, so
the time has come. As I already mentioned, after all my not so successful 
experiments, it runs Alpine Linux again, and I&#x27;m going to keep it that way 
from now on. However, the set of software I&#x27;m running on top of it is quite 
different this time, and as bizarre its choice might seem, it is purely a 
result of a series of experiments on its own. So, grab your snacks and let&#x27;s 
begin from the very beginning.

The main idea of my setup is to run everything (and I mean everything) from
the bare console, without any X or Wayland sessions. At first I thought of 
using framebuffer-based terminals like fbterm or kmscon (to just be able to 
use the same font I got used to in X-based terminals), but they had 
introduced more problems than they solved, so I settled on the bare console 
with Terminus font (ter-u20b.psf.gz, to be exact). I don&#x27;t fully like it but 
I couldn&#x27;t really find any viable Unicode-aware (and particularly 
Cyrillic-aware) alternatives that would look decent in such an environment 
as of this moment. And yes, I created my own layouts using ckbcomp that 
fully mimic the X layout setting I&#x27;m using on my Arch/Garuda with the 
exception the console cannot use all three together so I just created two 
separate layout files for two pairs. But they are the same phonetic layouts 
and the switching is still with Caps Lock which is perfect for me.

Still, one of my tty sessions does run a multiplexer. At first it was dvtm,
but now I settled on a4 ([1]). Yes, I had to compile it but everything is 
flawless and the config is very powerful. I had also modified my shell 
information script to be able to output the information into a4&#x27;s statusbar 
([2]). Although in real life, I rarely use more two tags and open two 
windows per tag in it. And, of course, all video/gaming/browsing has to be 
done in another tty outside of it. I also manage sessions with abduco, 
although that one is more useful on my VPS.

Speaking of video/gaming/browsing, I have been successfully using the
following tools that involve (pixel-based) GUI of some sort:

- Links browser in links -g mode: compiled it myself as the Alpine package
version didn&#x27;t have graphics enabled, had to install GPM just to enable 
mouse there (although it doesn&#x27;t require GPM running, I actually disabled it 
afterwards as this daemon is pretty much useless with the A1370&#x27;s touchpad) 
and install all sorts of extra graphics format libraries to support it and 
SSL too, TTF fonts are also supported just fine, very pleasingly surprised 
with the results and overall customizability of this browser;
- Screenshooting with fbgrab, image viewing with fbi (lol) and PDF viewing
with fbgs: the first two work amazing, the third one works... OK;
- mpv video player: had to append nouveau.atomic=1 to the kernel command
line, afterwards running it successfully with --vo=drm option (more on that 
later);
- Crispy Doom: although the SDL2 backend here has some extreme troubles with
mouse support (hence I couldn&#x27;t get netsurf-fb to work), I&#x27;ve been able to 
use the keyboard and even a gamepad with Crispy Doom when exporting 
SDL_VIDEODRIVER=kmsdrm environment variable;
- Schism Tracker: again, mouse doesn&#x27;t work even with GPM running, but
everything else works fine, especially in the fullscreen mode (although with 
this backend everything is fullscreen anyway, but the text becomes much 
crispier if you set it);
- RetroArch: heck yeah! Unlike Mednafen that freezes the system even after
displaying the help screen, RetroArch works wonderfully on the GL backend 
here (without the mouse, of course), even gamepad autodetection works. Just 
make sure to configure _both_ keyboard and gamepad correctly and 
independently, and don&#x27;t forget to set correct system directories as well. 
Haven&#x27;t yet tried livestreaming directly from it though, but I&#x27;m sure it&#x27;s 
gonna be fun if it doesn&#x27;t fry the hardware.

And yes, of course I added my username to wheel, audio, input, video and
netdev groups. Still no luck with SDL+kmsdrm mouse support here. It _might_ 
have something to do with me using busybox mdev instead of udev and/or 
Alpine&#x27;s SDL2 being compiled without libudev support, but I&#x27;m not even sure 
how to test this out, and I&#x27;m definitely not going to replace the Alpine&#x27;s 
version with my own.

In case you didn&#x27;t know, mpv also allows you to view YouTube videos and
various platform livestreams if you also install the youtube-dl helper. In 
the Alpine testing repos (yes, I switched to the edge channel and also 
enabled the testing repo), you can find a wonderful utility called 
pipe-viewer that allows you to search and view YouTube videos anonymously... 
either through VLC or through mpv. So, in order to limit the downloaded 
picture to the 720p quality (as my A1370&#x27;s resolution is 1366x768 anyway), I 
created two aliases for stream/video viewing and youtubing respectively:

alias fbstream=&#x27;mpv --vo=drm
--ytdl-format=&quot;bestvideo[height&lt;=?768][vcodec!=vp9]+bestaudio/best&quot;&#x27;
alias yt=&#x27;pipe-viewer --player=mpv --append-arg=&quot;--vo=drm&quot; -7&#x27;

That&#x27;s it. The first command also allows you to view e.g. Twitch streams. And
there also are some CLI chat clients for Twitch that can be made use of 
(once you obtain the auth token from the website, that is). I found 
twitch-chat-cli ([3]) to work best for me, although yes, I did have to 
obtain the token on another machine and copy it over to the MacBook. Sad but 
true: Twitch requires you to use a bloated browser to auth into their API.

Speaking of chats, the setup still is kind of in progress. At first, I wanted
to use everything via WeeChat, but only weechat-matrix worked as expected: 
tox-weechat didn&#x27;t compile, and weecord just hanged the entire application. 
So, I settled on the Toxic client for Tox and still don&#x27;t know what to do 
with others. Maybe I&#x27;ll just get away with Bitlbee and a simpler and more 
KISS-friendly IRC client. For now, it&#x27;s not critical though.

For other multimedia, I did install essential tools like mpg123, sox,
alsa-utils, alsa-tools, alsa-plugins and ffmpeg, but I also installed mpd 
and mpc (as well as ncmcpp to quickly enqueue new music). Why? Because, 
first, Alpine doesn&#x27;t have Open Cubic Player readily available and mpd 
already has tracker format support, second, I finally had an ambitious idea 
to make multimedia keys useful on the bare console. But for this to happen, 
some additional effort had to be made. And I did make it and create the 
nnkeys.sh script ([4]) that can read a separate config file and even 
daemonize itself. It is a clever combination of using evtest utility (has to 
be installed from a separate Alpine package) and AWK scripting to monitor 
exact keypresses and run command actions on them. Later on, I also 
introduced a privileged daemon mode to my nnlight.sh backlight management 
script, so that it could run on boot and all other nnlight.sh invocation 
just talked to that daemonized script via a named pipe without having to 
request the privileges.

Also, since I don&#x27;t use Bash at all there (only busybox ash and my favorite
oksh which I made the default one), I cannot browse Gopher with my own 
Bopher-NG script. I also don&#x27;t like how Lynx does it. So, what do I use? I 
use sacc([5]) that is even available in the repos. The only thing is, I had 
to change the $PAGER to most (again, yet another package) because more works 
incorrectly with it and less displays trailing ^M&#x27;s on CRLF-terminated lines 
in text files. Other than that, sacc is a neat KISS-friendly Gopher client 
that even supports some TLS in case you need it there. For Gemini, I decided 
to use Amfora because it also has a ready-made Alpine package and because I 
didn&#x27;t really like Bombadillo&#x27;s controls and the fact it doesn&#x27;t reset 
terminal attributes on exit.

That&#x27;s if for the state of A1370. As you can see, the setup is pretty much
90% complete and still is in progress, but it definitely was worth it, 
seeing the _peak_ RAM usage at around 550MB with a video being streamed with 
pipe-viewer + mpv and a bunch of daemons and links -g running in the 
background, and the normal usage (without video streaming) doesn&#x27;t ever 
exceed 200 MB. Now, let&#x27;s talk about some other news. First, I completed the 
mirror scripts for this very phlog and LuxDocs section for them to be 
available at https://hoi.st in (obviously Links-friendly) HTML2 format. This 
was done to ease sharing my materials with the &quot;outside world normies&quot; who 
don&#x27;t want to install Gopher clients for some reason. As with all other 
static web pages on my VPS, the mirror is served via the spa-to-http 
container behind Traefik that, in turn, handles SSL and host-based routing. 
The setup already is time-tried and I doubt I&#x27;m going to move to anything 
else at this point.

Another thing you might have noticed is that have I started sharing URLs to
my own Git instance (https://git.luxferre.top) instead of SourceHut or 
GitLab. Yes, once SourceHut went down recently, I had had enough and decided 
to migrate to my standalone Git. What most githubbers don&#x27;t realize is that 
Git package already has everything you need to create both private AND 
public Git repos (via the git:// protocol)... short of a Web server. It has 
a CGI-based Web interface but I don&#x27;t quite trust it. So, with the thought 
&quot;GitHubs must die&quot; dominating my head in the second pard of the week, first, 
I quickly created a helper script called ghmd.sh ([6]) to ease setting 
everything up (except post-receive hooks), second, I chose stagit ([7]) as 
the Web UI generator for the repos&#x27; Web UI. It took me some time to set up 
everything correctly in terms of using post-receive hooks to autogenerate 
the repos on push, but now it works just fine.

So, it&#x27;s just a matter of time to migrate my repos out of SourceHut once it
comes up, and also do the same for GitLab just in case. I also see there is 
something called stagit-gopher, but no, thanks. First, it generates the 
indexes in some non-standard format (a bit similar to my internal links 
format in Kopher) specific to another piece of software I&#x27;m definitely not 
planning to use, Geomyidae. Second, I think browsing the Git tree with 
Gopher is a bit overkill. If I want to create an index to publish here, it 
will just be a text file with git:// URLs autogenerated from the repo list 
and their descriptions, using the same &quot;url&quot;, &quot;owner&quot; and &quot;description&quot; 
files in the repo directory that stagit uses. And that would be it. My repos 
are simple enough to clone and explore offline, and I don&#x27;t want to mimic 
HTTP experience in a totally different environment.

To sum all this up, this week has been extremely valuable for me as I was
able to prove that there is life without X or Wayland, and there is life 
without third-party Git services. And yes, I feel sorry for Drew, but I also 
feel this is something I should have done a long time ago instead of opting 
for paid SourceHut subscription. I&#x27;m paying for the VPS anyway.

--- Luxferre ---

[1]: https://www.a4term.com/
[2]: https://git.luxferre.top/nnscripts/file/nnstatus-stdout.sh.html
[3]: https://github.com/martinbjeldbak/twitch-chat-cli
[4]: https://git.luxferre.top/nnscripts/file/nnkeys.sh.html
[5]: gopher://bitreich.org/1/scm/sacc
[6]: https://git.luxferre.top/nnscripts/file/ghmd.sh.html
[7]: https://codemadness.org/stagit.html
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2024-01-08-minimalism-strikes-again.txt</link>
<title>(2024-01-08) Minimalism strikes again: Kindle, Alpine and TUI-first mindset</title>
<description><![CDATA[<pre>
(2024-01-08) Minimalism strikes again: Kindle, Alpine and TUI-first mindset
---------------------------------------------------------------------------
So much happened throughout the week that I really had to reimagine what to
write today. I wanted to just talk about the Amazon Kindle Basic 11th Gen 
the company I work for gave to me as a Christmas gift and some internal 
activity related award, but this just wouldn&#x27;t cut it. Yes, this is among 
the first MediaTek-based Kindles: while all previous generations were on 
Freescales or something like that, this one runs on MT8110 (so-called 
&quot;Bellatrix&quot; board). Yes, I rooted it with LanguageBreak ([1]), freezing the 
firmware version at 5.15, installed KUAL, KOreader, Kterm ([2]) and 
kual-linuxdeploy-armhf, but then the question &quot;what next?&quot; bothered me for 
quite a while. I experimented with Xephyr-based XFCE installation and tried 
to get something else in there, but then I just settled on an Alpine-based 
chroot running in the same Kterm. Overall, the main firmware being just a 
custom Linux (with X server and everything) and not Android, I see the 
potential of turning this e-reader into a full-fledged hacker tablet, but 
now even this chroot environment will do.

Alpine Linux, by the way, has been my distro of choice everywhere except the
main nettop where I still run Arch/Garuda. I had been running it on an old 
2010 Macbook Air (A1370) as well, and even prepared a nice set of scripts 
and CWM configuration specifically for it. However I foolishly believed that 
I&#x27;d manage to find something even more minimal. Haiku and OpenBSD have 
troubles with the Macbook&#x27;s Broadcom B43 series WLAN chip. Void Linux is 
just plain glitchy on that hardware for no apparent reason. This is how I 
came to installing CRUX 3.7 on that very laptop.

A grave mistake.

For the first couple of days, I honestly thought I would get a usable and
rock-solid system if I would endure this long process of compiling 
everything starting from the kernel and then whatever prt-get sysup command 
was pulling in. On a 2-core 2GB laptop from 2010, uh-huh. On the third day, 
while waiting for Rust (!) to compile, I managed to break what seemed 
impossible to break and I believe couldn&#x27;t be broken on any other system: 
GCC. Namely, header files that I must have overwritten with a parallel scc 
installation. Or cproc, or chibicc, I don&#x27;t remember exactly. As a matter of 
fact, some headers were gone. And guess what: restoring from the initial GCC 
package DIDN&#x27;T FIX THEM. I surely must have missed something, but at this 
point I had enough and went back to Alpine. By the way, I spent more time 
copying the 3.19-extended ISO contents to the flash drive than actually 
clean-installing it with its wonderful setup-alpine script.

By the way, Alpine is not so obvious to install from a USB flash drive, at
least on this Macbook A1370, unless you know a simple trick, because just 
dd-ing the ISO image won&#x27;t work. And, by the way, no one is preventing you 
from using the same trick on all other x86-64 machines. First, format your 
flash drive with a GPT partition table (that&#x27;s a requirement) and a single 
FAT32/vfat partition. Then, just extract the Alpine ISO contents into the 
root of that partition. That&#x27;s it. You now have a flash drive that&#x27;s both 
usable as a &quot;normal&quot; flash drive and bootable into an Alpine 
live/installation system image on any (U)EFI-enabled machine, including such 
Macbooks (which should detect it when booting with the Option/Alt key 
pressed down).

Once you created this flash drive though, everything else is a breeze. For
x86 and x86_64, I recommend using the &quot;extended&quot; Alpine images because they 
already contain all the firmware you might need to get connected. And again, 
with this method, you totally may continue using the flash drive normally 
for other storage purposes, just don&#x27;t touch the boot, efi and apks 
directories, and also probably the .alpine-release file in its root. This is 
as human-oriented as it can possibly get for such a minimalist distribution.

As for the brief moment of using CRUX, I have to give it one credit though:
it really forced me to review my perception of minimalism even more. Having 
to waste time to compile a lot of things that we take for granted in other 
distros made me wonder whether or not we could get away without using those 
things. Unfortunately, this includes the compilers themselves. Again, I&#x27;m 
all for deploying cproc/chibicc/scc in any environment I can, but a lot of 
software nowadays really depends on monstrous GCC/Clang-based toolchains 
(and yes, zig cc belongs to the same category as a Clang derivative). And 
yes, Nim doesn&#x27;t help the situation as it uses the same monstrous toolchains 
as the middleware. This is why, for my personal projects, I&#x27;m also thinking 
about exploring some simpler options like (again, qbe-based) Myrddin ([3]) 
but I&#x27;m not really sure yet how independent it really is.

Another important topic that CRUX has directed my thoughts to and that I
really am looking forward to recreate in Alpine (instead of the cozy 
Xorg+CWM) is purely TUI experience. Maybe even with framebuffer enabled, 
because why not? E.g. fbterm looks fantastic when pointed to the Fira Code 
Retina font. But I also have learned to generate pure-console keyboard 
layouts, learned about some more TUI and framebuffer-aware software and so 
on. Probably some of my next posts will be about the details of what I have 
done on my newly-reinstalled Alpine and how exactly I have configured 
everything there.

Moral of the story: 1) Amazon sucks as always, 2) true minimalism is not what
it seems, 3) the more you use CRUX, the more you appreciate Alpine.

--- Luxferre ---

[1]: https://github.com/notmarek/LanguageBreak
[2]: https://www.fabiszewski.net/kindle-terminal/
[3]: https://myrlang.org/
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2024-01-01-html2-rocks.txt</link>
<title>(2024-01-01) In 2024, HTML2 still is (almost) everything you really need</title>
<description><![CDATA[<pre>
(2024-01-01) In 2024, HTML2 still is (almost) everything you really need
------------------------------------------------------------------------
Hi there. It&#x27;s a miracle for me to be able to survive this far through all
this. But here I am, not willing to reflect on the past shitty year, knowing 
that this one might probably get even shittier. In my next post, I&#x27;m going 
to tell you something about my company gave me as a Christmas present, but 
now I&#x27;m going to concentrate on something really basic: the Web.

Again, it might seem strange to write about the Web on Gopher, but first,
I&#x27;ll probably create a Web mirror of this phlog at some point (to replace 
the aging chronovir.us which expires soon and I&#x27;m not sure if I&#x27;ll prolong 
the domain), second, let&#x27;s face it, despite how good Gopher is, it will 
never get the majority of the audience you&#x27;re trying to reach out to. This 
might be a good thing (as a filter from the lame crowd), but this also means 
that there are some tech-savvy people who might be interested in your ideas 
and projects but just don&#x27;t know about Gopher or are too lazy to take an 
additional step to read your material. Heck, even on Gopherspace, I 
constantly stumble upon links to GitHub that can&#x27;t even load properly 
without JS anymore...

Anyway, what I&#x27;m trying to say is that Web presence is inevitable nowadays.
However, it is fully on us to make this presence non-bloated. Of course, we 
cannot fully get to the Gopher&#x27;s level of purity but we still can make our 
pages the oldschool way, with minimum amount of styling and no unnecessary 
luggage. The secret is that we must employ... a lot of practices that are 
considered &quot;bad&quot; by modern web design conventions.

What do I mean by this? First and foremost, I mean this HTML boilerplate you
should use for every page:

&lt;!DOCTYPE html PUBLIC &quot;-//IETF/DTD HTML 2.0//EN&quot;&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width&quot;&gt;
    &lt;title&gt;&lt;!-- your title here --&gt;&lt;/title&gt;
  &lt;/head&gt;
  &lt;body bgcolor=&quot;&quot; text=&quot;&quot; link=&quot;&quot; alink=&quot;&quot; vlink=&quot;&quot;&gt;
  &lt;font face=&quot;sans-serif&quot;&gt;
    &lt;!-- your HTML-formatted text here --&gt;
  &lt;/font&gt;
  &lt;/body&gt;
&lt;/html&gt;

Yes, now I hope you see all the &quot;bad&quot; practices. For instance, if you look up
the &lt;font&gt; tag and the &lt;body&gt; attributes, you&#x27;ll see that they are not 
recommended and discouraged to use all over the MDN and other resources. In 
fact, however, this is the most efficient and universal way to style your 
text. Just like with CSS, you can supply a comma-separated font list in the 
&quot;face&quot; attribute of the &lt;font&gt; tag with the most generic one taking the 
least priority by putting it at the end of the list, so here&#x27;s your font 
adaptation. And using &lt;meta name=&quot;viewport&quot;&gt; doesn&#x27;t contradict the HTML2 
spec either. Ancient browsers that don&#x27;t know what a viewport is will just 
ignore this tag, but the newer ones will do their job by reflowing your text 
according to the screen specs.

To be on the safe side, you can also add some inline styling to the &lt;body&gt;
tag style attribute, e.g. for duplicating the font family or setting the 
line height for the browsers that understand it. But the amount of CSS 
should be kept at an absolute minimum and your pages must be at least 
readable and functional with no CSS applied at all.

I recommend testing your pages on three or four browsers: Dillo, NetSurf, any
Firefox derivative and (optionally) any WebKit derivative. Yes, in this 
particular order. Dillo hasn&#x27;t been updated since 2015 but still is the 
default browser in some minimalist distros and has the lowest RAM footprint 
out of all GUI-based browsers, not counting Links2 in the GUI mode. If you 
make your page look fine in Dillo, I guess it will look fine elsewhere. The 
only quirk about Dillo you can disregard is that it tends to make all 
non-standard fonts bolder than they are.

As for mobile device testing, try to use the smallest and the oldest ones. If
you have a KaiOS phone, that&#x27;s nice, but ideally, your pages should be 
functional even on 2.3-inch MAUI phones and 6-inch browser-enabled 
e-readers. And for these purposes, I would only limit the set of tags to the 
following ones: html, head, meta, title, link, base, body, br, div, p, h1, 
h2, h3, h4, h5, h6, big, small, center, b, i, u, a, img, input (only these 
types: text, password, button, submit, reset, image, checkbox, radio, 
hidden), select (+multiple), option, textarea, form, ul, ol, li, table, tr, 
th, td, hr. Of course, other formatting tags allowed in HTML2 can be used 
too, but the results might vary from browser to browser. Use of tables 
should be really limited if you aim for mobile support.

Speaking of which, there is another controversial topic: HTTPS. Old devices
usually don&#x27;t like it: either CAs or algorithms are unsupported, or, if 
we&#x27;re talking _really_ old devices, they lack the processing power to deal 
with SSL/TLS. So, here&#x27;s my take on this problem:

1. Whenever possible, serve both plain HTTP and HTTPS content. Do not enforce
HTTPS redirection if the user explicitly visits HTTP URLs.
2. When serving HTTPS, enable TLS v1.2 and the following cipher suites on the
server side (IANA convention): TLS_RSA_WITH_AES_128_CBC_SHA, 
TLS_RSA_WITH_AES_256_CBC_SHA, TLS_DHE_RSA_WITH_AES_256_CBC_SHA. This will 
ensure maximum compatibility with old systems that are not possible to 
upgrade to TLS v1.3.

For OpenSSL, these identifiers translate as:

TLS_RSA_WITH_AES_128_CBC_SHA     = AES128-SHA
TLS_RSA_WITH_AES_256_CBC_SHA     = AES256-SHA
TLS_DHE_RSA_WITH_AES_256_CBC_SHA = DHE-RSA-AES256-SHA

Usually, enabling just the first or the second suite is sufficient. Same goes
for gzipping the content: the server must only compress your pages when 
requested by the browser to do so.

If you, just like me, hide HTTP services behind Traefik and like them
containerized, if you only need static HTML content to be served and can&#x27;t 
stand bloatware like Nginx, you can use a wonderful spa-to-http container to 
do the job by putting something like this into the existing compose.yaml 
file with your Traefik-based TLS resolver (in this example, it&#x27;s called 
&quot;myresolver&quot;):

  spa-to-http:
    image: devforth/spa-to-http:latest
    container_name: spa-to-http
    restart: unless-stopped
    environment:
      GZIP: true
      BROTLI: true
      SPA_MODE: false
    labels:
      - &quot;traefik.http.routers.websrv.rule=Host(`[your_domain]`)&quot;
      - &quot;traefik.http.routers.websrv.tls=true&quot;
      - &quot;traefik.http.routers.websrv.tls.certresolver=myresolver&quot;
    expose:
      - 8080
    volumes:
      - &quot;[your_website_root_dir]:/code&quot;

Note that the spa-to-http container doesn&#x27;t seem to support live reloading as
of now but you can restart it every time you&#x27;ve deployed your website. It 
doesn&#x27;t take long.

Another important topic is navigation within the page. Some young folks think
it is impossible without JavaScript. In fact, it has been there from the 
very beginning of HTML, and it&#x27;s called named anchors. Yes, the name 
attibute initally only made sense in the &lt;a&gt; tag and served exactly this 
purpose: to mark various parts of the page (&lt;a name=&quot;somename&quot; /&gt;) that can 
be later referenced to with &lt;a href=&quot;#somename&quot;&gt;such links&lt;/a&gt;. Now, you 
don&#x27;t even have to use named anchors explicitly (unless you find a browser 
where you have to): modern browsers also allow you to reference any existing 
element that has an id attribute (instead of the name attribute) in an &lt;a&gt; 
tag in the very same way.

When thinking about readability, we can also take some care of the people who
just download the raw HTML source code and view it &quot;as is&quot;. Unless in 
preformatted text block elements like &lt;pre&gt; or &lt;xmp&gt;, all consecutive 
whitespace characters in raw text nodes are collapsed into one whitespace 
(0x20). So you still can write your paragraphs (in &lt;p&gt; tags, for instance) 
in a 78-column formatted fashion and the browser will have no problem 
rendering them as they should: as long as it&#x27;s not a preformatted text 
element, it doesn&#x27;t care, but now the people who view the raw HTML can read 
this text more easily too. Same about tag indentation: you can only leave it 
where it helps _humans_ recognize important parts, and all inevitable junk 
that doesn&#x27;t carry any meaning for the reader can be collapsed into a single 
line.

Do not overuse text coloration. And don&#x27;t ever use it for stressing anything
meaningful, only for some decoration that can be treated as fully optional. 
Remember there still are greyscale e-ink devices, B/W LCD devices, console 
terminals and so on. Same thing can be said about images: insert them 
sparingly. Not to mention they also increase network traffic consumption.

Last but not least, forget about &quot;pixel perfection&quot;. Unless absolutely
necessary, don&#x27;t use image maps and other things that depend on pixel 
positioning. Don&#x27;t expect every browser to have the same margins, the same 
fonts, the same rendering and flow. Trust the browser defaults to perform 
the most basic formatting tasks for you. Concentrate on the contents, not 
presentation. After all, this is what made the old Web so valuable that we 
miss it now.

Happy new year!

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-12-25-pen-and-paper-crypto.txt</link>
<title>(2023-12-25) Pen and paper cryptography: breaking down what's still worth</title>
<description><![CDATA[<pre>
(2023-12-25) Pen and paper cryptography: breaking down what&#x27;s still worth
-------------------------------------------------------------------------
A long time ago (a _very_ long time ago) I created a protocol for SMS text
encryption. It was based on a well-known Bifid cipher (we&#x27;ll get to this a 
bit later) but applied a little bit differently, and, of course, all 
encryption/decryption must be done manually. Back then, I thought of it as 
of something worth publishing because almost no one had been touching &quot;hand 
crypto&quot; in the age of computers anymore. But my protocol also had several 
issues I only realized much later and abandoned it in the depth of my GitHub 
Gist. The topic is still alive though, and may see another resurgence as we 
are drowning deeper and deeper into the modern dystopia. The only problem is 
what is still worth studying within this topic, because most hand ciphers 
existing in the world really are obsolete and can&#x27;t be used nowadays.

In this post, I&#x27;m only going to focus on _symmetrical_ cryptography.
Asymmetrical cryptography still is somewhat possible to do by hand but it&#x27;s 
much less practical as of now.

Before I get into the details of what we can build, let me share my own
criteria of what I consider a usable pen and paper crypto algorithm today:

1. It must meet both Shannon&#x27;s requirements for confusion and diffusion.
2. It must support (or be able to extend to support) encrypting alphanumeric
messages (so, at least 36 characters).
3. It must not involve any calculations beyond some modulo addition and
subtraction that can be done mentally, and preferably should not make the 
operators to do even those.
4. Guessing the key length by analyzing the ciphertext must be impossible.
5. Average encryption/decryption speed must not be lower than 10 characters
per minute by a trained operator.
6. All printed or handwritten material that doesn&#x27;t involve key information
must be as reusable as possible.

Based on these criteria, here&#x27;s what I&#x27;m NOT considering viable pen and paper
ciphers or their building blocks:

1. Straddling checkerboards. Yes, they are useful but not fast enough and
restrict you to modulo-10 arithmetic. And for keystream generation, their 
output is not uniform as they will generate the letters that map to single 
digits much more often.
2. Any playing card based system such as Solitaire/Pontifex. Too slow to
encrypt even a single character.
3. Columnar transpositions, except when preparing the key material. When used
on the plaintext, they can expose the key length. Also, only the relative 
order of the letters in the key matters for a columnar transposition, not 
their values, so different keys with the same relative order (like, for 
example, words &quot;key&quot; and &quot;law&quot;) will produce the same results.
4. Bigram substitutions (like Playfair), except when preparing the key
material. When used on the plaintext, they expose too many irregularities 
and can&#x27;t provide any protection against statistical analysis.

Now, let&#x27;s finally list the building blocks for a pen and paper cipher that
meets the above criteria:

1. Tabula recta. For an alphabet of size N, this is a square table of NxN
characters consisting of all possible rotations of this alphabet. For the 
user&#x27;s convenience, the table might be indexed with the duplicates of the 
first row and the first column respectively. This is what&#x27;s used to &quot;add&quot; 
and &quot;subtract&quot; letters of the target alphabet without having to convert them 
to digits and performing any modular arithmetics. For any chosen character 
set, a tabula recta can be generated, typed or even handwritten from memory 
with virtually no effort. In fact, for a &quot;one-time pad&quot; cryptosystem, which 
is proven to be unbreakable if properly used, all one needs is a tabula 
recta and securely distributed sheets of key material.
2. Polybius square. For an alphabet of size N (and N should be a full square
in this case), this is a square consisting of all N characters of this 
alphabet, and all rows and columns are usually numbered from 1 to sqrt(N). 
Such a square can be used for different purposes in the cryptosystem, but 
the most promising uses are based on achieving the diffusion goal by 
splitting the letters by their row and column numbers and recombining them 
from those numbers in a different way. This is how ADFGVX and Bifid ciphers 
worked. 
3. Autokey. Although mistakenly attributed to only one (and pretty insecure)
method of just putting the beginning of the plaintext into the keystream 
after the key itself has ended, the autokey concept in fact means multiple 
ways of extending the key material based on the previous values, and it 
doesn&#x27;t (and shouldn&#x27;t) necessarily involve plaintext at all.
4. Polyalphabetic substitutions. The way the alphabet gets modified doesn&#x27;t
matter much, what matters is that it&#x27;s not static. However, not every way of 
doing this meets our initial criteria, so we must be careful about what we 
choose here. For instance, just using a plain tabula recta as the source of 
a polyalphabetic substitution and calling it a day won&#x27;t work.

So, these are the main ideas that can be utilized for our paper crypto
purposes. Now, let&#x27;s see them in action by designing a pen and paper stream 
cipher!

Part 1. The tabula recta

This is the definite tool that will help us with all our &quot;calculations&quot;.
Print out (or write down) the tabula recta for the alphabet of your choice, 
in our case it&#x27;s 36 characters: first A to Z, then 0 to 9. Writing down 
36x36 squares can be tedious, so I recommend preprinting them. The columns 
and rows are indexed from both sides for ease of use.

Here&#x27;s an AWK script to generate and display the full 36x36 tabula recta:

BEGIN {
  alpha = &quot;A B C D E F G H I J K L M N O P Q R S T U V W X Y Z &quot; \
          &quot;0 1 2 3 4 5 6 7 8 9 &quot;
  l = length(alpha)
  printf(&quot; | %s|\n%s&quot;, alpha, &quot;-|&quot;)
  for(i=0;i&lt;l;i++) printf(&quot;%s&quot;, &quot;-&quot;)
  print(&quot;-|-&quot;)
  beta = alpha
  do {
    ind = substr(beta, 1, 1)
    printf(&quot;%s| %s |%s\n&quot;, ind, substr(beta, 1, l - 1), ind)
    beta = substr(beta, 3) substr(beta, 1, 2)
  } while(beta != alpha) 
  printf(&quot;%s&quot;, &quot;-|&quot;)
  for(i=0;i&lt;l;i++) printf(&quot;%s&quot;, &quot;-&quot;)
  print(&quot;-|-&quot;)
  printf(&quot; | %s|\n&quot;, alpha)
}

It creates the following table that you can print out:

 | A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 |
-|-------------------------------------------------------------------------|-
A| A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 |A
B| B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A |B
C| C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B |C
D| D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C |D
E| E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D |E
F| F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E |F
G| G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F |G
H| H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G |H
I| I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H |I
J| J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I |J
K| K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J |K
L| L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K |L
M| M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L |M
N| N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M |N
O| O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N |O
P| P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O |P
Q| Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P |Q
R| R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q |R
S| S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R |S
T| T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S |T
U| U V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T |U
V| V W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U |V
W| W X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V |W
X| X Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W |X
Y| Y Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X |Y
Z| Z 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y |Z
0| 0 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y Z |0
1| 1 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 |1
2| 2 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 |2
3| 3 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 |3
4| 4 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 |4
5| 5 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 |5
6| 6 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 |6
7| 7 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 |7
8| 8 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 |8
9| 9 A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 |9
-|-------------------------------------------------------------------------|-
 | A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 |

Note: use a monospace font to display and print it correctly.

Part 2. Alphabet keying and converting your passkey into the seed

Now we need to scramble our alphabet to use in the keystream. Just like the
seed that we&#x27;ll get to in a moment, this is going to depend on the passkey 
we&#x27;ve chosen. The process of keying the alphabet is as follows: 

1. Uppercase your passkey and remove all characters not belonging to the
target alphabet (in our case, [A-Z0-9] range).
2. Start writing the remaining passkey characters into your new alphabet. If
the character repeats, keep going down the alphabet (wrapping around the end 
if necessary) until you find the one that hasn&#x27;t been used. Note that this 
modification is only used for substitution keying, not the seed!
3. Write down all the remaining alphabet characters in the reverse order.
4. Write the resulting alphabet as the first row in the substitution box, and
the unscrambled alphabet as the second row.

Suppose we have the alphabet represented by the above 36x36 tabula recta, and
our passkey is SiVisPacemParaBellum-2024. Let&#x27;s key the alphabet using the 
above rules:

1. Prepare the passkey: SIVISPACEMPARABELLUM2024
2. Start the alphabet by writing the passkey characters: SIV... Ok, we
already have I so we go down 1 position and write H. If we continue like 
this, we&#x27;ll end up with the following modified passkey: 
SIVHRPACEMO9Q8BDLKUJ2014. This is the start of our keyed alphabet.
3. Finish the alphabet by writing all remaining characters in the reverse
order. In our case, we should be left with this (exactly 36 different 
characters):

SIVHRPACEMO9Q8BDLKUJ20147653ZYXWTNGF

This is the first substitution box row. Complete it with the plain alphabet
in the second row:

SIVHRPACEMO9Q8BDLKUJ20147653ZYXWTNGF
ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789

Now, to create a seed for your keystream, there are several methods. The
laziest one is to just reuse the adapted passkey (in our case, 
SIVISPACEMPARABELLUM2024) as the seed directly. This works but can possibly 
reveal some correlation information. To break this correlation a little, we 
can just &quot;add&quot; the passkey to the _reversed_ keyed alphabet string using the 
tabula recta.

In our example:

SIVISPACEMPARABELLUM2024
FGNTWXYZ35674102JUKLDB8Q
------------------------
XO81ECY17HL7L116U54X510K = our seed for the autokey part.

Part 3. The autokey

For keystream generation from the seed (the autokey element), let&#x27;s use
lagged Fibonacci generators. The main idea is to add or even subtract 
(numerically or directly via the tabula recta) modulo N (the alphabet size) 
not the previous two characters like in a normal Fibonacci sequence, but the 
previous one and the character placed at some larger fixed distance before 
it. In our case, the obvious choice for this distance would be the seed 
length minus 1. So, for instance, if we use addition and a standard 26x26 
tabula recta, then for the seed CRYPTO, we&#x27;d add O and C and get Q, then add 
Q and R and get H, then add H and Y and get E and so on. As shown by 
Francisco Ruiz ([1]), this particular autokey technique allows to generate 
keystreams that exhibit nice statistical properties using nothing but a 
single tabula recta, which might be additionally keyed (the substitution box 
we&#x27;ve generated in the previous step performs exactly the same function). In 
our case, we use a 36x36 tabula recta but the idea is absolutely the same.

Let&#x27;s demonstrate how this works. Start with the seed we generated earlier:

XO81ECY17HL7L116U54X510K

To get the next keystream character, we just &quot;add&quot; the current last character
to the first one using the tabula recta, and then use our keyed alphabet 
substitution:

X + K = 7, sub(7) = Y

To get yet another keystream character, we &quot;add&quot; the new last character to
the second one and then do the substitution:

Y + O = C, sub(C) = H

And so on. Whenever we&#x27;re going to &quot;add&quot; a new character at (pos), we&#x27;re
&quot;adding&quot; the character at (pos - 1) and the character at (pos - slen), where 
slen is our seed length.

Since we use keyed substitution as the middle step, this process is nonlinear.

Important note: the initial seed must be dropped from the keystream after
enough characters are generated to cover your plaintext messages. So, for 
the plaintext sized M, you have to generate at least M new keystream 
characters from the seed. I recommend preparing keystream sheets by both 
parties before the transmission, just like they would do for one-time pad 
communication. And, just like for one-time pad communication, the keystream 
characters already used for encryption must be securely discarded and never 
be reused.

Part 4. The protocol

Once both parties have the keystream sheets ready, the protocol is extremely
simple:

1. Alice wants to transmit a message M1 of len L1, so she uses L1 bytes of
the keystream to encrypt M1 (by &quot;adding&quot; each its character to the 
corresponding keystream character with the tabula recta, which takes very 
little time) and transmits it as the cryptogram C1.
2. Bob receives the cryptogram C1 and determines its length L1. He then uses
L1 bytes of the keystream to decrypt C1 (by &quot;subtracting&quot; each keystream 
character from the corresponding C1 character using the tabula recta) and 
recovers the initial message L1.
3. Both parties cross out L1 keystream characters. The next round starts with
the (L1+1)-th character.
4. When the parties are about to run out of the keystream characters (50-60
remaining), they agree on a new key phrase to generate the next keystream. 
The old key phrase must not be reused. For instance, chen trying to set up a 
new key phrase like &quot;Opus Magnum, Veni Vidi Vici&quot;, Alice might send 
&quot;NKXOPUSMAGNUMVENIVIDIVICIXCNF&quot; to Bob, and Bob might send the confirmation 
&quot;NKXOPUSMAGNUMVENIVIDIVICIXACK&quot;, both encrypted with the remaining old 
keystream characters. The new transmissions afterwards would already use the 
new keystream.

Verdict

36x36 tabula recta is noice. You can estimate how many bits of key
information is contained in N base-36 characters by multiplying N by 
log2(36), which is approximately 5.17. And it might surprise you that you 
can already exceed the current symmetric cipher standard key size (128 bits) 
with just 25 (significant) passphrase characters. Even 11 passphrase 
characters give you DES grade of security (over 56 bits), 13 characters give 
64 bits, 16 characters give 80 bits. So, while 36x36 may seem large in 
comparison to the traditional 26x26, in fact it&#x27;s much more functional and 
allows to keep your passkeys at reasonable sizes while still maintaining a 
decent keyspace.

But the tabula, after all, is only a &quot;calculator&quot;. The heart is in the
algorithm itself. Keep in mind that the above system isn&#x27;t yet complete and 
polished and is more of a mind excercise of mine for now, it might be 
simplified even more if something is proven unnecessary. I&#x27;m also probably 
going to come up with something _much_ simpler that (unlike e.g. Hutton 
cipher) doesn&#x27;t require you to write any intermediate results at all in case 
you can&#x27;t have anything else besides your keypad phone where you read or 
write encrypted messages. Whenever I deem either of them finished, I&#x27;ll 
publish this algo and the &quot;mental&quot; one in the LuxDocs section, alongside the 
computer-aided simulations for both. Overall, I hope this longread was 
interesting and useful for you.

--- Luxferre ---

[1]: https://arxiv.org/pdf/2312.02869.pdf
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-12-18-on-passion.txt</link>
<title>(2023-12-18) On passion</title>
<description><![CDATA[<pre>
(2023-12-18) On passion
-----------------------
There is a huge difference between passion and motivation, however big the
latter is.

With motivation, you can clearly explain why you&#x27;re doing what you&#x27;re doing.
Passion is when you generally can&#x27;t do this. You still understand why, but 
can&#x27;t explain. At least in a way that general public will get your point. 
But if you really don&#x27;t know why you&#x27;re doing that, that&#x27;s not even passion, 
that&#x27;s called stupidity.

There are some things I&#x27;m motivated to do (e.g. in order to survive) and some
things I&#x27;m just passionate about. For instance, I can logically explain why 
I started learning Toki Pona but I can&#x27;t explain why I recently created the 
Toki Pimeja encoding for arbitrary binary data. If I try to explain this, 
I&#x27;ll look like a crazy person to anyone who doesn&#x27;t know the premise of what 
I&#x27;m living in now, and even to those who do. This is an actual problem for 
me because I can&#x27;t find same-thinking people to help me with some of my 
initiatives, just because I never can fully explain what they are for. This 
can lead to me running out of passion for a particular thing that completes 
an already long list of abandoned projects of mine.

GerdaOS is an example of such a thing. It started as a project to essentially
make the Nokia 8110 4G stock ROMs more secure and give KaiOS back to the 
community in terms of liberating it from the KaiStore lock-in. The eventual 
idea was to turn it into a custom and more secure KaiOS distribution that 
gives its users much more control over their devices than they have, making 
it a keypad smartphone platform to hack and build upon. But, while being 
overwhelmed by work, I ultimately couldn&#x27;t deliver that idea to other 
community members. Most of them didn&#x27;t even want that degree of freedom and 
security, they just wanted a less laggy system that could also run WhatsApp 
and other proprietary BS. Meanwhile, I tried to integrate at least some VoIP 
but the KaiOS 2.5.x API is so buggy that I couldn&#x27;t even acquire the 
earpiece control the way it should have worked 99 times out of 100, only the 
multimedia speaker. Even WhatsApp does this via some system .so library, not 
the Gecko API. But that was the least of my problems, and eventually, I 
couldn&#x27;t even understand who my target audience was and whether it was worth 
to continue any effort in that direction. By the way, Kopher and RCVD are 
relatively new but too niche, so my last &quot;big impact&quot; KaiOS applications 
appreciated by the community were CrossTweak, FastContact and FastLog. But I 
wrote them for myself in the first place, and I did so before finally 
realizing KaiOS was a dead end from the start, at least for my true goals.

But why? One word: complexity. Linux kernel is extremely complex. Android is
extremely complex. Any modern browser engine is extremely complex. So, 
KaiOS, being a combination of Linux kernel + Android base system + a 
suboptimal UI running on top of a browser engine, is a nightmare to even 
start thinking of building any really secure communication system with. And 
let&#x27;s not forget it is also running on a piece of smartphone hardware which 
itself is obscure enough to not trust it to build such a system, not to 
mention how complex it is as well. This is the closest I could get to any 
logical explanation about why I ran out of passion of developing anything 
for KaiOS anymore, and I tried to share these thoughts with the KaiOS 
community in their Matrix bridged with Discord. I was met with dead silence. 
No one was interested in this. I even tried joining some other Discord chat 
dedicated to featurephones, but it turned out to consist of straightaway 
noobs who couldn&#x27;t even get that any commercial non-UMS9117(L)-based 
&quot;featurephone&quot; that supports LTE nowadays actually is running a 
stripped-down Android version on fully smartphone hardware. There was 
nothing to discuss with them. Other groups who claim to develop &quot;secure 
phones&quot; use the same approach of putting modified Androids onto smartphone 
hardware.

If you ask me, I really am stuck alone with my vision right now. I think some
answers might have been given by the first luxury XOR phones and their 
firmware, but this is something I really doubt will appear in any leaks 
anytime soon, and, of course, you can&#x27;t order the first model from the 
official sources anymore. SC6531E isn&#x27;t something I&#x27;d view as a platform of 
perspective, and SC770x and UMS9117, as I said, really are hard to crack 
even on the handshake level as of now. On the other hand, I do have plenty 
of MT6261-based devices to work with (and even some gigabytes of leaked MAUI 
sources to study) but don&#x27;t have a reliable open-source way to flash any 
memory area yet, only the BROM method to load and run any code from RAM. I 
also have some SIM800 modules that can be soldered into a phone of my own 
design... but that&#x27;s exactly what I want to avoid. I want my solution to 
eventually be able to fit into some of the cheapest featurephones on the 
market, with these official Chinese Nokias like 130-2023 being a good target 
as they are available internationally. Don&#x27;t get me wrong, they still are 
incredibly complex, but first, alas, only MIPS-based CT8851/SC6533G (which 
no major brand uses and they are being phased out) are simpler than that, 
and second, they still are orders of magnitude simpler than any smartphone 
hardware and corresponding firmware.

Maybe there really is no logic in this vision and doing all this. Maybe
there&#x27;s no logic in researches aiming to create my own firmware for such 
phones (with the planned application scripting language being based on TRAC 
T64 like nntrac). Maybe no one will appreciate it. But, as I said before, 
I&#x27;m sort of a dreamer, and this particular passion of mine takes a lot to 
put off.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-12-11-on-toki-pona.txt</link>
<title>(2023-12-11) There is something special about Toki Pona, isn't it?</title>
<description><![CDATA[<pre>
(2023-12-11) There is something special about Toki Pona, isn&#x27;t it?
------------------------------------------------------------------
As a wise man once said, &quot;you can learn anything in about 30 minutes, and
then spend a lifetime learning the details about it&quot;. This saying is more 
than true for any artificial language, especially for Toki Pona. Yes, I 
capitalize it although the name in the language itself is not capitalized as 
it&#x27;s not a proper noun and just means &quot;a good language&quot; or &quot;a simple 
language&quot; or, as its author, Sonja Lang, named her first book, &quot;the language 
of good&quot;. The first version of the language appeared in 2001 and it has been 
developed by the author _alongside the community_ since then, and in January 
2022 it finally got an ISO 639-3 code, &quot;tok&quot;. Also, Gemini network users may 
be familiar with the &quot;moku pona&quot; phrase, which is the name of an open-source 
news and phlog feed aggregator. This name means &quot;good food&quot; in TP, but 
&quot;moku&quot; also can mean other forms of consumption, so the name fits perfectly 
here. See, this is the main thing with this language: each core word, except 
special particles, has multiple meanings. This is what is called &quot;semantic 
space&quot;. And TP speakers operate with semantic spaces instead of exact 
definitions, like, all the time. And this is the most interesting feature of 
this language that became the primary reason I&#x27;ve continued learning it. For 
real this time.

This post is going to be rather short as I&#x27;m not going to turn it into a
basic TP guidebook (I guess Anglosphere has enough official, unofficial and 
not-even-remotely-close-to-official materials on it already), I want to 
share my own impressions about this language and some thoughts on its 
real-life usage perspectives in the nearest future. Because right now, TP is 
in its active growth phase, as well as the online community around it. It&#x27;s 
only a matter of time when serious players get interested in it as well. But 
let&#x27;s talk one thing at a time, okay?

So, onto my impressions. Well... Remember my stories about 58mm Victorinoxes
I carry around? Classic SD, Classic Alox, Rambler... Well, Toki Pona is the 
Classic SD/Rambler of languages. It&#x27;s extremely small, lightweight and 
somewhat cute yet extremely functional and reliable to get you throughout 
the day. The smallness of its core vocabulary (right now, the &quot;essential&quot; 
wordlist has 137 words but again, it&#x27;s up to the community to expand it if 
necessary) is fully compensated by the flexibility of the grammar and 
ability to use almost any word as almost any part of speech and any part of 
sentence. Yes, some types of compound sentences can&#x27;t be expressed in TP and 
you have to split them into individual sentences, but the other and most 
substantial part of compound sentences is totally possible using the 
ni-clauses and la-clauses. The latter ones, used to specify the context for 
the main sentence, are a very powerful feature on its own that eliminates 
the need in multiple tenses, adverbs, &quot;if&quot;, &quot;when&quot; etc. All it takes is 
just... adopting another way of thinking.

Yes, there&#x27;s that. Not only was the language developed to test the idea of
linguistic relativity aka the Sapir–Whorf hypothesis that says that the 
structure of a language influences its speakers&#x27; worldview or cognition, but 
it also forces _everyone_ who speaks it to bind their mind to think in 
semantic spaces and flexible vocabulary usage. Unlike e.g. Esperanto which 
is pretty much eurocentric, Toki Pona puts everyone in equal conditions. Its 
simplistic phonology was designed to be universally understandable across 
the globe, and the graphics is even better: besides the Latin alphabet 
(sitelen Lasina), there&#x27;s another, fully independent official hieroglyphic 
script called sitelen pona which has been already defined for all 137 
&quot;essential&quot; words and some extended vocabulary as well, and also has a 
unique way of writing proper names inside cartouches. Given that we 
currently have some fonts that can properly render all sitelen pona glyphs 
and there also is a standing UCSUR proposal to include them into Unicode, I 
guess this writing system has a bright future, at least it will be better 
than the unofficial and kinda cringey &quot;sitelen Emosi&quot; spec for encoding all 
the glyphs as existing emoji characters.

Speaking of future, I think TP has one. Its community is now more vibrant
than ever, its media coverage and the amount of speakers grow year by year, 
it got an ISO code and some popular things like Minecraft translated into 
it, it began growing its own unique cultural layer and, most importantly, 
its demographics began reaching beyond the first-world countries and conlang 
enthusiasts per se, giving it the potential to become (if not already) the 
second most popular constructed language in the world after Esperanto. On 
the contrary, other conlangs seem to be stagnating at best, only living in 
small conlang-related online groups and committees. Because, let&#x27;s face it, 
there&#x27;s no real demand for another eurocentric Esperanto replacement as of 
now, and as for non-eurocentric conlangs, which one is more appealing to the 
general public outside those groups — Lojban or Toki Pona? Especially if we 
consider they are pretty much opposite in almost every aspect, simplicity 
included: Lojban&#x27;s official grammar textbook is a 584-page PDF, while the 
TP&#x27;s grammar can fit into just one page ([1]), if we omit sitelen pona. Yes, 
you can learn Lojban if you&#x27;re a conlang enthusiast, maybe even in a 
relatively short period of time, but can you get a 5-year-old child or even 
any adult who&#x27;s not a conlang enthusiast to learn Lojban in a short period 
of time? My overall point is simple: languages don&#x27;t exist in a vacuum, they 
exist to serve a purpose of communication. So, if two people who don&#x27;t know 
each other&#x27;s languages want to talk, the total time they both need to spend 
on learning the third one (that would allow them to understand each other) 
must be drastically less than any one of them spends the time on learning 
the other person&#x27;s language. And in terms of the least time spent on this, 
Toki Pona may be the absolute leader and can be a real lingua franca for 
those who didn&#x27;t or couldn&#x27;t even learn English well enough, because, 
newsflash, English actually is pretty hard to learn, both by time and effort.

And like I said, it&#x27;s a matter of time before interested parties recognize
this potential in Toki Pona and try making some profit out of it. I hope the 
community won&#x27;t let it happen and won&#x27;t cede the control over the language 
to anyone in particular. Because the language of good must only belong to 
the people.

mi tawa.

--- Luxferre ---

[1]:
https://jansa-tp.github.io/tpcheatsheet/Toki%20Pona%20Cheat%20Sheet%20v2.pdf
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-12-04-i-am-a-dreamer.txt</link>
<title>(2023-12-04) I am a dreamer. Sort of</title>
<description><![CDATA[<pre>
(2023-12-04) I am a dreamer. Sort of
------------------------------------
I have a confession to make. Ever since I was a little child, I&#x27;ve been
constantly dreaming about something. No, not living in a fantasy world, as 
I&#x27;ve been fully understanding the surrounding reality and the dangers it 
carries. But I&#x27;ve always been imagining myself in various situations that 
might not seem realistic at the moment. Did any of those situations ever 
come true? Not sure, not that I remember of. But it always helped me relax 
and even fall asleep quicker, or, on the contrary, to focus on my primary 
goals. And for this purpose, it kinda worked when I was 5 and it still works 
when I am 32.

It is a delight lurking through Bongusta with my own Gopher browser
(Bopher-NG) and finding that more people are getting interested in LPC and 
other things similar to mine. Although I&#x27;m not even sure how many people 
here (among those who still use Gopher) also use mechanical watches, I have 
updated the LuxDocs section and published my complete Orient F6/F49 movement 
regulation guide there nevertheless. I hope it will be useful for someone 
like-minded who stumbles upon this place or the archive of my gopherhole 
wherever it might turn up in the future. But I also have a strong feeling 
that whatever my recent hobbies became, they still share one common goal 
across them: regaining freedom.

On YouTube (yes, I still watch it), someone said that there is a distinction
between digital minimalists and neo-luddites. As I saw this on YouTube, I 
can&#x27;t be the latter anyway, so I guess I&#x27;m closer to digital minimalism 
right now. It&#x27;s all about selective and limited use of only those pieces of 
technology that serve your own benefit and whose usage can be fully 
controlled. But yes, on the smartphone (which I still pretty much need for 
work and banking purposes, as well as e.g. mech watch regulation), I have 
migrated back from eSIM to a physical SIM. This gave me back the freedom to 
choose from a much broader spectrum of devices in an event something bad 
happens to this one, and also the feeling of security that I can just pull 
out the SIM instead of trying to recover it through the carrier office 
giving them much more of my personal data than I&#x27;m ready to. And as for my 
main/talk SIM that was faulty and behaved strangely on pre-2008 
featurephones, I had replaced it with a new one (again, using an official 
but fully anonymous procedure supported by my carrier) and now can freely 
use the rest of my collection with my &quot;talking&quot; phone number.

The same thing can be said about using notebooks and notepads more actively
than before: they don&#x27;t depend on electricity or online connection, they 
don&#x27;t have backdoors/trojans installed by vendors, they can&#x27;t be shut down 
or hacked remotely. Yes, they can be stolen physically but the &quot;interested 
parties&quot; don&#x27;t even know they exist, not to mention what&#x27;s in them. And if 
necessary, I live in a village so I can burn them down myself. Of course, 
you need something to write with in these notebooks. And this is why I 
switched to fountain pens: they are much more economical and ecological. And 
even when you run out of bottled ink (yes, I think that using ink cartridges 
kinda defeats the purpose), you can even make your own. By the way, I&#x27;m 
going to try out some ink recipes sometime in the future and will publish it 
on my Gopherhole if they are successful. But again, the point is that you 
are not limited to a certain pace of consumption anymore, you are free from 
constantly buying new disposable refills and throwing away the old ones.

This principle also can apply to how my watch collecting hobby has
transformed over this year. Yes, 10-year lithium CR batteries are great for 
watches, but first, the tendency is to phase them out in favor of 3-year SR 
batteries, second, they still are batteries that need to be disposed of 
after their runtime ends. So, whichever watches I&#x27;m going to buy further, 
they will be either solar-powered or automatic. Yes, the rechargeables in 
solar-powered quartz watches need to be replaced from time to time as well, 
but usually they last as long as the quartz crystal itself before it starts 
degrading, so I wouldn&#x27;t worry much about that. Automatic watches, on the 
other hand, need additional precautions and preparations but, if treated 
right, also can serve you for a long time and even be as accurate as the 
quartz ones if you regulate them correctly. That&#x27;s exactly why I, being a 
solar quartz advocate, have written such a comprehensive guide on how to 
regulate modern mechanical Orients to be as accurate as ±30 seconds per 
month deviation at most. But this is only my humble advice: either choose a 
solar quartz with the longwave and/or Bluetooth sync options, or choose an 
automatic that you are able to adjust at home and know its movement has good 
positional stability.

Last but not least, going back to phones, let&#x27;s not forget the hobby that
started it all: featurephone firmware research. Yes, it&#x27;s kinda stalled 
recently, mainly because I hit what seems a dead end in several areas: I 
haven&#x27;t yet found a way to reliably flash any memory block for MT626x (on 
normal OSes using open-source tools, I mean), I can&#x27;t find a way to decrypt 
half of the MT6261 firmware dumps and extract all secret codes out of them, 
I haven&#x27;t even found a way to make a handshake for MT6276, SC770x and 
UMS9117(L), and I can&#x27;t understand the internal structure of the SC6531x 
firmware too even after unpacking its partitions. And the META mode for 
MT626x and diagnostic/calibration modes for SC6531x/SC770x/UMS9117(L) remain 
complete black boxes to me as well, with no significant info about them 
whatsoever. Sad but true. I really don&#x27;t know where to go now to start 
untying this giant knot...

...But I am a dreamer. Sort of.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-11-27-got-a-tristar-what-now.txt</link>
<title>(2023-11-27) Got a Tristar... What now?</title>
<description><![CDATA[<pre>
(2023-11-27) Got a Tristar... What now?
---------------------------------------
The troubles at my workplace suddenly have returned, and I got some more
unwanted free time on my hands again. But the day that happened, I was at 
least rewarded with something: the Orient RA-AB0F09L arrived to me a day 
earlier than expected.

Yes, I did it, I bought a Tristar, as the Kamasu had shown consistent
performance for over 2 months not deviating beyond ±1 spd, which is fully 
satisfactory for a mech watch. But I really wanted to check on the legendary 
Orient lineup of the cheapest non-Chinese mechanical watches you can get 
these days, especially that now they too are produced under Epson&#x27;s control. 
Besides, the Kamasu is still a bit bulky and heavy (even on a 22mm mesh): 
not by a critical margin but that really is the limit of what I can put onto 
my wrist and still get away with wearing it on a daily basis. So, I might 
have postponed the inevitable but the reasons had piled up and I made the 
decision. And yes, the white-dialed RA-AB0F12S actually turned out to be out 
of stock when I ordered it, but the dark blue Vega, RA-AB0F09L, was 
available from another seller, and I thought it would be a nice choice as 
well to match the color of my Nokia 130-2023, my fleece and my winter coat. 
And you know what, I guess it was an even better choice than the white dial 
one, and I&#x27;m really impressed with it so far.

However, before I get to the details, I&#x27;d like to talk about this thing&#x27;s
philosophy. Yes, all things we can buy have some marketing image behind 
them, but not all of them have an idea behind them, and it is important not 
to mix these up. Especially for watches: all of them have their price, 
design and niche, but most of them do not convey a fundamental message about 
the principles they are trying to represent. In case of the Orient Tristar 
lineup, which is over 50 years old as of now, the main idea behind it is 
self-sustainability. Although the 1970s gave rise to quartz wristwatches, 
this lineup was specifically created to coexist with them. It was initially 
designed for the countries where people didn&#x27;t have access to batteries and 
watch service as a whole, so they got everything they needed right away when 
they bought the watch: date and day display, dustproof/splashproof cases and 
very reliable automatic movements that didn&#x27;t need any servicing for 
decades. The latter was an additional humongous advantage over the early 
quartz: remember that in the 1970s, even 2-year battery life was a lot for a 
quartz watch, usually it was less, and they didn&#x27;t run on a single battery 
either, two- or even three-battery configurations were common instead. Even 
the stock bracelet of the Tristars, despite its overall questionable 
quality, has always had _eight_ micro-adjustment positions so that anyone 
could fit it without removing the links so that the watch could be further 
sold or passed to other people basically intact. This mindset of 
self-sustainability is what made this lower budget lineup so successful 
around the world, and this is what I also resonate with in terms of reducing 
overall resource consumption.

Now, onto my own impressions. Of course, the first thing I did after the
unboxing was to take off that bracelet (which, for the Tristar lineup, is 
traditionally awful, but I wouldn&#x27;t keep it anyway) and put the watch on my 
20mm quick-release steel mesh strap. In fact, I think that steel mesh straps 
fit into the Tristar ideology even better, as they are just as durable as 
the regular ones but have none of their problems and can be adjusted to any 
wrist size much easier than any other bracelet type. They really transform 
the look of any watch instantly, not to mention they make it much more 
comfortable to wear. The dial itself is really dark blue but exhibits a 
gorgeous sunburst effect when put under a light source. Combined with the 
overall case shape, Kamasu-style indices, adequately sized hands that 
actually reach their markers and much stricter dial style than it was in the 
pre-Epson era Tristars, all this makes the watch look much more expensive 
than it actually is. Although it&#x27;s not as cheap as the earlier Tristars 
either, at least where I live. And for its price, I&#x27;d expect a bit more lume 
longevity, but I guess I might have been spoiled by the Kamasu&#x27;s one. The 
overall case size and weight of this watch is also much more suitable for 
day-to-day wear without even noticing it until you actually need to look at 
it. And when you do... oh man, it doesn&#x27;t disappoint.

&quot;But wait, there&#x27;s more!&quot; As you may know, all current Tristars (except some
Brazil-exclusive models and the RA-AK05 (Altair) which I don&#x27;t know what 
it&#x27;s doing in that lineup) run on the F4902 movement (code AB), which was 
marketed as the direct successor of the famous Orient&#x27;s 46943 movement (code 
EM), and, like the 46943, cannot hack (stop the seconds) and handwind. 
Normally, inability to stop the seconds would be a strong deterrent for me 
(e.g. it&#x27;s one of the primary reasons I don&#x27;t wear my Seiko SNK809 with its 
7S26), but in this case, I do have a strong suspicion that the F4902 has 
much more in common with the Orient&#x27;s F6 platform than with its predecessor, 
up to the point of this movement essentially being just an F6922 with extra 
details removed to cut costs and thickness, like the hacking lever and 
manual winding gears, but with the same positional stability and other 
timekeeping merits. The movements have the same manufacturer&#x27;s accuracy 
rating (-15/+25 spd) and look extremely similar too, except the F4902&#x27;s 
rotor is not decorated as nicely. And when I started the accuracy 
measurement, I began receiving some more confirmations of this suspicion.

The watch had been set and wound by the seller before sending it to me, but I
doubt they performed any additional regulation or opened the caseback at 
all. Nevertheless, the second hand was off by about 5 seconds from the 
reference time, so I recorded that when starting the measurements, and 
started recording the relative deviations every 24 hours. And... I can&#x27;t 
really say it&#x27;s performing any worse than the unregulated F6222 I have here 
(maximum relative deviation being +5.5 seconds per day, but it was a 
one-time fluke after a very active day, usually it&#x27;s much lower). Of course, 
too little time has passed, we&#x27;ll see which daily deviation it eventually 
settles on, but for now it might need just a tiny bit of intervention to run 
within the grail ±1 spd. But if all of this is true and F4902 is in fact 
nothing more than a stripped-down F6922, this means two things. First, it 
means that even if it settles on some large deviation like +8.5 spd my 
Kamasu had out of the box, then it can be regulated the same way as I did 
for F6922, and it can yield some nice and predictable performance results. 
Second, it means that F4902 is not just a direct successor of 46943, it is 
lightyears ahead of both 46943 and 7S26 because it in fact belongs to the 
newer F6 family which is much more stable than those ever could be. As the 
Tristar lineup was initially designed to compete with quartz watches in 
terms of affordability and reliability, it would be extremely cool if its 
newest generation can compete with them in terms of accuracy too by reaching 
under ±30 seconds per month deviation.

So, let the experiment begin. I hope it will be a success. Anyway, to me, a
mere possibility to make one of the cheapest Japanese automatic watches not 
only look not cheap but also be more accurate than most expensive Swiss 
automatic watches definitely is worth giving a shot.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-11-20-nokia-end-of-epoch.txt</link>
<title>(2023-11-20) Is this the true end of the epoch for Nokia phones? Not sure yet</title>
<description><![CDATA[<pre>
(2023-11-20) Is this the true end of the epoch for Nokia phones? Not sure yet
-----------------------------------------------------------------------------
Finally, something about phones on this phlog. Moreover, about phones of my
favorite brand. I could blabber about any (genuine) Nokias from 1993 up to 
2023 for hours but there&#x27;s a single thing I always lacked in them (except 
KaiOS models which compensate for it with plenty of other issues): ability 
to edit IMEIs without special boxes or proprietary software. This _might_, 
however, be about to change sooner than we think. A 30-year epoch of 
untouchable IMEIs in Nokia non-smartphones might come to an end in the 
forseeable future.

I personally thought it would never be possible until I realized that the
newest (2023-made and probably some 2022-made) Nokia feature phones (with 
non-smartphone hardware, that is) are no longer made in Vietnam like they 
were before. I also heard HMD had set up some local manufacturing for the 
Indian market, well, in India, but as I&#x27;m not there, this doesn&#x27;t concern me 
much. All current-gen Nokia feature phones produced this year onwards and 
sold where I live — 105-2023 2G, 106-2023, 110-2023 2G, 130-2023 and 
150-2023 — are coming to us from China. Yes, officially. These are not 
fakes. For the first time in history, we see official Nokia-branded keypad 
phones manufactured in mainland China. This was enough for me to pick one of 
them and see all the differences from the previous models. As HMD fully 
stopped announcing the chipset for their GSM-only models, it was a bit of 
hit and miss for me, but I ordered a new 130. And when it arrived, I was 
surprised it didn&#x27;t have Unisoc SC6531F or something like that (I know that 
the new 105 and probably 106 and 110 do have SC6531E inside), but was still 
running on MediaTek MT6261D. Anyway, this was only the beginning of my 
surprises.

So, here it is, the model TA-1576, Nokia 130 2023. In some regions, it&#x27;s
called 130 Music, and for the market of mainland China itself it was 
released as a 125 (probably because they totally missed the real 125... and 
for the greater good, I must add). It is quite a large barphone, 
dimensionally similar to the aforementioned 125 but a tiny bit thinner and 
narrower, which is a good thing in this case, and sporting yet another new 
removable battery type, BL-L5H with 1400 mAh rated capacity. As if the 
well-known BL-4UL wasn&#x27;t good enough for that. Also, it looks like they 
changed the UI font one more time, no one knows what for. Anyway, judging 
merely by this amount of NIH syndrome, I thought that the inside of the 
phone would be just as different. Oh, how damn right I was...

You see, I didn&#x27;t even have to dump the firmware with MTreader, as the main
partitions seem to be encrypted anyway, to see the obvious: this phone, or 
at least its board, was made in a totally different place. In case you 
didn&#x27;t know, all of the Vietnamese MediaTek-based Nokias (that were running 
on MT6260, then MT6260A and MT6261D) tried to conceal in every possible way 
they were MediaTek-based. The Series30+ was a major overhaul of MAUI, and a 
good one at that. Not a single MAUI-specific secret code, besides a couple 
of debug-oriented ones, actually worked on those devices. Microsoft and then 
HMD spent a lot of effort even to conceal the fact those phones had an AT 
command interface... Well, not for long, but those events are already 
history, and I&#x27;m glad I was a part of it. But the fact remains the fact: on 
those Nokias, you couldn&#x27;t even dial the *#63342835# (*#mediatek#) code and 
see the &quot;MediaTek&quot; word. If you could do this, it was an outright sign of a 
fake.

Well, guess what: on this new fully official and original 130, you can. You
also can enter an engineering menu with *#3646633# (*#engmode#), see the 
internal software version with *#8375# (*#ver5#), and yes, it&#x27;s a different 
screen than what you get on the traditional Nokia&#x27;s *#0000#, you can also 
enter a hardware test menu with *#15963#, or run quick tests with *#8378# or 
stress tests with *#87#. Again, I found all this without being able to fully 
analyze the firmware dump, but this was already enough for me to realize 
this firmware is much, much closer to the vanilla MAUI than any of its 
predecessors. I didn&#x27;t, however, find the *#15963# code randomly. The phone 
had a hidden clue where to start looking for it. But what I found is 
something more.

Any MAUI firmware version string, as you might now, contains a hardware
revision substring. Usually it&#x27;s an alphanumeric board identifier followed 
by the last three characters of the chipset model and then some other data 
after underscores. If you don&#x27;t have a way to view this information via 
codes (which I initially didn&#x27;t have), you can use various options for 
AT+EGMR subcommand. I ran the subcommand to get the board ID (AT+EGMR=0,4) 
and I saw the following string there: SAGETEL61D_11C_HW. This tells us that 
the chipset inside is indeed MT6261D and this is the revision 11C of the 
board codenamed SAGETEL. In fact, if we run an Internet search on the 
complete string, we already can find the device this board already had been 
used in: Itel IT2160, a barphone from Transsion released in ca. 2018. Of 
course, only the board is common with this Nokia but this inspired me to 
download some firmware for this IT2160 (which, of course, wasn&#x27;t encrypted) 
and check for some codes from there. And, bizarrely enough, *#15963# was the 
only new code that actually fit my 130-2023.

So, we have pure-MAUI secret codes for version, engineering and test menus
for this phone (and I&#x27;m pretty sure they are the same for 150-2023 too). The 
main mystery, however, remains unsolved: are the IMEIs here editable in any 
way? Well, my first thought would be to go the traditional AT command route 
(by the way, yes, you have to sacrifice the USB storage mode if you set the 
PS config to USB in the respective engineering menu setting). So I tried 
AT+EGMR with corresponding parameters (AT+EGMR=1,7,&quot;[new_imei]&quot; for SIM1 and 
AT+EGMR=1,10,&quot;[new_imei]&quot; for SIM2) but got &quot;CME ERROR: unknown&quot; in both 
cases, while the read commands (AT+EGMR=0,7 and AT+EGMR=0,10 respectively) 
do work fine. In the Vietnamese MediaTek-based Nokias though, the write 
commands worked too but the result was ignored due to the NVRAM protection. 
Here, it just looks like it was disabled on the AT command processor level, 
whether or not protection is still there, I don&#x27;t really know. Not gonna 
lie, if I find a working code for this, it will become my favorite post-2013 
Nokia. For now, I&#x27;m stuck. There is, however, some hope based on what I have 
seen with *another* NVRAM field in the same area: PSN.

PSN (product serial number) is the Nokia&#x27;s name of the internal serial number
that all phones like this have, be they on MediaTek or Unisoc. It&#x27;s assigned 
fully independently of IMEIs and, in case of MediaTek, can be accessed with 
AT+EGMR command too under the field #5: AT+EGMR=0,5 for reading and 
AT+EGMR=1,5,&quot;[new_sn]&quot; for writing. The biggest problem, however, is that 
the PSN itself takes 25 characters, but the NVRAM field for it reserves 63 
bytes and it actually is padded with whitespaces and ends with &quot;10P&quot; 
substring that&#x27;s not a part of the serial number per se. But that&#x27;s not all: 
it turns out that AT+EGMR command itself doesn&#x27;t check the input length for 
this field, so if you don&#x27;t include the padding, you can easily misalign all 
the subsequent NVRAM fields and mess up all calibration until you enter a 
63-character long PSN. And guess what: this field is actually editable and 
unprotected in this Nokia. So, in theory, by manipulating its contents, we 
could manipulate all fixed-length fields/files that come after PSN in that 
area. But that&#x27;s something that has yet to be investigated. For now, the 
IMEI question remains open.

From the normal user&#x27;s perspective, some very strange decisions had been made
there as well. For instance, this phone has absolutely no way of viewing 
images from the SD card and absolutely no way of setting them as wallpapers. 
I didn&#x27;t use any of the previous iterations of 130 and can&#x27;t say whether or 
not this is the case for them, but to me it sounds most illogical. Luckily, 
there are 6 pre-installed wallpapers, but what&#x27;s the reason to limit the 
choice if you definitely allow to set your own ringtones here? Although this 
is a strange one too — you can&#x27;t do this from the profile or general tone 
settings, only from the SD file manager itself. Same for message and alarm 
tone customization.

In the mass storage connection mode, the phone gets identified as &quot;0e8d:0002
MediaTek Inc. phone (mass storage mode) [Doro Primo 413]&quot;.  They didn&#x27;t even 
try to conceal anything at this point. But I also had a trouble connecting 
*just to this Nokia* in the Mass Storage mode from my Arch Linux (Garuda), 
until I found out I had to comment out the following line in 
/lib/udev/rules.d/40-usb_modeswitch.rules file:

ATTR{idVendor}==&quot;0e8d&quot;, ATTR{idProduct}==&quot;0002&quot;, RUN+=&quot;usb_modeswitch &#x27;/%k&#x27;&quot;

After that, everything went smoothly. Except, of course, the transfer itself
being extremely slow. If you have a card reader and have a large amount of 
music to move, it will be your best bet. And, besides music (+ custom 
ringtones and audio recordings), there&#x27;s pretty much nothing else you can 
use the SD card for in this phone (yes, even the phonebook VCard backup is 
scrapped). The player, by the way, is marketed as the central feature of 
this 130 and can be quickly entered by long-pressing the central D-pad key. 
On the first run, it scans the entire card for music files and generates the 
@Playlists/audio_play_list.sal file, whose format matches the one I 
described in my MAUI knowledge base ([1]). It also creates some temporary 
copy of this file&#x27;s previous version, audio_play_list.sal.tmp, and the 
MyFav.sal playlist file that reflects your &quot;Favorites&quot; player selection. As 
far as I have seen, the format of MyFav.sal is exactly the same, with the 
only visible difference being that the &quot;Favorites&quot; entries end with the 01 
00 bytes instead of 00 00 bytes in both audio_play_list.sal and MyFav.sal 
files.

Out of all this, what conclusions can I make? Is this really the end of the
epoch for Nokia featurephones? Not quite yet, but it is very close to that. 
I mean, it still is a genuine Nokia, cased into very hard polycarbonate 
plastic, having some IP52 dustproof rating, booting extremely fast, offering 
good sound capabilities and (I suppose) not having any trojans in its 
firmware. But in terms of how this firmware differs from all other 
China-originated phones (and we see a proof that the hardware literally is 
the same as one of them), the difference is almost non-existent now. And the 
only thing that globally keeps this firmware from being fully identical is 
not the S30+ UI on top of MAUI, it is the uncertainty about whether or not 
boxless/dongle-less IMEI editing is possible here. That&#x27;s why my research in 
this area needs to continue, regardless of how long the pause has been.

--- Luxferre ---

[1]: gopher://hoi.st:70/0/docs/own/maui-kb-mt6261.txt
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-11-13-small-events-big-impact.txt</link>
<title>(2023-11-13) Small events, big impact</title>
<description><![CDATA[<pre>
(2023-11-13) Small events, big impact
-------------------------------------
This is gonna be a rather short one. My last week consisted of several small
events (mostly on the weekend) that, however, have had a significant impact 
on my perception of the outside reality during these harsh and rainy autumn 
days.

First, I took part in two small contests in a local poetry group. As a result
of what I heard regarding my two contest poems, I decided to finally do what 
I planned on doing after these contests: to create my own online Ukrainian 
poetry community. We did this with a close friend of mine, who&#x27;s a 
philologist by education and a poet by hobby too. For now, this community 
has a Telegram channel and a dedicated closed chat, but there are some plans 
for expanding to Matrix and other networks once it gets some traction. This 
activity might be small on the larger scale of things, but this step had to 
be taken and is very important for me personally.

This week, I also extensively tested the Pilot MR (aka Metropolitan or
Cocoon, but this one is the Euro version) fountain pen among others and 
became fairly satisfied with it as my primary EDC pen for the next period of 
time. Its EF nib is perfect for jotting down quick notes in the pocket 
notebook, and the pen isn&#x27;t any longer than the notebook itself. Unlike some 
reviewers, I don&#x27;t experience any discomfort with the grip section and with 
the big step down from the barrel to the grip, moreover, this big step 
allows the pen to look extremely sleek and seamless in the capped state, 
unlike many more expensive pen designs like the famous Lamy Safari (if we&#x27;re 
talking more expensive, that would be Lamy Al-Star) or even Lamy 2000. As I 
said before, I&#x27;m not falling for the &quot;entry-level&quot; trap and fully enjoying 
the experience of what I have here, knowing that Pilot can deliver quality 
at any price range. Probably the only more expensive fountain pen I could 
ever spend anything on would be Pilot Capless (aka Vanishing Point), but 
when and why it&#x27;s going to happen, is a completely different story.

Another &quot;small but big&quot; thing is that I have replaced the faulty SIM card on
my non-internet number (purely for talking), that prevented old devices 
(pre-2009 or so) from even receiving SMS messages, with a new one (my 
carrier actually supports this process even for prepaid SIMs without having 
to disclose any additional personal data to them). The impact here is that I 
can now actually use the older half of my collection: so many wonderful 
phones, especially old Nokias (1112, 3100, 8310, 6310i, 8850 etc), that I 
actually have all the chargers and working batteries for, finally can be 
used with my main talk SIM. Of course, they are not gonna offer 3G voice 
quality (only the AMR-NB codec, although most of them only support GSM-EFR) 
or any advanced multimedia features, but that doesn&#x27;t bother me at all. In 
fact, not having a memory card or a camera is an advantage for me, and being 
monochrome is a double advantage. I do wish, however, that someone would 
release a *small* feature phone (like Nokia 1112) with no cameras but with a 
true monochrome display *and* UMTS/LTE connectivity. Oh, such dreams, such 
dreams...

So, despite how tough the overall situation here is, life is somehow going
on. And it really is made up of such small things that still can being us a 
piece of joy and hope.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-11-06-entry-level.txt</link>
<title>(2023-11-06) The myth and the curse of "entry-level" products</title>
<description><![CDATA[<pre>
(2023-11-06) The myth and the curse of &quot;entry-level&quot; products
-------------------------------------------------------------
Just like the misconception of &quot;user friendliness&quot; had plagued the software
world beyond repair, it appears that a similar misconception has been 
dominating the realm of material goods all this time. Whenever I stumble 
upon a review of some item, say, a watch, a knife, a pen, a phone etc, more 
often than not I see/hear the phrase &quot;entry-level&quot; or &quot;starter&quot; or 
&quot;beginner&quot; applied to it. One recent occasion of this became my tipping 
point after which I decided that I no longer can hold this to myself.

As I said in my previous phlog entry, I was going to have some update about
my EDC pen arsenal. It still is underway, and it&#x27;s pointless to brag about 
all the options I have tried so far, but my next step is going to be a Pilot 
MR from Animal series, Euro version. In case you didn&#x27;t know, this version 
takes &quot;international&quot; standard cartridges and converters instead of 
proprietary ones from Pilot. Why have I ordered it? Well, that&#x27;s one of the 
cheapest all-metal FPs that are not a part of Chinese QC lottery and that do 
have F/EF nib options (I ordered the EF, of course), and I really do need 
all-metal pen body after realizing what happened to the plastic barrel of my 
Parker Jotter CT FP. Why didn&#x27;t I order the Jotter SS FP instead? Because it 
turned out to be much more expensive than Jotter BP and the Pilot MR here, 
that simple. But I&#x27;m keeping this in mind as a last-resort fallback option. 
Why Animal series? Well, that&#x27;s even simpler: because it&#x27;s the only MR 
series I could currently find where I live, even online. But the 
black-on-black &quot;crocodile&quot; strip pattern doesn&#x27;t bother me at all.

Now, here is the real kicker. I have ordered this fountain pen on Saturday
(and, of course, this means they will only react to my order today and I&#x27;ll 
get it tomorrow or even on Wednesday (if not on Thursday), so, while waiting 
for it, I decided to read and view some reviews both from regular users and 
&quot;professional&quot; pen reviewers. Most of them are about the US and/or Japanese 
versions with proprietary refills, and I thought &quot;well, OK, not a big deal, 
the rest of the pen is still the same&quot;. But I kept seeing/hearing how each 
and every one of these reviews said how good this pen was _as an entry-level 
fountain pen_. Well, at first I didn&#x27;t pay much attention to this, but then 
a thought crossed my mind: &quot;an entry-level pen with proprietary refills that 
costs about 30x Centropen Student? Something is not right with this term&quot;. 
But then I remembered how most reviewers called Orients &quot;entry-level 
automatic watches&quot;, Victorinoxes &quot;entry-level knives&quot; and so on. And 
everything fell into its place. It has nothing to do with quality, usability 
or affordability. It is pure marketing once again.

You see, if you, as a reviewer, publicly call Centropen Student an
&quot;entry-level&quot; fountain pen (which it actually is, as it&#x27;s perfect for 
beginners and students to try them out without spending a lot), then you 
will have to admit that Pilot Metropolitan/MR no longer is one. You&#x27;ll have 
to call it a &quot;mid-range&quot; fountain pen, which it really is price-wise (by the 
way, doesn&#x27;t MR stand for &quot;mid-range&quot;). I&#x27;d even say &quot;upper-mid-range&quot;, but 
that would mean that the prices for even more expensive pens are even less 
justified. And that conclusion is perfectly fine for me as a normal user, 
but not for the reviewers who often shamelessly advertise much more 
expensive items and get their share of profit out of it.

But even this Overton window sliding is not the main issue with the
&quot;entry-level&quot; term. The main issue is a subconscious attempt to program your 
mind to buy more even if you are fully satisfied with what you already have. 
By labeling an item as such, they are trying to say something like &quot;this is 
just the beginning of your journey, you are going to find more substantial 
items as you grow and delve deeper into it, you are not going to use an 
entry-level product for the rest of your life, are you?&quot; And a lot of people 
really fall for this shit and give up their money to own something that&#x27;s 
not labeled &quot;entry-level&quot; without even asking who labeled it and why. I 
guess I own a lot of such &quot;beginner&quot; items and they do their things well, so 
why would I even want to change them? Heck, I even saw some YouTube reviews 
that call Orient Kamasu (RA-AA00..) &quot;an entry-level automatic dive watch&quot;. 
Yes, they said this about a Kamasu, not a Tristar. Look around you, how many 
people are going to pay even that amount of money for a mere mech watch in 
2023? How can you call it &quot;entry-level&quot; at all? But no, they know what 
they&#x27;re doing. They are trying to set your brain to anticipate the &quot;growth&quot; 
you never need in the first place and to come to them once again for a 
&quot;higher-tier&quot; Seiko, Citizen or some Swiss brands (almost all of which are a 
pure ripoff to begin with). Combined with FOMO and planned obsolescence, 
this is the third consumption-driving trick being played on you these days.

Don&#x27;t fall for it.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-10-30-edc-fad.txt</link>
<title>(2023-10-30) On EDC, one of the greatest fads of our time and place</title>
<description><![CDATA[<pre>
(2023-10-30) On EDC, one of the greatest fads of our time and place
-------------------------------------------------------------------
Maybe I&#x27;m getting too old and/or too wise to understand what&#x27;s really going
on, but I just don&#x27;t seem to get it. Since when did fashion trends start 
dominating the everyday carry items market? Not even in terms of how they 
look, but in terms of what should be paired with what... And yes, by color, 
weight and other totally irrelevant BS. As if we are talking about some 
haute couture and not friggin&#x27; tools. Yes, this is yet another post about 
material goods, but I always thought EDC was philosophy-first.

Apparently, I&#x27;m the only one left who still thinks that way. Others on teh
internets gladly spend tons of money on pretty useless titanium knives, 
Knipex pliers, &quot;special&quot; pocket gear pouches, tacticool pens and other 
nonsense to show off and honestly never use IRL anymore. I must be from a 
different planet as my EDC (when I&#x27;m outside of the house) usually consists 
of a watch, two phones, two Victorinox SAKs (usually, that&#x27;s Spartan + 
Rambler), a wallet (that also has a SwissCard and a cardphone with a 
backup/emergency SIM), a keyholder (with a Nitecore Tiki flashlight and a 
Rodenstock microdriver on it), a lighter (usually Bic but sometimes Zippo 
205) and a pocket notebook with a pen attached to it (nowadays it&#x27;s one of 
the Parker Jotters, but I&#x27;m gonna have a slight update on this soon). When 
appropriate, sunglasses also are added. And that&#x27;s it. And even this I 
consider a bit excessive from time to time. Inside the house I&#x27;m now living 
in or its territory, I only move around with the phones (although sometimes 
I omit them too), the notebook + pen combo, a pocket flashlight (usually 
Brennenstuhl PL 200 A) and the Victorinox Rambler, and, obviously, with the 
watch I&#x27;m wearing 24/7 anyway. Optionally, either wired (JBL T205) or 
wireless (Aftershokz Aeropex) headphones are also added to this setup from 
time to time, although this is something I&#x27;m still deciding upon.

I&#x27;m writing all this not to boast about my gear, but to emphasize on the fact
I&#x27;m fully satisfied with it. I know that most, if not all, of these things 
are going to last me for a very long time. And my set of tools still is 
quite redundant in some areas, which I don&#x27;t mind for now but am ready to 
drop if deemed necessary. So, for all the EDC boasters on the YouTube, I&#x27;m 
not even a potential buyer. I&#x27;ve already bought all I needed long ago. If 
something gets lost or broken, I&#x27;ll buy the same item (or a similar one if I 
can&#x27;t find the same). E.g. if I lose my Rambler, I won&#x27;t buy your $200 
ultralight knife from a brand no one knows the truth about, or the 
Leatherman you&#x27;re trying so hard to sell, hell no, I&#x27;ll buy another Rambler 
(maybe in black since they have started importing it here). Just because its 
Philips screwdriver is so damn useful that I have actually screwed more 
Philips screws with it than I had with a dedicated tool. Just because it&#x27;s 
not trying to pretend to be anything it actually isn&#x27;t.

As a wise man once said, &quot;carry what you need and need what you carry&quot;. Don&#x27;t
turn this thing into a fashion charade. Please.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-10-23-several-new-things.txt</link>
<title>(2023-10-23) Several new things and my first impressions about them</title>
<description><![CDATA[<pre>
(2023-10-23) Several new things and my first impressions about them
-------------------------------------------------------------------
Today, I want to share my experience with some of the new things I&#x27;m in
contact with right now. The first thing is that my work conditions finally 
got fully stabilized, and now I can announce which C replacement language I 
started to learn. You might think it&#x27;s Go, but no, it&#x27;s Nim. I got over the 
fact it&#x27;s using transpilation via C, as it still proven to be rather 
efficient and allows to use a lot of various backends (not to mention an 
independent JS backend as well, which I&#x27;m still yet to find a good use for). 
Of course, the best learning path is via practice, so I decided to rewrite 
one of my existing C projects in Nim. I had a hard time choosing which one, 
but then the choice turned out to be the nne text editor. So yes, the 
current nne version development has stopped, and the next one will be a full 
rewrite and it&#x27;s going to have a bit different name. I&#x27;ll announce which one 
when I have at least a beta version of this new editor to showcase. Don&#x27;t 
expect fast progress though as I still have much to learn about Nim, but I 
definitely expect the new editor to have much less SLOC count than the 
original 777-line nne. The language itself does look like Python to some 
extent, but actually not as much as I thought at first. My overall first 
impressions are positively positive and I as of now I think Nim has a 
humongous potential as a universal, all-target system programming language 
of the future. Let&#x27;s see if these impressions hold over time when I finish 
rewriting my text editor in it.

The other two things I got on the past week were two Parker pens I already
told you about in the previous post: the Jotter SS BP and Jotter CT FP F, 
where BP and FP stand for ballpoint pen and fountain pen respectively. The 
ballpoint Jotter is an all-time classic, and I can understand why. Even 
though the body is made of stainless steel, it does have some plastic parts 
internally, although this fact doesn&#x27;t bother me at all. It takes ISO 
standard G2 refills (not to be confused with Pilot G2), and although the 
black QuinkFlow refills I bought are not bad, they are not great either, so 
I&#x27;ve yet to find the best ones that work for me. As for the fountain pen 
version, although I said I wasn&#x27;t planning on using any disposable 
cartridges, I did start with the long black one included with the pen itself 
(and I&#x27;m also probably going to refill it too from time to time, besides 
using the converter). The writing experience with the fine nib is totally... 
well, fine, I think it&#x27;s a decent replacement for my seemingly neverending 
one-time-use Aihao AH-2005 needlepoint rollerball pen. Although you have to 
be mindful of the nib position all the time, e.g. you can&#x27;t rotate it like a 
normal pen, this is pretty much the only (and small) inconvenience that 
doesn&#x27;t outweigh its advantages. Now, I&#x27;m pretty much EDC-ing it alongside 
the Jotter BP, and haven&#x27;t been dissapointed with it ever since.

Another thing that&#x27;s not new (I&#x27;ve had it for quite a while as of now), but
was hardly ever used due to various reasons, is now in my EDC rotation as 
well. And that&#x27;s a Zippo lighter, the cheapest one with a classic shape and 
a stock liquid fuel insert. I don&#x27;t even know what this finish is called — 
&quot;street chrome&quot;, &quot;satin chrome&quot;? Whatever. I only collect watches and 
featurephones (although now even that&#x27;s debatable), not Zippos. I also have 
a small can of genuine Zippo fluid, but the main reason I hadn&#x27;t been using 
it was because it evaporates too quickly for my very infrequent usage (on 
the contrary, my disposable isobutane Bics seem to last forever). However, 
as I stumbled upon a video on how to properly repack the wick and use some 
plastic packaging to seal the bottom of the insert, I decided to give it a 
try too. The repacking part boils down to the following tips: unscrew the 
flint (don&#x27;t lose it!), remove the felt, remove the cotton and the wick, 
bend the wick in the S-shape leaving only the tip straight, reinsert the 
wick with the straight tip up, straighten the cotton and then repack it in a 
way so that the wick sits exactly in the middle of the case. Then, you can 
return the felt to its place, or just omit it if you use the plastic bag 
trick (I used a scotch tape for the first tryout). Essentially, you fill the 
cotton with the fluid and then seal it off with a piece of a plastic bag or 
tape. You can wrap some more tape around the insert, just make sure it still 
fits tight in the casing. All this leads to much less fluid evaporation and 
much better wick saturation so that it lights at the first strike every time.

And now, to end all this on a bit more philosophical note, I&#x27;ll answer your
possible concerns that, for an anticonsumerist and an LPC afficionado (what 
a word...), I talk too much about various material stuff: watches, phones, 
pens, lighters, calculators, slide rules... Well, that might be true, 
probably too much. Moreover, I wholeheartedly agree with the famous saying 
from the Fight Club: &quot;the things you own end up owning you&quot;. But the real 
and pretty much horrible truth is, we live in a world saturated with crap. 
And the world&#x27;s gigantic marketing machine is working 24/7 to make you buy 
this crap over and over again. And, unless you become enthusiastic at least 
to some extent even about the simplest everyday things like pens, unless you 
deep-dive into the topic and learn to distinguish that crap among the goods, 
you are going to be fooled into buying it, just to have to pay even more in 
the future. And it&#x27;s not like the realm of software, where almost everything 
becomes clear just by the fact whether the source code is open or not and 
whether normal OSes are supported or not; here, the low quality is concealed 
with much greater effort in order to be able to sell you more and more. So, 
whatever I&#x27;m writing about those material things here, is a part of my 
honest pursuit of everyday goods that would last a lifetime and perform 
their functions well enough to not be further replaced with anything else, 
exactly to *stop consuming* in that particular direction. It is not in any 
way the philosophical foundation of my life, but merely an important part of 
it.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-10-16-docker-gopher-analogue.txt</link>
<title>(2023-10-16) Docker, Gopher and analogue lifestyle</title>
<description><![CDATA[<pre>
(2023-10-16) Docker, Gopher and analogue lifestyle
--------------------------------------------------
Yes, I know, I still haven&#x27;t put everything together at my workplace, so in
the last week I still was a bit exhausted by that situation. However, it&#x27;s 
almost over, so I&#x27;m going to continue with my language challenge as soon as 
I really can. This doesn&#x27;t mean, however, that on that week I didn&#x27;t have 
anything else to do or think about.

Going back to OCaml and the Gopher server I wrote in it, I should note that
even the one I replaced with it (my mod of Gofor) wasn&#x27;t dockerized. And it 
worked perfectly fine, after all, it&#x27;s a single Python script. Now, I 
switched to a static binary. And here&#x27;s another phenomenon that bothers me: 
nowadays, lots of server applications are built from Go/Rust/Nim/whatever as 
static binaries for easier distribution, yet people still dockerize them for 
some strange reasons. Of course, sometimes the reasons are not so strange, 
e.g. when you have to run multiple instances of the same server binary with 
different configurations but the binary itself only expects a config file at 
a single place, but that&#x27;s not so common and is a sign of straight bad 
software design. All normal server binaries have command line switches to at 
least point them to a proper configuration file, and then unroll everything 
else from that file. Most of them even allow you to fully duplicate the 
configuration in the switches themselves, although that&#x27;s not very practical 
except for some short-term testing. Another use case would be when it&#x27;s 
absolutely necessary to put the server behind a reverse proxy, for instance, 
to handle SSL or some other traffic manipulation. I&#x27;m doing this myself on 
my VPS for a lot of services, but no need to do this for Gopher. And 
besides, I&#x27;m using a Compose+Traefik combo just because it makes things 
easier and quicker to set up from third-party distributions, not because I 
couldn&#x27;t do it without containers in an oldschool way by manually assigning 
ports and limiting bind addresses to 127.0.0.1. So, the point is: learn to 
use containers only when there is a real business need in them. In all other 
cases, keep things simple, especially when dealing with static binaries. If 
you feel the urge to multiplex, sometimes the xinetd/tcpsvd approach is the 
best one.

I&#x27;m also thinking whether or not to add some pluggable CGI functionality to
my next Gopher server iterations (if there are any) for search selectors and 
so on. One part of me says it can introduce a huge security risk. Well, 
every CGI-enabled server carries some security risk. But of course, we can 
always drop the privileges and require that these scripts run under a 
non-root user that can only access a single directory and everything under 
it. Another part of me says that this is pretty much the only thing left to 
create a complete Gopher server (I don&#x27;t count various Gopher+ extensions, 
that&#x27;s not what I&#x27;m interested in implementing anytime soon), the other 
major thing being TLS support but that&#x27;s a different story. Either way, all 
this involves development of some configuration format and routing engine, 
so it might take some more time than I initially thought. Maybe, someday...

The Orient (RA-AA0001B) is running just under +8 seconds in 15 days of
accuracy measurement after the regulation. If this translates to +16 s/mo, 
I&#x27;ll be more than pleased. If the deviation at the end of the month is even 
less, it&#x27;s going to mean that this automatic Orient is on par with most 
quartz Seikos. Just reminding once again that my Casio W-800H, in the same 
24/7 wearing mode, runs -24 s/mo. Of course, due to DST change at the end of 
October, I&#x27;ll have to stop my measurement a bit earlier than that (Oct 29), 
but I guess the picture will be clear nevertheless. And again, I think I&#x27;m 
going to wear this watch at least until the end of this year, unless I go 
crazy and buy the RA-AB0F12S as I already said. Speaking of which, I think I 
also understood something really important about that Tristar lineup. First 
of all, yes, the 90% of it looked really ugly, but now, under full Epson 
control, they became quite decent-looking (aside from the bracelets but I 
don&#x27;t count them as a factor at all, I got used to swap them on day one). 
Second, regardless of how they look and can&#x27;t be accurately set (because 
F4902 is non-hacking and the second hand never stops there), they were 
designed for the countries and environments where little to no watch service 
is available for most people (that&#x27;s why that lineup is still a huge deal in 
Brazil, for instance) and this means you can, again, easily regulate them at 
home and then wear for decades to come with no worries, provided you don&#x27;t 
submerge them in any liquids. Combined with their pricing (at the target 
markets, not our crazy dealers), this makes for a really affordable and 
reliable EDC mech for those living more of an analogue lifestyle.

Among the other attributes of analogue lifestyle, something I never touched
before is good stationery. Of course, I don&#x27;t consider my current stationery 
bad (otherwise I wouldn&#x27;t be using it) but by &quot;good&quot; I mean &quot;lasting 
longer&quot;. Not BIFL by any means, but closer to that. For instance, last week, 
besides that Pininfarina &quot;eternal pencil&quot;, I got two notepads and a pack of 
12 automatic pens. The pens (Axent Reporter AB1065-A) are not disposable, 
you definitely can unscrew them and replace the ink refill (it&#x27;s a thinner, 
Chinese-type refill with spring holder flaps), and they also write perfectly 
fine with their 0.7mm thickness and are very comfortable to hold despite 
being thin (because of the soft-touch plastic material), but their mechanism 
is still prone to breaking, the refills can be hard to find and they don&#x27;t 
feel like they would last long enough. On the other hand, I have two 
&quot;tactical&quot; pens, one of them being a Sigma-branded Chinese OEM pen (mostly 
known as Sminiker Professional Defender), another one being a less 
intimidating Nitecore NTP21. Both of them take standard &quot;Parker-style&quot; G2 
ink cartridges (normal ballpoint or gel), with the preinstalled ones being 
kinda meh in terms of quality (even though the one in NTP21 is a Schneider 
Gelion+). &quot;These are more like it&quot;, I thought. There are, however, some 
places where tactical pens are explicitly prohibited too, so I thought, &quot;why 
not order a normal steel Parker Jotter?&quot; Well, this week it comes, along 
with a pack of 6 G2 black refills that I&#x27;m going to use for the other two 
pens as well, and I&#x27;ll definitely have something to say in my next posts 
about them all.

This, however, wasn&#x27;t the only pen I have ordered in addition to my initial
demand. You see, with the (mostly) metal pens and G2 standard refills we no 
longer have a problem with disposable pens polluting the environment, but we 
still have the same exact problem with disposable refills. And you can&#x27;t 
refill these cartridges at home — even if you could, most of the ink you can 
have at hand just won&#x27;t work properly because it won&#x27;t have the right 
viscosity and other features required specifically for ballpoints. Well, 
guess what? Fountain pens to the rescue! They can take any type of ink as 
long as it&#x27;s water-based (not pigment- or oil-based), and you can repair or 
replace their individual components if you need to. Of course this involves 
some learning curve, but I&#x27;m not afraid of it. That&#x27;s why, along with the 
ballpoint Parker Jotter version, I ordered a fountain version too. Good 
thing that I looked up some information online and found out that the 
fountain Jotter uses proprietary ink cartridges instead of a normal 
refillable ink chamber, so I also had to place a compatible ink converter 
into the same order. Because, of course, I don&#x27;t plan on using any 
disposable cartridges in a fountain pen — that would kinda defeat the whole 
purpose. But I also don&#x27;t like how thick most modern fountain pens are, so 
I&#x27;m glad that the FP Jotter maintains roughly the same dimensions as its 
ballpoint counterpart. We&#x27;ll see how it performs over time though. And yes, 
I plan on experimenting with every kind of water-based ink I can get my 
hands on: HP inkjet printer ink (from a local third-party supplier, of 
course), liquid food coloring, leaves, walnuts and so on. Because I want to 
evaluate how versatile this kind of pens really is and whether or not it&#x27;s 
worth all the hassle before making any bold conclusions. So it&#x27;s going to be 
one of my future topics in this phlog as well.

But... Challenge first. Just need to come up with an idea of what exactly to
write in the new language.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-10-09-calming-down.txt</link>
<title>(2023-10-09) Calming down</title>
<description><![CDATA[<pre>
(2023-10-09) Calming down
-------------------------
I&#x27;ve had an extremely tense week. So tense I actually couldn&#x27;t focus on any
constructive activities (except maybe writing another poem with a bit of a 
trolling mood — yes, I do write poems, mostly in Ukrainian, sometimes in 
English). I really feel like the most productive time of the year is 
effectively being stolen from me, which is very disappointing. That&#x27;s why I 
had to postpone my new C replacement language challenge once again. 
Hopefully I&#x27;ll finally be able to dedicate this week to it.

So far, the Orient RA-AA0001B has been performing brilliantly: in 8 days
after the regulation session, it was 6 seconds fast. If this behavior 
continues, it means +0.75 spd that can be projected onto +22.5 s/mo, which 
would mean a mech more accurate than my Casio W-800H which is 24 s/mo slow. 
I&#x27;m also *thinking* about ordering something of the Orient Tristar line, 
with the most accessible variant (where I live) being RA-AB0F12S (aka Orient 
Vega White). But still not sure whether or not I really need another 
non-hacking automatic watch when I already have a 7S26-based SNK809 that 
annoyed the heck outta me with its inaccuracy (I didn&#x27;t try regulating it 
though because I didn&#x27;t have proper tools back then). Maybe just out of 
curiosity on how F4902 compares to 7S26, I&#x27;ll buy this Vega and try 
regulating both of them. But I still haven&#x27;t made a decision about this: 
random shopping for stress relief is definitely not the way to go. Besides, 
I don&#x27;t have a single complaint about the Kamasu.

I have ordered some stationery nevertheless: a desk organizer, two A6 pocket
notebooks, a bunch of cheap automatic pens and... an expensive pencil 
(Pininfarina Prima) with an extra Ethergraf tip. Still not sure whether or 
not it finds a good use in my scenarios, but yes, I&#x27;m really thinking of 
getting my lifestyle even more analogue than it is right now. It&#x27;s not even 
about just power consumption reduction or Internet independency anymore, 
it&#x27;s about the peace of mind. The very peace of mind which is really hard to 
preserve in this time and place without deliberate efforts to restrict the 
flow of outside information through your own brain cells. This means: ditch 
the news (and if you can&#x27;t, and in our situation here I really can&#x27;t, then 
restrict the time to read them), don&#x27;t feel obliged to instantly respond to 
every message you&#x27;ve received (except the direct messages or mail from the 
people who actually put their trust in you — I think it is kinda immoral to 
break it, be it personal or business relationships), only reach out to 
public places (especially online) when you have something important to say 
or share yourself, not just consume the stream of data from those places, 
and learn to take advantage of information underconsumption by directing the 
released brain resources to something creative. And in my case, since I 
don&#x27;t have a lot of possibilities to make music in my current living 
conditions, I can focus on writing instead.

This doesn&#x27;t mean the Internet can&#x27;t be useful for peace of mind though. Even
the &quot;big Web&quot;, not only Gopher or Gemini. For instance, recently I had 
stumbled upon an article that described how Orient had established a new 
plant in Brazil (the city of Manaus) in the 1970s, and now that plant 
manufactures a lot of Orient watches that can&#x27;t be found anywhere else in 
the world, and even have a different model code designation scheme. 
Apparently, the Tristar lineup still is a very big deal there, and the 
current Brazilian models of this lineup all start with the F49 code, which 
means the F4902 base movement or its variations. And the difference between 
what they have and what the rest of the world has in this lineup seems like 
night and day. And it looks like they are cheaper than the international 
models too, but only available locally. It&#x27;s like JDM watches, but on the 
other side of the pricing scale. Another article on the same website 
described the history of Orient&#x27;s 46 movement family, and how they became 
what they are today (F6xxx). This article also hinted about the further 
developments of more accurate F7 and F8 movement series, the latter having 
silicon balance/escapement parts and greater power reserve. It&#x27;s only a 
matter of time when these movements are going to appear in more affordable 
watches. And it&#x27;s always pleasant to see when the Japanese offer a piece of 
serious engineering and not just marketing, be it mechanical or quartz.

So, this is it. Calming down is important. And I don&#x27;t even know yet when
I&#x27;ll be able to return to the normal work process, but I really hope it will 
be this week.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-10-02-can-a-hackers-watch-be-mech.txt</link>
<title>(2023-10-02) Can a hacker's watch be... mechanical?</title>
<description><![CDATA[<pre>
(2023-10-02) Can a hacker&#x27;s watch be... mechanical?
---------------------------------------------------
In case you didn&#x27;t notice, I have added an instruction on DIY micro notepad
to the LuxDocs section. This is just something I wanna preserve from my 
chronovir.us blog, as well as some other useful things from there (stay 
tuned for updates). Anyway, yes, I had touched some fully analogue topics 
there, as well as the topic of what can really be considered a &quot;hacker&#x27;s 
watch&quot; versus what the mass culture tries to position as such. There, I also 
performed some calculations I really don&#x27;t want to describe now, but the 
bottom line was that any watch can store some additional bits of data if 
treated properly. But here, I want to touch another aspect of the &quot;hacker&#x27;s 
watch&quot; concept, that is, what if we just use it as a watch and not try to 
squeeze anything else out of it? Can even an automatic watch qualify in this 
very case?

From the philosophical point of view, having a mechanical watch does indeed
go well along with the LPC ideology — it is fully autonomous, never needing 
to charge or change batteries, at the cost of requiring more attention 
throughout the year due to reduced accuracy and having to adjust the date 
every two months. Also, it can be vulnerable to strong magnetic fields but, 
on the other hand, fully immune to EMP attacks. Obviously, a hacker&#x27;s watch 
should be rugged, functional and accurate enough for its intended usage. 
This is why, if we&#x27;re talking mechs, it should have a decent amount of water 
resistance (screw-down caseback and screw-down crown), scratch resistance 
(sapphire glass and no domed crystals), strap versatility (even lug width), 
good amount of lume on hands _and_ markers, a chronograph or at least a 
rotating bezel and — most importantly — a reliable automatic movement that 
can be regulated at home, obviously with the second hand hackability and 
handwinding option. Who does all this for the price that doesn&#x27;t make you 
cry if anything goes wrong? Orient does, for one.

Not to say the Orient RA-AA0001B (aka Kamasu Black) isn&#x27;t heavily overpriced
where I live — well, it is, but I bought it without a slightest hesitation 
when I saw it selling here. Yes, replacing the bracelet to a 22mm steel mesh 
strap was the first thing I did, because the stock one made it so heavy for 
me I didn&#x27;t even bother downsizing it, but other than that, it&#x27;s a step up 
compared to Invicta 8926OB in every aspect: the crystal (sapphire vs. 
mineral with that stupid date cyclops), the bezel action (much smoother than 
in the Invicta), the lume (really shines all night through), the bilingual 
weekday display (as someone who&#x27;s learning Spanish right now, I really 
appreciated it) and... the movement. Yes, it is something better than the 
NH35/4R35 (or NH36/4R36 if we&#x27;re talking the day-date version) it&#x27;s directly 
competing with. It is from the same Epson&#x27;s family as the small-second F6222 
in RA-AP0003S, the new in-house Orient&#x27;s movement generation, the one and 
only F6922. By the way, there also is an unbranded version of it called 
Epson YN56, but it&#x27;s not so popular in OEM/ODM mechs as NH36 as of now. 
Anyway, adding to the whole package, this movement really is the star of the 
show that truly justifies 3x the price of 8926OB.

Out of the box, the Kamasu had been displaying not so great of an accuracy:
in my 24/7 wear mode, the first day showed the +12 spd, then it was +11, +10 
and finally settled on about +8.5 seconds per day. And the keyword here is 
&quot;settled&quot;. The movement has shown extremely good positional stability of its 
timekeeping. This is why I didn&#x27;t worry at all about the +8.5 spd deviation: 
I knew that I could regulate the movement and be fine with a very stable 
_and_ accurate timepiece. Well, guess what? I did. After all, I do have some 
watch related tools — not top-notch but they get things done. Of course, I 
don&#x27;t have a hardware timegrapher yet and not sure whether or when I can get 
one at all. I also haven&#x27;t created a software timegrapher myself, so I had 
to rely on the &quot;Watch Acccuracy Meter&quot; application I found for my 
smartphone. Sure enough, it&#x27;s not very precise but definitely helps you 
orient (no pun intended) your regulation efforts in the correct direction.

In F6922, the speed regulator is the smaller, topmost lever above the balance
wheel (the lower and the thicker one is the beat error stud — don&#x27;t ever 
touch that unless you have a proper hardware timegrapher!), and the + and - 
signs opposite to the speed regulator show you where to turn it to speed up 
or slow down the movement. Now, I&#x27;m not exaggregating anything when I say it 
really takes micrometric precision to move this lever to the required 
position, and it took me a good half an hour in total of two large efforts 
to finally get it close to what I wanted. Also, as the operation was done in 
the &quot;dial down&quot; position, what I didn&#x27;t get at the first effort (but did at 
the second) is that I needed to subtract about 5 seconds from what I was 
seeing on my software grapher. After finally getting the closest to what I 
could get with no special equipment, I screwed the caseback, set the watch 
to the correct time and started a new accuracy measurement.

Speaking of which, as much as I despise Rolex and its subbrand Tudor, I have
to give them one credit for testing their movements and certifying every 
watch to not deviate beyond -2/+2 spd. Not saying this could justify their 
exorbitant pricing (and no less expensive service) but it definitely shows 
that the manufacturer at least tries to give them credibility as, well, 
watches, not just jewellery. A minute per month is something of a deviation 
that most rich people can live with, I guess. Well, you know what, maybe I 
already wrote this but I have a wonderful Casio W-800H in my collection that 
is 24 seconds slow per month, which disappointed the hell outta me: only 6 
seconds over the manufacturer&#x27;s negative deviation allowance (most 
mass-market quartz watches are guaranteed to have +/-30 s/month). Do you 
think there is a random Rolex or (mechanical) Tudor in the world that&#x27;s 
currently more accurate than this particular Casio? Maybe, why not?

But back to my newly regulated Kamasu. So, can you guess how much deviation I
could register in 48 hours after this regulation?

A bit under two seconds fast. Yes, this means under +1 second per day in the
24/7 wearing mode. I&#x27;m going to wear the watch all the way throughout this 
October and then tell you the final monthly deviation, but you can already 
see that it&#x27;s going to be well within the quartz-grade monthly allowance, 
let alone various mechanical movement certification ranges. And that&#x27;s after 
a single regulation session that even an amateur like myself can easily do 
at home.

Now, the question is: do you still want a Rolex or a Tudor?

I know I don&#x27;t. I want a bit smaller, titanium version of the very same
Orient.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-09-25-how-i-started-and-paused-ocaml.txt</link>
<title>(2023-09-25) How I started learning OCaml... and paused it</title>
<description><![CDATA[<pre>
(2023-09-25) How I started learning OCaml... and paused it
----------------------------------------------------------
Two posts ago, I told that I would start looking at three native-compiled
(that is, no VM) programming languages as potential C replacement 
candidates. So, OCaml became my first guniea pig of choice. And, to tell you 
the truth, this is the language I really had the highest hopes for. And I 
really can recommend it to everyone. No matter what they say, it&#x27;s really 
easy to pick up and start writing real-life code. It&#x27;s also relatively easy 
to compile fully static binaries using it. You can see the Makefile of my 
OSmol server ([1]) for an example of how to do this. Yes, OCaml is so easy 
that I was able to write a simple and robust Gopher server (that&#x27;s powering 
this very Gopherhole right now!) within &lt;45 SLOC and about 2 days of digging 
through courses and manuals. And the binary of this server doesn&#x27;t require 
any runtime dependencies on my VPS, as it&#x27;s fully self-contained and 
statically linked with musl libc. I couldn&#x27;t be happier with the results of 
my preliminary tests...

...Except one &quot;minor&quot; issue. Can you guess the final binary size?

979184 bytes.

I&#x27;m not kidding. This is with musl — with glibc, it&#x27;s well over 1.6 megs. And
I had installed Flambda optimizer and used all kinds of optimization tricks 
on the musl-gcc and/or zig cc sides as well. If anything, I was expecting 
around 45-50K, as this is the usual size of a static musl linked binary that 
is that simple. Although I *kinda* guess what&#x27;s going on there, it&#x27;s obvious 
that the OCaml compiler does absolutely nothing to remove unused code 
present in the standard library and included &quot;.cmxa&quot;s from the final binary. 
I looked around on teh interwebz and found nothing significant on the topic 
except some &quot;post-link optimization frameworks&quot; that don&#x27;t actually change 
the bigger picture much in terms of executable size, and the mere existence 
of such frameworks shows that the current OCaml implementation just wasn&#x27;t 
designed with static linking and _low-level_ dead code elimination in mind. 
For a Gopher server, that&#x27;s kinda OK, but for my entire spectrum of intended 
language purposes, that&#x27;s just unacceptable. I want my programs to be able 
to run in RAM- and disk-limited environments, where even a phone with 256 MB 
RAM is not the worst case scenario, so a megabyte per single process image 
(not even a whole process!) is too costly, regardless of how awesome the 
language itself is.

That&#x27;s why I decided to put learning OCaml on hold and move on to my next
candidate. It will take some time as well, so I guess I&#x27;ll share my thoughts 
on it in two weeks or so.

--- Luxferre ---

[1]: https://git.sr.ht/~luxferre/OSmol
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-09-18-a-bit-more-on-mechs.txt</link>
<title>(2023-09-18) A bit more on mechs</title>
<description><![CDATA[<pre>
(2023-09-18) A bit more on mechs
--------------------------------
This post is gonna be quite short. Yes, I have started what I promised in the
previous one. However, there is another thing I want to tell you about 
before fully diving into the C replacement topic. And this thing is again 
about watches. Moreover, it&#x27;s about mechanical watches. I just can&#x27;t believe 
the NH35 really is the pinnacle of what can be done in this area without 
paying exorbitant money for such inaccurate timekeeping technology, so I 
gave a shot at two more timepieces, one being Chinese and one Japanese. The 
Chinese one is something I&#x27;ll be able to tell you anything about in ca. a 
month, but the Japanese one is something that already allows me to state 
some conclusions. And here I&#x27;m talking about Orient RA-AP0003S, colloquially 
known as &quot;Bambino Small Seconds Champagne&quot;.

Being modeled after vintage Omegas from 1950s but significantly larger in
size, this obviously isn&#x27;t the most practical timepiece in the world, even 
in the world of automatic watches. But the look, especially on a third-party 
steel mesh strap (yes, I do have one even in 21mm), is just stunning, and 
the crown action is flawless. I wish Orient had a 38 or even 36mm version 
with the same small second configuration, but this one will do fine too. As 
for the timekeeping, the accuracy measurement is still ongoing, but I&#x27;m not 
very disappointed with what I have been seeing so far: +10 seconds per 3 
days deviation in all-day-round wearing mode. This means that, just like 
with Invicta 8926OB, this Orient can be kept at zero deviation by taking it 
off the wrist and positioning it the right way overnight. I just choose not 
to do this.

Besides, I really like the &quot;small seconds&quot; concept. If you have to stick to
analogue indication, it&#x27;s at least nice to have seconds in their own subdial 
and not cluttering the main dial with another hand. It reminds me of 
ana-digi Casios that omit the second hand entirely and dedicate that 
function to the small display window somewhere on the watch face. When done 
right, this approach doesn&#x27;t increase the complexity of the dial, but makes 
its look more interesting and a bit more complete than before. Also, such 
configuration historically means having to use a bit less gears in the 
movement, so it cheapens the production for both mechanical and quartz. 
Thus, I think &quot;small seconds&quot; should return to the analogue mainstream. As 
well as the smaller men&#x27;s watch sizes, because hockey pucks are not cool 
anymore.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-09-11-where-to-move-on-from-c.txt</link>
<title>(2023-09-11) Where to move on from C?</title>
<description><![CDATA[<pre>
(2023-09-11) Where to move on from C?
-------------------------------------
Not gonna lie, I love interpreted programming languages and became really
good at several of them (like JS, Python or AWK, to name a few). But, of 
course, all those languages need some basis they themselves build upon: 
another class of languages which compiles to the native machine code. As of 
now, when we ask ourselves which language is this or that high-level runtime 
written in, we&#x27;ll most probably find C, C++, Go or Rust. I also love pure 
ANSI C. Still not sure if I&#x27;m good at it but that definitely is my favorite 
&quot;low-level&quot; option of a natively compiled programming language.

Or... was.

You see, with C you get (almost) full control over your program&#x27;s behavior,
but this is the exact reason you just cannot fully focus on the logic you 
are trying to implement. I got away with it so many times (the most recent 
of them being Equi, NRJ16, LVTL-O and nne, of course) but when I implemented 
nntrac according to the original T-64 specification, this issue hit me as 
hard as it could. Yes, it fully conforms to the spec, but making it conform 
to the spec is only a half of work, and the other half is something I&#x27;m not 
even sure where to start: making it safe to use (e.g. not segfault on 
unbalanced parens and so on). Most probably, if/when this second half is 
done, the codebase SLOC count is going to increase x1.5 or even x2, and the 
actual processing logic may be even further removed from the original 
algorithm specified in the T-64 document. Just think about it: we already 
have SLOC overhead high enough just to make it stable and working according 
to the spec, but we have to add at least half of that amount to make it 
fully secure. Just because this is C.

But what alternatives do we have in 2023 that I could really use as a C
replacement for my own projects? Here, I&#x27;m going to do a (very) brief 
overview of ten programming languages I know at least something about that 
compile to native optimized machine code (no bytecode, bitcode or other 
VM-like runtime) and do have multiplatform/multiarchitecture support, with 
the &quot;yes&quot;, &quot;no&quot; or &quot;maybe&quot; verdict at the beginning of each item.

1. C++. A big NO. It doesn&#x27;t solve any real problem present with C but adds a
whole lot of unnecessary complexity on top of it. I guess the fact that 
Linus Torvalds himself despises C++ is informative enough.
2. D. No. A sugarized (and no less complex) C++ derivative that didn&#x27;t even
bother removing those stupid mandatory semicolons and making functions 
first-class objects.
3. Pascal (family). No. By &quot;family&quot; I also meant Modula, Oberon-2 and so on,
although Pascal itself is also still alive. A lot of syntactic overhead, 
mandatory semicolons, no way of doing FP either.
4. Go. Maybe. It (almost) does not have any bits that annoy me in other
programming languages, besides having little type inference (you still have 
to write parameter types when declaring functions) and that pascaloid := 
operator (which you are not required to use though as x := ... is a 
syntactic sugar for var x = ...). It also does a good job at static linking. 
However, real-life code in Go often looks too verbose and begging to compact 
it a bit more, which is not possible in most cases. Overall, a good starting 
choice for your C replacement journey. The compiler package isn&#x27;t so small 
though.
5. Rust. No. Just no. I see that code, it makes me go and wash my eyes, if
not puke first. This language might be as safe, fast and powerful as they 
boast about it, but it&#x27;s just extremely unpleasant to work with. This makes 
it a compiled cousin of Perl if you ask me.
6. Zig. No. Too many breaking changes even between minor versions, too many
unnecessary syntactic features, and, as always, mandatory semicolons. The 
&quot;zig cc&quot; cross-compiler is beautiful though. They should focus on developing 
that part (especially when combined with the next language in this list) and 
drop their own language altogether.
7. Nim. Maybe (at least worth looking at), but most probably no. I like its
expressiveness and a lot of features but, to be honest, we already have 
Python that enforces indentation. Besides, the compiler does not generate 
the machine code at once, it uses C (by default), C++, Objective-C or JS as 
the intermediate language and then delegates the remaining work to an 
external compiler. So, Nim is not as self-sufficient as the others in this 
list. Nevertheless, I&#x27;ll give it a chance too.
8. Haskell. No, not really. The de-facto standard implementation, GHC (it&#x27;s
not GNU by the way, it&#x27;s Glasgow), is too monstrous (larger than Go if you 
check) and the package system, Cabal, adds to this monstrosity even more. 
Not to mention the language itself is not very oriented at system 
programming.
9. OCaml. Maybe. In fact, this is the most likely candidate for me to switch
to. The runtime is not as huge as Haskell&#x27;s, its type inference is 
excellent, it has low-level data structures like &quot;bytes&quot; mutable type, and a 
nice machine-optimizing compiler. Besides, OCaml serves as an entry to other 
languages from ML family, ReasonML and SML being the most promising among 
others.
10. SBCL. No. I wish I could but if I wanted performance comparable with
Java, I&#x27;d choose a language that would compile to JVM, such as Clojure. Too 
slow bro.

Note that I didn&#x27;t include Forth here because its major implementations that
do have optimized compilers are quite different from one another in every 
aspect you can think of. Whether or not Forth can replace C, is a discussion 
for another time. I also didn&#x27;t include Red as I don&#x27;t consider it 
production ready yet. Of course when it becomes production ready, this will 
be a game changer, but for now, alas.

So, here are my three favorites selected from the list: Go, Nim and OCaml.
Being in a fair spot as I have zero experience with either of them yet, I&#x27;m 
going to answer the following questions for each of them (maybe not in this 
particular order) over the course of several following weeks:

1. How easy is it to write text processing CLI applications in this language?
2. How easy is it to write networking CLI applications in this language?
3. How easy is it to write GUI and/or Web applications in this language?
4. How easy is it to write and build mobile applications in this language?
5. How easy is it to retarget code in this language for different platforms?
6. Does this language allow to create fully static binaries?
7. How big of a performance overhead does this language create compared to C?

Of course, all these questions can only be answered by practice. So, you get
the idea what my several next posts are going to be about.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-09-04-when-marketing-beats-common-sense.txt</link>
<title>(2023-09-04) When marketing beats common sense... again</title>
<description><![CDATA[<pre>
(2023-09-04) When marketing beats common sense... again
-------------------------------------------------------
Let&#x27;s take a look at the official Panasonic&#x27;s 3V lithium battery capacity
chart, shall we?

CR1616: 55 mAh
CR2012: 55 mAh
CR2016: 90 mAh
CR2025: 165 mAh
CR2032: 225 mAh

I chose the Panasonic brand for this comparison because that&#x27;s what Casio
installs into their (CR-compatible) watches out of the box. Now, let me 
clarify the designations. In the CRxxyy type code, xx is the battery 
diameter in millimeters and yy is its thickness in tenths of a millimeter. 
So, for instance, CR2016 means 20 mm wide and 1.6 mm thick. You get the idea.

First, lets omit CR1616 as I just included it here to showcase there&#x27;s no
need in it when we have CR2012. But some watch modules deliberately make you 
buy a more rarely occuring battery type that&#x27;s not interoperable with 
anything else... just because. That&#x27;s the first marketing trick I&#x27;m going to 
mention there, but in fact it&#x27;s much less dirty compared to what I&#x27;m about 
to tell you.

Now, let&#x27;s take three of my favorite Casio watch modules from all classes:
2719 (analogue, which is in fact Miyota 2S60 I&#x27;ve already written about), 
2747/5574 (ana-digi, the module is pretty much the same, only the display 
differs a bit) and 593 (digital, powers F-91W and a lot of other classic 
digital watches by Casio) as examples. The 2719/2S60 is powered by a CR2012 
and promises about 10 years of run time on this battery, which already is 
good enough, the 2747 promises the same ten years on the CR2025 battery, and 
the 593 module promises around 7 years on CR2016. Nice. But... can it be 
even better? So, I dismantled three Casio watches with these modules: 
MTP-1219A, AW-80 and F-84W respectively. Let me tell you what I saw there.

With the 2719 module (MTP-1219A), fitting even CR2025 there is definitely out
of question. It&#x27;s too thick. With CR2016 though, it&#x27;s a different story: the 
movement technically has nothing to prevent fitting it in there except the 
metal fixers tailored for 1.2mm height. But, with a bit of trickery and 
scotch tape on the metal caseback, it can be done and caseback screws down 
properly. So, we lost nothing but gained in longevity. How much did we gain? 
I&#x27;ll calculate this a bit later. With the 593 module (F-84W), I was able to 
fit a CR2025 instead of CR2016 although the outer battery frame wasn&#x27;t 
closing so nicely (by the way, not every genuine 593 watch even has this 
frame). Again, a bit of scotch tape on the caseback and we&#x27;re good to go. 
With the 2747 module (AW-80), I managed to fit a CR2032 instead of CR2025 in 
the very same manner. And the movement started working even nicer than 
before (to be honest, I thought I had damaged that watch beyond repair with 
some of my previous experiments).

So, was it all worth the risks? Let&#x27;s calculate the relative capacity
increase in each case. Upgrading from CR2012 to CR2016 leads to (90/55 - 1) 
* 100 = 63.6% more battery life (16.3 years instead of 10 years projection 
for 2719), upgrading from CR2016 to CR2025 leads to (165/90 - 1) * 100 = 
83.3% more battery life (12.8 years instead of 7 years for 593), and 
upgrading from CR2025 to CR2032 leads to (225/165 - 1) * 100 = 36.4% more 
battery life (13.6 years instead of 10 years for 2747/5574). And the real 
life figures can be even more stunning but these upgrades are significant 
even compared to what the manufacturer states. I mean we can confidently say 
something like 19 to 20 years for 2719 and 15 years for 593 and 2747/5574, 
provided we install an absolutely fresh battery and (in the latter two 
cases) don&#x27;t use the piezo signal and backlight a lot.

Let me stress it again: if even I was able to fit CR2016 instead of CR2012,
CR2025 instead of CR2016 and CR2032 instead of CR2025 with little to no 
effort and screw the casebacks properly afterwards, there obviously were 
absolutely no technical obstacles for the manufacturer to support those 
longer lasting batteries in these watches out of the box. And the decision 
not to do this was a purely marketing one. And the fact this decision was 
made long before the newer trend of returning back to &quot;3-year&quot; SR batteries 
just proves that this struggle against common sense isn&#x27;t a new thing at 
all, and it merely intensified during the last decade.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-08-28-another-basic-but-iconic-movement.txt</link>
<title>(2023-08-28) Another basic but iconic analogue watch movement</title>
<description><![CDATA[<pre>
(2023-08-28) Another basic but iconic analogue watch movement
-------------------------------------------------------------
Recently, I stopped my Bertucci A-1R experiment and got a stunning result:
it&#x27;s less than 0.5 s/month slow. And it&#x27;s running on Miyota 2035, the most 
basic non-Chinese quartz movement you can possibly imagine. And I already 
talked about it enough in my previous posts, but now I&#x27;m going to tell you 
about another iconic Miyota movement I&#x27;ve been familiar with since a long 
time ago without even knowing it. Enter Miyota 2S60... or, in Casio&#x27;s 
classification, module 2719.

Yes. That one. The one in LIN-168, MTP-1219A, HDA-600, EF-125 and other
three-hander greatest hits from mid-2000s. My personal encounter with this 
movement began when I was a poor student in 2007 who, nevertheless, could 
finally afford his first &quot;serious&quot; watch, and Casio MTP-1219A became one. It 
suited all my needs, the only annoying part being the requirement to 
manually adjust the date every two months. Back then, I didn&#x27;t really value 
what that watch had offered me though: fully stainless steel case, decent WR 
rating AND long-lasting battery. In fact, when I gave it away to my friend 
14 years later, it was still ticking. But in my student years, this watch 
quickly became too boring to me. It just worked. It just told the time 
(provided you adjust it twice a year because of that damn DST) and date 
(provided you don&#x27;t forget to adjust it every two months). It did nothing 
else. I quickly lost interest in it and kept buying whatever I could afford 
in different leagues: Illuminators, ToughSolars, even a titanium LIN-168, 
uh-huh. And I didn&#x27;t even notice that LIN-168 had the same module 2719 
inside. The exterior and case material mattered to me more. But at the end 
of the day, it was just the same three-hander with a date window. And I 
didn&#x27;t pay much attention to it. Oh, how dumb and shortsighted I was.

Fast-forward to the present day, August 2023. I have a HDA-600B in my
collection, again, no fucks given about the module. And I decide to try my 
luck with another sapphire model... you guessed it: MTS-100. Actually, it&#x27;s 
100L, but I stripped off the L on the first day of possession and put on a 
normal (but thinner) all-black NATO strap instead of that awful stock 
leather. Actually, I also planned to try it out with the stock strap from 
the Bertucci A-1R but that one turned out to be too thick to fit under the 
bars. And then I realized something really terrible: not only is it the same 
2719 module I had actively used 15+ years ago, but this module is the only 
pure-analogue one still sporting a CR-type battery that lasts 10+ years, and 
you can&#x27;t buy any other (analogue) modules with the same battery life 
anymore in 2023. And I also realized the battery itself is not CR2025 and 
not even CR2016 (although I bet it _might_ fit there): it&#x27;s CR2012. And it 
still runs for 10, 11, 12 and even 13 years on this extremely thin lithium 
cell. Whoever designed this movement is a genius. But who did?

Citizen, of course. In case you didn&#x27;t know, all low-cost (non-solar and
non-radio-controlled) all-analogue Casios are Citizens inside. Only after 
buying the MTS-100 did I do some additional research and find out that this 
module 2719 is indeed pure, unmodified Miyota 2S60 in all its glory. But as 
you probably know, you just cannot buy a new dressy-looking Citizen 
three-hander with a flat sapphire glass for such price. Well, you obviously 
can if it&#x27;s branded as Casio. Interestingly though, the sister movement 2S65 
which also features a day of the week wheel, has been used in much fewer 
Casio watches (it&#x27;s denoted as module 3716 in Casio&#x27;s nomenclature and 
mostly known to be used in the MTP-1228, MTP-1229 and MTD-1085 models). It&#x27;s 
a real shame so few Casio models keep using these 2S60/2S65 movements right 
now (although EF-121D and MTD-1085 still can be bought new somewhere, they 
are already discontinued long ago). The overall picture is a bit depressing: 
Casio is surely moving towards the &quot;single-use things&quot; trend in their low- 
and mid-price segment.

This is why I&#x27;m going to create another rating table with the best
movements/modules you should look for in case of digital, analogue and 
ana-digi scenarios, and going to publish it soon enough.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-08-22-old-chips-new-perspectives.txt</link>
<title>(2023-08-22) Old chips, new perspectives</title>
<description><![CDATA[<pre>
(2023-08-22) Old chips, new perspectives
----------------------------------------
I hope you have noticed that a new document has appeared in the &quot;LuxDocs&quot;
section of my server. It&#x27;s titled &quot;MAUI knowledge base (MT6261 edition)&quot;. 
That&#x27;s quite an ambitious title, but this document really contains 
everything I have learned throughout my MT6261-related journey since 2015. 
Unfortunately, I had to omit plenty of things that only apply to phones 
manufactured before 2018, as the manufacturers have dumbed them down even 
further as of now.

In general, I&#x27;m not satisfied with the progress of my research on both
SC6531E and MT6261(D/M). But for SC6531E I already can at least read and 
write full flash contents. For MT6261 though, I can only read. There is no 
information on how to write it correctly: neither with DA nor via the SPI 
interface the chipset exposes. Regarding SPI, I tried implementing some 
interface according to the Fernly source code, but no success so far with 
running anything through it (at least for the standard 0x06, 0x02 and 0x03 
commands and so on). Most probably I&#x27;m missing something, but I don&#x27;t have 
any source of information as to what exactly. In fact, I have been on the 
verge of starting disassembling/decompiling the working DAs I have at hand, 
because for SC6531 there are at least some FDL source codes floating around, 
but not for MT6261. But it looks like there is a giant archive with some 
MT6261-related source code leaks sitting on MEGA, so I&#x27;m going to study that 
abomination first. By the way, a huge shoutout to Megous for writing 
megatools CLI downloader. This was the only way I could download all these 
huge .rars on my Arch. And yes, RAR must die.

On the other hand, buying this new Sigma 31 Power Type-C edition has given me
some additional motivation to resume this research, as well as to compile 
this KB document. Especially about MAUI WAP Browser which the Sigma Mobile 
brand owners said it was never there until I publicly pointed their noses at 
it. Guys, if you want to hide the Web/WAP browser functionality in your 
phone, at least don&#x27;t leave it available in the shortcut/fast access key 
configuration. They even left a &quot;screenshot&quot; in their own support page 
explaining how to configure fast access keys, and this picture contains the 
&quot;Internet service&quot; item! And if that&#x27;s not enough, one can also enter the 
browser via opening .url files. That&#x27;s not your usual Faildows-originated 
[InternetShortcut] INIs but a format called vBookmark which I also described 
in my document.

Given how old the MT626x platform itself really is, I&#x27;m not sure why MediaTek
still is so greedy about it and doesn&#x27;t release full specifications and 
flashing protocols to the public, only in the form of DA blobs and obscure 
Faildows-only tools. Yes, I know, there is a Linux version of SP Flash Tool 
that recently started working in different distros other than Ubuntu... if 
you supply the correct LD_LIBRARY_PATH, that is, but you need to find a 
correct DA multi-binary, have the correct scatter file and run all this on 
an x86 machine. Why? Because no libflashtool.so source code for ya, that&#x27;s 
why! Hadn&#x27;t it been for this stupid policy, I&#x27;d rather work with MT6261 than 
with SC6531E as the potential candidate target for FOSS feature phone 
firmware, as the hardware itself is generally much more reliable and 
energy-efficient than Spreadtrum/Unisoc. 

Raw flash access to the devices you buy with your own money must be a
universal right, not a privilege.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-08-17-the-future-of-nntrac.txt</link>
<title>(2023-08-17) The future of nntrac</title>
<description><![CDATA[<pre>
(2023-08-17) The future of nntrac
---------------------------------
The week has passed too quickly. But anyway, the first more-or-less working
nntrac version is on my SourceHut ([1]). I won&#x27;t go over all its features as 
they are described in detail in the repo&#x27;s README file. But I promised to 
tell you what I have been planning to do with it.

You see, I loved the idea of FirefoxOS/KaiOS: all phone userspace is
scriptable in a high-level language and non-obfuscated. The implementation 
though... you know how it went. This is why I have an idea of creating my 
own phone runtime (the target doesn&#x27;t matter at this design stage, be it 
SC6531E, MT6261 or anything more performant) that would be much better than 
J2ME as it would be based upon a very lightweight and open scripting 
language with a very optimized runtime. TRAC turned out to be the ideal 
candidate for such a language. Especially with an implementation like mine, 
which allows to easily embed new primitives into the language and the 
language itself into C-based applications. And, as with any other homoiconic 
language extended to the point of raw file loading, nntrac code can easily 
be arranged into a set of modules responsible for various functionality. In 
other words, I plan on creating a framework. The first TRAC-based framework 
for featurephone applications.

I also cannot emphasize strong enough how far this language has been ahead of
its time. Especially with its &quot;everything is a string&quot; approach and, of 
course, its homoiconicity. Too bad its history is more associated with 
copyright freaks than with a technological genius of the 1960s. As long as 
I&#x27;m alive, I promise to try and change that.

Stay tuned.

--- Luxferre ---

[1]: https://git.sr.ht/~luxferre/nntrac
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-08-10-the-world-we-live-in.txt</link>
<title>(2023-08-10) The world we live in: the maxim of Arkham</title>
<description><![CDATA[<pre>
(2023-08-10) The world we live in: the maxim of Arkham
------------------------------------------------------
First off, there are some good news: nntrac is mainly complete and almost
ready to showcase. My next post will be solely dedicated to it. Also, I have 
found some extra bugs in nne and going to try and fix them as soon as I find 
the time. But today, there&#x27;s something else I want to talk about.

When researching TRAC history I shared in my previous posts, I also dug a bit
more into the topic of Ted Nelson himself and the history of his dream 
project called Xanadu. I&#x27;ll get back to what I think about it later, now I 
want to focus on something else. As stated in a wonderful 1996 article in 
the Wired magazine, called &quot;The Curse of Xanadu&quot;, which is probably the most 
complete piece of information about the man&#x27;s biography (and that&#x27;s why I 
have saved it locally in the plaintext format and probably will share it 
somewhere on the Gopherspace), Nelson invented the very idea of hypertext 
because of his real struggle to remember and structure things in his head. 
In other words, hypertext was primarily devised by (and for) ADD people. 
Having realized that, I was amazed how everything else fell into its place: 
starting with why Xanadu had failed and ending with why the modern bloated 
Web and mobile touchscreen &quot;apps&quot; flourish.

You see, until ca. 1995, computers in general didn&#x27;t try to be
&quot;user-friendly&quot; in the modern understanding of these words (which I have 
already written about in some of my earlier writeups), thus requiring people 
to actually think what they are doing and why. This, in turn, required some 
fair amount of discipline, literacy and overall sanity. But then, businesses 
in charge decided that smart people are not to be milked that easily, and 
started dumbing down their products for larger audience to lift those 
requirements. This process has never stopped to this day, and the 
consequences are already devastating and promising to be catastrophic in the 
forseeable future. This process has already reached far beyond computing 
into other aspects of our daily life, and led to what I call &quot;the maxim of 
Arkham&quot;:

Everyone is now forced to live in the world that is specifically tailored for
people with mental disorders.

What if you don&#x27;t have any significant mental disorders? Then you&#x27;re screwed:
according to the Newspeak, you are not healthy anyway, you&#x27;re 
&quot;neurotypical&quot;. You don&#x27;t have any more influence on the world than anyone 
else. You have the same right to vote as the illiterate dumbass next door 
and the psycho-the-rapist across the street. You are the one who will be 
made guilty if you say anything the mentally weak don&#x27;t like, including the 
truth about them. You are the one who all these &quot;codes of conduct&quot; are 
really turned against. You have to walk on the same roads, visit the same 
shops, buy the same things and suffer from these things&#x27; inferiority to what 
you had earlier, because earlier those things were designed with mentally 
healthy people in mind. And one day, you understand that the hardest task as 
of now is to not go insane yourself when watching this madness surrounding 
you every day.

But why? Because profit. Weak-minded people are easier to trick into paying
for thin air and buying trendy &quot;one-time-use&quot; products, to deceive them 
about their real perspectives and values in life, to promote various 
political agendas and so on. Compulsive consumption of material and virtual 
goods is a monetary driver strong enough to wilfully keep everyone insane. 
And if anyone awakens and realizes what&#x27;s going on, this army of consumorons 
will itself stomp on the dissenters without a second thought.

Gopherspace seems like one of the few places out there still not taken over
by consumorons. This is why I prefer this place as a safe harbor for 
&quot;neurotypicals&quot; who don&#x27;t suffer from Attention Deficit and Hypertext 
Disorder even in 2023.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-08-05-saving-trac-from-extinction.txt</link>
<title>(2023-08-05) Saving the oldest embedded scripting language from extinction</title>
<description><![CDATA[<pre>
(2023-08-05) Saving the oldest embedded scripting language from extinction
--------------------------------------------------------------------------
I have started some work that almost no one in the world is probably going to
appreciate: reviving a programming or, should I say, scripting language that 
got first developed from 1960 to 1964, then got a major revamp in 1984, then 
was supposed to get another revamp in 2001 but instead fully died out around 
2005, and all the info on this language can now only be found on the 
Internet Archive. And yes, I am basing my work on the very first version of 
this language from 1964. Those select few who knew it are mostly very old 
now, if alive at all. I think it&#x27;s safe to say there are less than 100 
people left on the planet who know how to write scripts in this particular 
version of this particular language, and less than 1000 people who know any 
existing dialects of it at all. Yet I still hold my ambitious hopes to 
change this status quo and to help this language reach the popularity it 
really deserves.

The language is called TRAC (short for &quot;Text Reckoning And Compiling&quot;, sic).
Yes, it is almost 60 years old by now, yet I heard nothing about it until, 
like, July 2023, when I, having started developing my nne editor, also 
researched various MicroEMACS clones (like mg) and stumbled upon the mention 
of a DOS-based editor called Freemacs, which is small indeed (~21k in size, 
as they say), but other than that, the only worthy detail about it was that 
it is scriptable in a language called MINT (which is a recursive acronym, 
&quot;MINT is not TRAC&quot;). When I started digging deeper into the topic of TRAC 
itself, it turned out to be nothing like I had seen before. And I had seen a 
lot: various Lisps and Forths, Rebol/Red, Tcl, even implemented my own VTL-2 
flavour and ran a Brainfuck interpreter in it, but this... This was 
something else, something way ahead of its time, something designed from the 
perspective of linguistics and actual human-machine interaction, not 
computer science or mathematics, and yet remained so simple at its core that 
the entire parser algorithm is explained in the spec itself in 15 rules, and 
this algorithm essentially allows anyone to implement a call stack with 
unlimited recursion level with absolutely no memory or processing overhead. 
Yes, it does rely on your ability to insert arbitrary amount of characters 
in the middle of the string or to delete them from the string, so it&#x27;s not 
so trivial if your system doesn&#x27;t have basic malloc/realloc capabilities, 
but other than that, the language is *extremely* lightweight for what it&#x27;s 
capable of. And, unlike e.g. Lisp or Forth, it doesn&#x27;t enforce you to create 
or emulate any special data structure beside the simplest lookup tables for 
primitive functions and &quot;forms&quot;, which are just named strings that can hold 
variable data as well as other code: yes, TRAC is fully homoiconic and 
functional.

After seeing all this, my first natural questions were: why isn&#x27;t TRAC still
popular nowadays, especially in embedded sector which is mainly scripted in 
Lua, various Forths, BASICs and even JS? Why didn&#x27;t they finish the T2001 
standard? Why did the TRAC Foundation website go down in 2005? Well, the 
original TRAC creator, Calvin Mooers, tried to control its redistribution 
until his very death in December 1994, and he even trademarked the TRAC name 
itself and tried to sue people for cloning it (after specifying full 
algorithms in his papers, aha). This is why, despite several clones had been 
developed (like SAM76 or the aforementioned MINT), the original language 
didn&#x27;t gain enough TRACtion because of such a vigorous copytardism of the 
author (and this is why MINT was probably based upon the T-64 standard when 
the T-84 version already came out). After Mooers&#x27; death, the control over 
TRAC was taken by his daughters: Helen Sophia Mooers Solorzano and Edith 
Augusta Mooers. They eventually founded the TRAC Foundation, published all 
the specs they could find and open-sourced the T-84 version under GPLv1. But 
it looks like it was too late to recover from their father&#x27;s mistake, and no 
one was really interested in this language anymore. And the development of 
T2001 was started in... Java and got frozen at the pre-alpha stage. Four 
years later, the website went down. Only the IA remembers.

Gotta admit, the more I read about the history of this language and the folks
around it, the more it gave me shivers. And then, here comes The Man. Ted 
Nelson himself. With a very bizarre video I&#x27;m even gonna link here ([1]). In 
this video, he explains how he participated in the creation of an 
unauthorized TRAC clone which would eventually become SAM76 by the 
R.E.S.I.S.T.O.R.S. group, and also mentions a fairly recent recreation of 
the original T-64 specification in Python (v2) by Nat Kuhn from the same 
group (his son Ben Kuhn also took part in it). I thought, &quot;well, this is 
something but definitely not enough&quot;. Indeed, I can&#x27;t even imagine how an 
implementation in a high-level (and not very resource-frugal) language would 
be of any use in this case, aside from some modeling or prototyping. So, I 
decided to start creating my own implementation in ANSI C, again, just like 
with nne, aiming it at sub-1000 SLOC and maximum portability. This 
implementation is going to be called nntrac (all lowercase) and is mostly 
being written in accordance to the original T-64 specification, aside from 
several differences I&#x27;m going to specify in the docs and in my next 
TRAC-related post when the first full version of nntrac is ready. For now, I 
can just say that I plan on implementing a fully embeddable API (simpler 
than Lua&#x27;s one), ability to write custom primitives and an extended set of 
built-in primitives in addition to the 34 of them that belong to the spec. 
And all this under 1000 SLOC of C.

Now, after I finish nntrac and put it into a repo, what am I going to do with
it next? Well, I have in mind a bigger project of my own featurephone UI 
shell, and I want the apps for this shell to be fully text-based scripts. 
TRAC in general and nntrac in particular are going to ideally suit such an 
embedded and low-powered environment with a custom set of graphics and input 
handling primitives I&#x27;m going to specifically add for this phone UI. Of 
course, there is a lot more work to do regarding documenting all these 
things, and I don&#x27;t mean just my custom APIs and primitives, I mean the 
entire language itself. All current TRAC manuals exist in a form that&#x27;s not 
very comprehensible by today&#x27;s programmers. They need to be refined and 
properly structured. For nntrac and its future usage perspective, this is 
going to be a big deal and probably will take just as much time as the 
creation of the interpreter itself.

Oh, and did I also mention that nntrac, along with all its documentation, is
going to be released into public domain? Because just like SQLite and nne, 
public domain deserves a decent lightweight scripting language with no 
nonsense included. This 60-year-old treasure must not be forgotten and 
surely can serve for the greater good many many years forward if taken care 
of. Looks like I&#x27;m the one who has to do it.

TRAC must live.
.NET must die.

--- Luxferre ---

[1]: https://www.youtube.com/watch?v=wFiHE2NVQOY
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-07-29-switching-from-vim-to-own-editor.txt</link>
<title>(2023-07-29) Switching from (Neo)Vim to... my own text editor</title>
<description><![CDATA[<pre>
(2023-07-29) Switching from (Neo)Vim to... my own text editor
-------------------------------------------------------------
Right now, I kinda feel like a Jedi having built his own lightsaber.

Yes, I got used to writing some tooling myself before, but those mostly were
simple scripts (or not so simple, like FrugalVox or Bopher-NG) and not a 
friggin&#x27; fullscreen text editor for VT100-compatible terminals on POSIX 
systems written in under 777 SLOC of pure ANSI C that I&#x27;d switch to since 
day one I considered it more or less stable and complete. This is not _the_ 
largest project I have written in pure C (I reckon Equi is the largest so 
far), but it surely is the most important for me personally. Because I use a 
text editor literally every day I use any of my personal computers. And 
writing own text editor for fun is one thing (and it surely has been fun), 
but writing it to replace Vim/NeoVim/Vis/busybox vi/etc as the main tool for 
daily usage is totally different. And there really was a lot to consider, as 
well as a lot to sacrifice. But let&#x27;s begin from the beginning.

During my week of exploration (two posts before), I also stumbled upon sta.li
and Oasis Linux. I wasn&#x27;t able to fully build either of them but I became 
more interested in all the lightweight permissive-licensed software that 
could be built statically. Before that, I also had discovered Zig project 
(that itself is based on an LLVM derivative) and its zig cc subcommand that 
allows easy cross-compilation of C code into a bunch of different 
architectures. Among the targets, there was static linking with musl libc. 
So, I started taking whatever pieces of MIT-licensed, BSD-licensed or public 
domain software I could and building them statically against musl (and glibc 
whenever musl was impossible). I was generally satisfied with the resulting 
binaries size, with one notable exception: text editors. Statically built 
Vim binary, for instance, weighs 3433600 bytes, and Vis weighs 644288 bytes. 
Really? An entire programming language runtime (Lua 5.3) weighs 363088 
bytes! Anyway, this was the first time it hit me that, unless I find a 
decent lightweight editor, I must create one myself.

For some more time though, I continued searching. My first options were to
separate busybox or toybox vi, but either of those is too cumbersome to use 
and doesn&#x27;t even offer line wrapping. And having to use horizontal scrolling 
really makes me puke. There also were some microEMACS derivatives like mg, 
but an editor that requires a double combo to exit simply cannot earn my 
trust. Also, even mg was full of sheer nonsense and functionality I never 
found myself using. I also found a very ancient (ca. 1991) public domain 
editor called ue with puny codebase around 345 SLOC of C, but it physically 
had been unable to handle any terminal size other than 80x25 and didn&#x27;t 
handle it very well overall, the code required several fixes just to be able 
to compile it with anything at my disposal, and on top of all that, it used 
Ctrl+SDEX for arrows, and other keybinding were no less unusable. I liked 
the idea of such small codebase though, I just needed something more 
practical. This is where I gave up searching and finally started my own 
design, only setting a single hard limit: no more than 1000 SLOC of C.

At first, I wanted to make my editor a vi clone. Yep, that silly. I planned
to only implement a subset of POSIX vi that I actually was using day to day. 
Then, I understood two simple things: first, even Busybox or Toybox 
implementations of vi greatly surpass 1000 SLOC, second, why recreate 
someone else&#x27;s experience if I can tailor the entire application to my own? 
Key chords are awful, modality is not very obvious, but what else is left? I 
spent a good day or two thinking just about the control scheme I want to 
implement. And I settled on the semi-modal controls: it&#x27;s like chords but 
you have a prefix sequence (I call it &quot;modstring&quot; in the docs) instead of 
having to keep a modifier key like Ctrl pressed. The modstring I chose 
doesn&#x27;t conflict with any other application: it is double Escape key press. 
I found it a no-brainer to get used to. Pressing Esc Esc w to save the text 
file is faster than pressing Esc :w Return in vi. It&#x27;s fascinating how far 
you can go when you ditch all the dogmas imposed onto you over all these 
years.

After the control scheme had been defined, the process went on much quicker.
Besides the usual routine work about terminal I/O and memory management, 
another techincal challenge arose: unlike most &quot;lightweight&quot; alternatives, I 
wanted my editor to be fully Unicode-aware as I also write poetry, mostly in 
Ukrainian. That&#x27;s why I decided to not perform any internal codepoint 
decoding but just store every UTF-8 character in a 32-bit integer as a 
sequence of 1 to 4 bytes, little-endian. Why little-endian? Because they are 
much easier to output to the terminal or a file with a single loop with 
shifts. And when the shift result is zero, you know that the character 
ended, because no valid UTF-8 sequence can contain a null byte. It&#x27;s a 
simple but elegant solution that made a firm distinction between a byte and 
a character, and all my further functions operated on characters in 4-byte 
integer boxes instead. And for the lower part of ASCII (&lt;128) these 
operations aren&#x27;t different from the usual char type anyway.

Several days had also been spent on implementing and perfecting line wraps,
scrolling and cursor positioning. I had to make a sacrifice though and not 
implement whole-word wrapping, as this part already was complicated enough. 
As I write code and poetry much more often than I write posts like this 
(which are then auto-wrapped with the gmi2txt.sh script), I don&#x27;t mind not 
having whole-word wrapping at all. After this had been done, I fixed the 
file loading process and a bazillion of other small things, implemented 
useful features like bracket matching and external shell runner and also 
added the icing on top of the cake: an in-app help screen that can be called 
with Esc Esc h. This way, you can learn this editor even if you have a 
single C file or a static binary without the readme.

Given all that, I decided to call this editor just what it represents: nne,
no-nonsense editor. You can see in action by building it from my SourceHut 
repo here: [1]. Not only is it below 1000 SLOC but managed to get it under 
the limit of 777 (at the time of writing, it&#x27;s 774 or so). And I have fully 
switched to it myself and writing this very post within it. Also, it is 
released into public domain, no compromises. Just like public domain 
deserves oksh and SQLite, it also deserves a decent lightweight text editor. 
Of course it&#x27;s missing a huge number of features (such as line end 
conversion), but that&#x27;s because I don&#x27;t really need them in my daily routine 
(e.g. if I need to convert the endings, I use dos2unix first). As such, I 
consider it feature-complete and will only focus on bugfixes and 
optimizations from now on. Although it already is quite fast and small: the 
musl-linked static binary for x86_64 currently weighs just 68880 bytes 
(compare it to Vis or mg, uh-huh).

Of course, nne is highly opinionated. It doesn&#x27;t have a way of changing the
tabwidth without recompiling, it auto-replaces all tabs with spaces (to type 
a literal tabulation character, you have to press Esc Esc Tab), you can&#x27;t 
turn off autoindentation but you don&#x27;t have any syntax highlighting (I 
explained why in my previous post) or visual line numbering (only in the 
status bar). It doesn&#x27;t even have a real undo, only the modcombo Esc Esc u 
to discard unsaved changes. I explained most of these aspects in nne&#x27;s 
README and its FAQ, but this vision is something you just have to accept if 
you want to feel comfortable with nne. And if you don&#x27;t... again, it&#x27;s 
public domain, it&#x27;s very small (for ANSI C and this set of features), 
well-commented and comprehensible code, feel free to make any changes to 
make it more suitable to your personal workflow.

For me though, being finally untied from both keychord and modal paradigms,
from any visual overhead such as colors and line number columns and, most 
importantly, from the feature creep that I would never use, made me feel 
much freer than I was before. I really hope this particular lightsaber makes 
a nice addition to my statically built collection and will stay in use as 
long as it can.

--- Luxferre ---

[1]: https://git.sr.ht/~luxferre/nne
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-07-24-syntax-off.txt</link>
<title>(2023-07-24) Turn off syntax highlighting and start living the full life</title>
<description><![CDATA[<pre>
(2023-07-24) Turn off syntax highlighting and start living the full life
------------------------------------------------------------------------
I know the topic is a bit controversial. Any writeup on teh interwebz about
whether or not syntax highlighting is actually of any value stirs up a 
tsunami of comments. But almost everyone on both sides misses at least one 
crucial point: syntax highlighting is a crutch for newbies. And if you don&#x27;t 
learn to live without it, you don&#x27;t grow up from a newbie to a hacker, you 
turn into a noob and then into a lamer, i.e. eternal noob who is ready to 
defend own noobness up to the point of open aggression.

Indeed, when you are just starting to learn a programming language, then
highlighting, to some extent, is a real bliss: it gives you an instant 
visual reward whenever you have typed a correct keyword or other important 
character. Not only it helps you write correct code (at least syntactically 
correct), but also helps to read your own code until you have memorized all 
the keywords and syntax you need to be fluent in this language IRL. Since 
then, however, it becomes your false friend: you don&#x27;t fully memorize these 
keywords precisely because you rely on the highlighting to do the job for 
you (and don&#x27;t even get me started on autocompletion). And you don&#x27;t notice 
how you stop caring about the overall readability of your code as well, 
since it looks fine anyway... when highlit. 

And no, indentation is not the same as highlighting. Indentation is an
inherent part of text. Indented code looks the same in the cat command 
output and in any blown-up IDE. It actually helps the code to look more 
readable and structured, especially with block-oriented programming 
languages such as C, JS, Lua etc, and in Python they even made it an 
essential syntactic element. But it is universal and portable to anything. 
The only environment-dependent indentation parameters are the displayed 
tabulation width and whether or not to convert tabs to spaces ([1]). And 
that&#x27;s it.

Highlighting, on the other hand, totally is a feature of the environment,
completely depends on it and, by proxy, makes _you_ fully dependent on this 
environment. Then, one day, you find out you&#x27;re next to blind when viewing 
the same source code with cat, less and other commandline tools, or opening 
it in a foreign editor that doesn&#x27;t have the coloring scheme you got used 
to. At that point, you have exactly two options: either continue to make 
everything use the same colors and whine whenever you can&#x27;t do this, or grow 
out of the newbies&#x27; pants and learn to finally distinguish the words and 
characters, not colors, and write in a style that&#x27;s readable anywhere. Of 
course, the choice is yours.

--- Luxferre ---

[1]: my personal and very strong preferences are: tab is 2 spaces, absolutely
do convert, unless editing Gophermaps or Makefiles. Why? That&#x27;s the topic 
for another time...
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-07-17-a-week-of-exploration.txt</link>
<title>(2023-07-17) A week of exploration</title>
<description><![CDATA[<pre>
(2023-07-17) A week of exploration
----------------------------------
I wanted to write a long post about a very insteresting topic but it still
ain&#x27;t the time for that. Besides, only a week has passed. During this week 
though, I felt like an explorer who doesn&#x27;t even have to leave his own home, 
because I found out that:

- Haiku R1beta4 works just fine on an old MacBook Air A1370, except the
Broadcom WiFi;
- OpenWRT can also be installed on A1370 like a normal Linux distro;
- there is plenty of awesome FOSS software that is non-GPL and even public
domain;
- using the public-domain oksh as the main shell is not as scary as I thought;
- one-true-awk is still known as nawk in some distros;
- doas really is much better than sudo;
- cwm and dzen2 are really awesome (now looking for their Wayland
replacements that can be built statically);
- CPU/RAM monitoring can be done with simple shell/AWK scripts, and it still
will be on par with what HTop can offer;
- for the same reasons, no one really needs connman, volumeicon and cbatticon
except when they are super lazy;
- if you&#x27;re really looking for a good, lightweight and non-GPL FOSS, look at
the ports from OpenBSD and Plan9 first;
- there even exists &quot;zig cc&quot; compiler which is also non-GPL and can
cross-build your code to a huge amount of targets, and static linking is no 
problem for it as well.

The following week promises no less interesting discoveries.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-07-10-100-buck-watches.txt</link>
<title>(2023-07-10) So, you only have $100 for a watch... What to choose?</title>
<description><![CDATA[<pre>
(2023-07-10) So, you only have $100 for a watch... What to choose?
------------------------------------------------------------------
Imagine a situation that you moved to a different country, and the customs
confiscated your entire collection you tried to import with you, and you 
can&#x27;t afford any watch except the cheapest ones. Or a simpler scenario: you 
just want to buy your first wristwatch but don&#x27;t want to spend a lot on it. 
And on top of that, let&#x27;s imagine you really want something lightweight 
(under 50 grams) to not burden not only your wallet but also your wrist 
itself. So, what do we have in this domain? While the digital part of the 
watch world seems simpler as I can only advise Casio AE-1200WH (39g) or 
W-800H (37g) or DB-36 (27g) or the new LF-20W (23g) or anything on the 593 
module (including but not limited do the famous F-91W that weighs 21g, and 
its JDM cousin, F-84W) for the sub-$100 price and sub-50g weight, the 
analogue part looks more complicated, especially these days with crazy watch 
price divergence around the globe.

The first thing that comes to my mind is Casio AW-80 (34g). While not
strictly analogue and rather ana-digi, it has everything you might need and 
even more (like a Telememo phonebook I have dedicated an entire article on 
the chronovir.us blog to). It&#x27;s pretty much hackable as well and one can 
resolder its jumpers to have a scheduler instead of the telememo (sorry, I 
don&#x27;t know the exact jumper config yet). However, I&#x27;m hesitant to advise 
this watch as the most preferrable choice because of two reasons: first, the 
analogue part is rather old-school and the hands only can move in a single 
direction, second, the glass is domed acrylic and picks up scratches like 
crazy.

Another, this time not so obvious choice would be Casio HDA-600. This is
still very lightweight (39g) and rugged like a G-Shock, but that comes at a 
cost of looking and feeling very outdoorsy. Those bullbars it comes with 
make it especially uncomfortable for indoor wearing, and if you take them 
off, it looks even weirder. The watch itself is as simple as it can get, and 
very legible too (especially for its dial size), just not for the 24/7 wrist 
usage. If we take a look in a different direction, we can also find the 
Timex Expedition Acadia series. I don&#x27;t know much about it but I had owned 
one of those watches, and it weighed about 33 grams or so. Again, the major 
problem with it is the domed crystal. Probably acrylic too, but I&#x27;m not so 
sure. Also, Timex quartz movements tend to be noisy in terms of ticking and 
not so accurate in terms of timekeeping. That Indiglo tho...

Finally, what I personally would be looking for if I ended up in such a
situation would be something based on the Miyota 2035 movement. Ever heard 
of Casio MQ-24? Well, this is it. Its 7B2 version dial design is purely 
iconic. But beyond that, there is MQ-71, which is basically an MQ-24 with 
lumed hands and which I have recently measured to only have -2s/month 
deviation. Maybe I got lucky but who knows. These small beasts only weigh 
about 19 grams and you can&#x27;t even feel them on the wrist. And most recently, 
I have got a Bertucci A-1R, which is basically an upgrade over these Casios 
adding a better casing with an easier to operate crown, extremely 
comfortable single-pass strap, better lume and better glass while being 
powered by the same movement. And it weighs under 33 grams on the stock 
strap and about 16 grams without. Since yesterday, this A-1R definitely is 
my new daily driver. I guess I&#x27;m going to have a separate post about it 
after measuring its accuracy too. And no, I don&#x27;t mind the second hand 
missing the marks because that&#x27;s something to expect if you deliberately 
rotate the movement 30 degrees clockwise to put the crown against the 4 hour 
position.

Yes, I know that 2035-based watches don&#x27;t even have a date display window.
But again, why would you need a date on the watch if it doesn&#x27;t have a 
proper auto calendar to move it at the end of the month and you need to 
remember to move it manually every two months or so, having the potential of 
accidentally stopping the second hand and thus messing up the time (which 
happened to my Seiko)? And for the sub-$100 price, no purely analogue watch 
will feature an auto calendar (even the much more expensive SSB401 doesn&#x27;t), 
so either go digital or scrap the date entirely. The only negative side for 
this movement is being powered by a small SR626SW battery which is rated for 
3 years of runtime, and since you can&#x27;t do much on this type of watches 
other than set the time, I reckon this might be a realistic estimation. I do 
have two SR626SW battery blister packs and I don&#x27;t believe this battery type 
will disappear anytime soon, but... Why not CR2025 or even CR2016? That 
would easily have ramped up the runtime to 10 years or more! Maybe Citizen 
(which owns the Miyota subbrand) just doesn&#x27;t think as much about the 
practical aspect anymore as Casio still does. 

With that said, my top-10 of sub-$100 sub-50g watches (both digital and
analogue) looks like this as of now:

1. Bertucci A-1R
2. Casio AE-1200WH
3. Casio W-800H
4. Casio AW-80
5. Casio MQ-71
6. Casio LF-20W
7. Casio F-84W (or F-91W if you can&#x27;t find it)
8. Casio MQ-24
9. Casio DB-36
10. Casio HDA-600

I know I didn&#x27;t touch some cool Seiko subbrands like Lorus or Alba yet, but
remember I only talk about the watches I can personally get my hands on. If 
and when Lorus and Alba are available here, I definitely may update this 
list.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-07-03-when-will-you-learn.txt</link>
<title>(2023-07-03) When will you learn to... learn?</title>
<description><![CDATA[<pre>
(2023-07-03) When will you learn to... learn?
---------------------------------------------
Case one. A retro game streamer complains about YouTube introducing mandatory
user handles &quot;with those stupid @-signs&quot; and Twitch tightening the 
censorship policies around the content and restreaming rights. When I 
mentioned some perfectly viable alternatives (like PeerTube, Odysee or even 
to set up an Owncast server — a turnkey solution to make &quot;your own personal 
Twitch&quot;) and that more streams on alternative platforms mean more motivation 
for their authors to improve them and less motivation for the monopolists to 
play gods, he told me he won&#x27;t bother even thinking of doing it until the 
majority does and until the audience moves to them. Not a single thought of 
how the audience moves to them if no streamer does. But yet again, he keeps 
complaining about increasing limitations of Twitch and YouTube Live, while I 
already have set up my Owncast server within 10 minutes on this very VPS.

Case two. Twitter is broken once again, this time beyond repair: no tweets
can be viewed without having an account (effectively rendering my and other 
Nitter instances totally useless), and even if you do have an account, there 
is a... daily view limit. Doesn&#x27;t matter how many tweets you can view: the 
mere fact of having such a limit in 2023 is a sign of total FUBAR. I don&#x27;t 
have a Twitter account since long ago, but I used to read some critical 
information via my Nitter instance. Well, no more. Conscious people moved to 
Mastodon and other ActivityPub-based services immediately, less sane people 
remembered they had a Juick account, but some &quot;activists&quot; started actively 
PR-ing some no less totally proprietary bullshit like Post or Bluesky.

I understand one thing. In both cases, these people didn&#x27;t learn to learn.
And it is their unwillingness to learn, their lame ignorance and passivity 
that leads to global dystopian disasters. They like when others think for 
them. They like to be controlled and spied on. They gladly trade their 
security for convenience, without a second thought about consequences for 
themselves and people surrounding them. At most, they want &quot;trendsetters&quot; to 
make the switch, not understanding those guys are paid to NOT do so or to 
give the plebs an illusion of choice. The latter happens much more often 
than we are used to think. For example, I don&#x27;t understand why the &quot;Linus 
Tech Tips&quot; YT channel hasn&#x27;t changed its name to &quot;Billy Tech Tips&quot;. Because 
that guy doesn&#x27;t deserve to be called Linus. Dirty Billy is his modus 
vivendi for sure.

When it comes to the choice of online services and/or software, I have a
single rule of thumb: &quot;those who choose bread over freedom, eventually lose 
both&quot;. In other words, look which services mostly try to take the control 
away from you, and avoid them no matter what. It&#x27;s not a big deal if the 
free/distributed/federated alternatives aren&#x27;t as convenient out of the box, 
it&#x27;s just a matter of time for them to improve while also giving you an 
opportunity to grow your own bread. But in this case you know that you own 
your data, your computing resources and your online identity. In any other 
case, you&#x27;re contributing to online slavery, global botnets and total 
surveillance. And you are personally responsible for all this tightening the 
knots around the Internet freedom&#x27;s neck.

By the way, that retro streamer also laughs at me for preferring Matrix over
Discord. We&#x27;ll see who has the last laugh.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-06-30-why-cassettes.txt</link>
<title>(2023-06-30) Why cassette tapes are still relevant</title>
<description><![CDATA[<pre>
(2023-06-30) Why cassette tapes are still relevant
--------------------------------------------------
I just finished recording my first mixtape. On the cheapest hardware I could
possibly find new. But I am quite satisfied with the result. The tech 
definitely is alive and well in 2023.

Besides the audio source, the recording device and the cassettes themselves,
you&#x27;ll need two precious resources: electricity and time. Time is necessary 
not only for the recording process itself, but also to adjust the correct 
input signal level, because in case of &quot;shoebox recorders&quot;, especially 
modern ones, you don&#x27;t have any level indicators at your disposal. Even in 
this case (which is the most desperate one, because your recordings will be 
mono and their overall quality is very lo-fi), the process is simple. First, 
just record a bright fragment of the audio you need to process and look for 
any distortions, starting with about 70% to 80% level. Keep lowering it 
until you don&#x27;t hear any distortions. In my case of Auna RQ-132USB aka 
CRS-132 (which is probably true for all the OEM &quot;shoebox recorders&quot; of the 
same generation) recording from VLC running on the Nokia T20 tablet onto a 
Type-I cassette from Sony, I had to lower the output down to about 20% 
(relative to the tablet&#x27;s headphone hearing limit recommendation) to get the 
optimal sound. Overall, if your mixtape is prepared correctly, you shouldn&#x27;t 
have to spend over 2 hours on a 90-minute recording. Of course, with a 
stationary deck, the process is simpler although more energy-consuming, and 
yields much better outcome.

Speaking of energy consumption, modern cassette &quot;shoeboxes&quot; and players have
one huge advantage despite all their inferiorities: the DC power they 
require is from 3V/0.5A (pocket-sized players) to 6V/0.6A (&quot;shoeboxes&quot;) at 
its peak. So, theoretically, they consume up to 3.6W at most. Realistically, 
they can even run on 1.2 NiMH cells, which maps to 1.2W for pocket players 
and 2.88W for &quot;shoeboxes&quot; which, in turn, can hypothetically run from USB 
too. Quite a solar-friendly solution we have here. And if the rechargeable 
Varta C cells I have bought for the tape recorder purposes really do have 
3000 mAh in them, then this CRS-132 can run for at least 20 hours straight, 
which is already impressive.

&quot;But wait&quot;, you might ask, &quot;aren&#x27;t solid state players/recorders much more
efficient in terms of space, energy and audio quality?&quot;

Yes, they are. They also break more often. They also don&#x27;t offer any
good-quality options that have removable batteries. Say hello to the planned 
obsolescence that makes them contributing to the e-waste problem much more 
than any broken cassettes and oldschool players could ever do. I wish I 
hadn&#x27;t lost my single-AAA-powered pocket MP3 player though. Yes, it was an 
OEM crap (by TakeMS), but this crap did most things right. Now, nothing does 
them right except cassette players. Also, because of the 30 to 45 minutes 
limitation of the tapes, you need to think multiple times about how to 
better arrange your music. At the end of the day, you only leave what really 
matters to you on the tape. And then you can decorate the box. I can hardly 
imagine presenting someone a microSD with music. CDs don&#x27;t require much 
effort either, and vinyl records are virtually impossible to produce at 
home. So, cassettes, with their perfect balance of DIY and physicality, are, 
to me, a symbol of the analogue rebellion in the digital age.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-06-23-input-methods.txt</link>
<title>(2023-06-23) On "user-friendliness", part 2: input methods</title>
<description><![CDATA[<pre>
(2023-06-23) On &quot;user-friendliness&quot;, part 2: input methods
----------------------------------------------------------
We&#x27;re surrounded by legacy, whether we want it or not. Sometimes, this legacy
is bordering with absurd-grade obsolescence. And, although I&#x27;m not a fan of 
touchscreen devices at all, they are a perfect example of such a case. Look, 
you have a capacitive touchscreen which is (at least supposedly) better 
suited for direct finger input than the resistive one. You no longer have 
physical keys the user has to press. Why in the mother of fuck did you need 
to implement QWERTY as the primary input method there in the first place?!

Oh, because of Blackberries and Nokia Eseries and other similar devices with
physical QWERTY keyboards that were popular right before the capacitive 
touchscreen invasion? Were they sure that their audience intersected with 
the audience of later smartphones? Are they sure that whatever works for 
physical keyboards also works for virtual ones? Not to mention those 
physical keyboards were not comfortable to use by everyone, just because of 
their size: for mobile devices, the traditional 12-key layout is much 
better, with or without T9. With that layout, people can operate the phone 
with a single hand, and they, once again, convinced them that it wasn&#x27;t 
necessary. But if you definitely need to type with one hand, here&#x27;s &quot;swipe 
input&quot; for ya: gesture-based prediction method that amazes with its 
awkwardness, privacy invasion and inaccuracy. It&#x27;s a rusty technological 
crutch for the &quot;solution&quot; that has been inherently lame since day one.

No, I get that the traditional 12-key layout doesn&#x27;t make much sense on
touchscreens either. But the world doesn&#x27;t stop at these two options. 
Recently, I switched both Androids I&#x27;m actively using (a smartphone and a 
tablet) to the Thumb-Key keyboard. It&#x27;s FOSS (and available on F-Droid if 
anyone is interested), supports all the languages I currently need and 
combines the sparseness of 12-key layout with the features that only 
capacitive touchscreen can offer. In short, any letter of the alphabet of 
your choice is available with a single tap or a swipe gesture, but there is 
almost no room for mistyping (except the wrong swiping angle) and one can 
get used to it very quickly in a day or two. What&#x27;s more important to me 
(besides the fact that it collects no user data) is that you can position it 
to the left or right of the screen and adjust the key size, which lets you 
operate it one-handed with the maximum efficiency for the size of your 
thumb. You can also left it centered and operate it with two hands, or train 
each hand independently. For each language, the layout is adjusted to not 
require any swipes for the 9 most commonly used letters of that language, 
and a single swipe for all the others. And the selection of the 9 most 
frequent letters doesn&#x27;t only consider their usage in dictionary words, but 
also in the most frequent letter clusters of the language. That&#x27;s why the 
layouts you see there might not look so obvious at the first glance, but 
then, as you use it more and more, you start to understand why the letters 
are placed the way they are.

This application, as well as some others, also features some layouts
specifically tailored for two-thumb typing. Not quite comfortable for me but 
I understand when it could be useful too. By the way, similar input methods 
are very popular among the Japanese. Because, if we think about it, there 
really is no better way to enter kana on touchscreens. But this is in fact 
true for any other language: once you are trained, this is lightyears ahead 
of QWERTY with any swipe crutches. Now, a more interesting quesion is: how 
could we possibly improve the physical 12-key input on normal phones with 
keypads? Especially that we know how bad T9 itself can be. 

After what I have told you, I think the idea is on the surface: just use two
fast presses of adjacent keys as a &quot;swipe&quot; gesture, and leave a tap as a 
tap. This is simple and straightforward to program (and even more 
straightforward would be to just make any keypress twice, that would avoid 
having to use an internal timer). If the keypad itself is well-built, the 
experience is going to be as smooth as on the touchscreen and definitely 
faster than writing with the usual ABC1 method. To get familiar with the 
method, one might still need some on-screen hints to indicate which 
characters are available after the first press at which digits. This way, 
for example, English layout (as in Thumb-Key v4) would map to:

A: 66
B: 53
C: 75
D: 95
E: 99
F: 85
G: 25
H: 55
I: 88
J: 51
K: 54
L: 65
M: 45
N: 44
O: 33
P: 56
Q: 52
R: 22
S: 11
T: 77
U: 35
V: 57
W: 15
X: 58
Y: 59
Z: 89
Space: 0
&#x27;: 86
.: 80
,: 87
*: 8*
-: 8#
Uppercase shift: 63
Lowercase shift: 69
Numeric/ABC switch: #

Just imagine how the cellphone industry might develop if such a method
appeared about 25 years ago and became the mainstream not only in Japan but 
around the globe. Maybe I&#x27;ll try developing my own input method for KaiOS 
that implements this, but can&#x27;t promise anything at this point. What I&#x27;m 
trying to say is that, from time to time, it&#x27;s useful to start thinking out 
of the box to make you more productive in the most basic day-to-day tasks.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-06-16-when-unpopularity-is-good.txt</link>
<title>(2023-06-16) When unpopularity is a good thing</title>
<description><![CDATA[<pre>
(2023-06-16) When unpopularity is a good thing
----------------------------------------------
Two days ago, I finally got the chronograph watch mentioned in the previous
post, and can now reveal its exact model: Seiko SSB401P1. I was about to 
make a single sideband joke about this model number, but my bandwidth turned 
out to be too narrow for it. Functionally, this Seiko looks like a very 
simple quartz watch: no solar charging (but fear not, I already got two 
spare SR936SW batteries made by Seiko Instruments themselves), a date window 
with no auto-calendar, no longwave/BLE sync, only the basic timekeeping and 
a 60-minute stopwatch from the first glance. However, there are some details 
that reveal that there is more to it than meets the eye.

First, the overall look. This is something that all reviewers of this Seiko
lineup notice: there&#x27;s a lot going on around the dial, and it&#x27;s definitely 
looking busy while not cluttered. For me personally, this particular 401 
variant with it white-silver dial in a steel case with no unnecessary 
coating looks like a perfect summer season watch, especially on the stock 
grey-white-black symmetrical NATO strap. And this is another thing I want to 
point out: I was glad to find this particular variant to have a 22mm NATO 
strap out of the box, because, just like Invicta 8926OB, it&#x27;s not titanium 
and would be too heavy for me if it had a bracelet. And all other strap 
options... meh.

Next, I&#x27;d like to point out the feature that a lot of people consider useless
or excessive for some reason: 24-hour subdial. Again, I already mentioned 
that a day has 24 hours in it, and it would be much more logical to have the 
main 24h-dial, but, unfortunately, physical dimensions also contribute to 
(un)readability of such a scale, so I&#x27;m thankful for at least having the 
subdial. In fact, besides the potential usefulness of such a feature in 
bunker-like conditions, it is also convenient to have a quick glance at the 
rest of your day. For example, my current work involves having 12-hour-long 
on-call shifts so I can very quickly estimate how much is left until the 
end. To me, that is much more useful than having the third subdial only 
serve the purpose of hours display for the chrono.

And finally, the chrono. Something that really makes this 8T63 movement (as
well as its predecessor, 6T63) different from dozens of others with the same 
subdial layout. While being pretty basic in function and only having the 
start/stop and reset pushers (and anyone can intuitively get which is which 
even without the manual), it actually behaves like mechanical stopwatches: 
the chronograph second hand moves with 0.2s intervals and pushing the reset 
button makes both chrono-second and chrono-minute subdial hands instantly 
snap back to their zero positions. Apart from that, any color variant of 
this particular model has a distinct color for the chrono-second and 
chrono-minute hands. For instance, my SSB401 has them painted in black, 
while all other hands are silvery. This allows to visually distinguish the 
hands while looking at the running chronograph with no apparent trouble. The 
&quot;normal&quot; second hand is positioned on the lower subdial and moves with the 
usual 1s intervals, and I don&#x27;t have a slightest feeling of discomfort about 
all that.

Another feature related to the chrono is the tachymeter scale. Usually, I&#x27;m
not a fan of it, but that&#x27;s purely because of how it&#x27;s implemented - a bunch 
of numbers on a large and obnoxious non-rotating bezel. Here though, it&#x27;s 
implemented on the edge of the dial itself, in the most unobtrusive way 
possible. And the font is pleasant to look at, so it doesn&#x27;t bother me at 
all, despite being, you know, quite inaccurately marked for a reciprocal 
scale (in case you didn&#x27;t know, the function it&#x27;s showing is 3600/x, where x 
is the amount of seconds you&#x27;ve measured). Like, if the tachymeter scale 
shows exactly 80 or 75, it&#x27;s fine, but if in between, it&#x27;s not so obvious 
which value we&#x27;re looking at, so, unless you are fine with a very 
approximate result of your calculations, I would just suggest dividing 3600 
by your seconds manually. But, as a pure decoration that doesn&#x27;t increase 
your casing size, why not?

But you may ask: &quot;Isn&#x27;t a digital watch much more suitable for chronography
and other additional functions?&quot; Yes, totally! And if I were to only leave a 
single watch and get rid of the rest of my collection, the one I&#x27;d leave 
would most probably be Casio GMW-B5000D or GM-B2100BD. Or even sell my 
entire collection for a single MRG-B5000, because that is the ultimate watch 
in every aspect in my system of values (I just don&#x27;t think that spending 
$4000 on a watch is the right thing to do when my country is at war). 
However, in case you do have a choice which watch to wear today/this 
week/this month etc, not every one of them has to be absolutely perfect. And 
in this aspect, this Seiko easily falls into my top-10 or even top-5 among 
the ones I have, along with GMW-B5000D, GMB-2100BD, OCW-T200S and Citizen 
AS2050. Of course, all four of them are more expensive now where I live, but 
this Seiko easily falls into the same category when it comes to 
look/performance ratio. Besides, when it comes to the stopwatch function 
itself, here it&#x27;s instantly available and you don&#x27;t have to activate the 
chronograph mode (or rotate the bezel in case of analogue three-handers) 
whenever you feel the need to urgently time something. Just press the top 
button at once.

To summarize, I like this watch despite its obvious imperfections. I don&#x27;t
get why it&#x27;s so unpopular (as well as other 8T63-based models) but maybe 
this is what makes it so accessible to general public: those who _are_ 
interested in Seiko chronos can find it with no problem. It is very 
affordable compared to other Seikos and other chronographs with this 
particular subdial layout (I&#x27;m looking at you, Casio Edifice series - Y U NO 
make a worthy alternative within the same dimensions? [1]), it is stylish 
and reliable enough for day-to-day usage. To me, the only vital question 
that is still unanswered for now is the real accuracy of the movement. So, 
I&#x27;ve started the measurement and hopefully will return with the answer in a 
month of wearing it. 

--- Luxferre ---

[1]: no, EFV-590D is a cringe, not a worthy alternative, and EFS-S600D is an
overpriced hockey puck
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-06-12-on-watches-in-2023.txt</link>
<title>(2023-06-12) On watches in 2023</title>
<description><![CDATA[<pre>
(2023-06-12) On watches in 2023
-------------------------------
I have ordered yet another wristwatch. Will tell more about it when it
arrives (hopefully it happens this week). For now, I&#x27;ll just tell that it&#x27;s 
a purely-analogue chronograph. When looking for some info about it, I had 
found the same YouTube folks as usual (&quot;watch review channels&quot;) boasting 
about &quot;affordable chronographs&quot;, the cheapest of which costs twice the price 
of it. As if they are living on a different planet or something. And this is 
why, despite having shared some watch-related thoughts here and on 
chronovir.us, decided that it&#x27;s finally time to share my thoughts about the 
current matter of things. And when my new chrono arrives, the next post will 
be solely dedicated to it.

Let&#x27;s begin with the fact that if you&#x27;re living in a real society and talk to
real people, you&#x27;ll discover about three categories of them by their 
perception of wristwatches:

1) &quot;I don&#x27;t need watches and I think no one really does, it&#x27;s 2023, phones
are enough to tell the time and do everything else&quot;,
2) &quot;For me, a watch is a purely fashion item so it doesn&#x27;t matter how much it
costs or how accurate it is, only the look matters&quot;,
3) &quot;I use all the bleeding-edge technology to my advantage, so I migrated
from G-Shocks to smartwatches and am happy with that&quot;.

I say _about_ three categories because there is indeed a fourth one, which is
now, alas, totally outnumbered by this avalanche of morons and hipsters, the 
category who just uses normal watches exactly to tell the accurate time 
while appreciating their true mobility and autonomy. Unfortunately, due to 
us homo sapiens being outnumbered, it&#x27;s becoming increasingly difficult to 
find a watch that&#x27;s really suitable for performing its main function well 
while not looking too ugly. Because the market is aimed at the categories 2 
and 3. I&#x27;m not saying that there are few watches that are good as watches - 
hell no, there still are enough of them, but I&#x27;m saying that they are 
saturated with a pile of garbage released just to create an illusion of 
variety, making really good models harder to find. The situation is becoming 
even more difficult because of sellers worldwide who simply don&#x27;t know what 
they are selling, while successfully (because people don&#x27;t have another 
choice if the brand doesn&#x27;t have an official retailer in the country) making 
1.5x to 3x margin, and that&#x27;s the best case. Combine this with an overblown 
supply of fakes and OEMs/ODMs, and you&#x27;ll see how deep the needle is in this 
haystack that desperately needs to be burned down.

In a nutshell, everything looks like it has been artificially arranged to
drive people off buying normal watches in favor of &quot;smart&quot; crap, while 
leaving the &quot;luxury market&quot; almost intact. Some zombies even say this 
directly: &quot;either a Swiss mech or an Apple watch, it makes no sense to buy 
anything else&quot;. Ridiculous. Not saying I want a war to come to their homes 
like it came to mine, but I do want them to feel what it is to have no 
access to civil infrastructure for a long time. I want them to have no way 
to charge their gadgets at least for a month, and to have no authorized 
service available when their Rolexes/Tags/Omegas finally break down. If a 
disaster is the only way to make watch snobs appreciate simplicity, 
reliability and interchangeability, then I want them to go through it. It&#x27;s 
a purification process after which no one will dare to judge people by what 
they wear. In a healthy society, there shall be no &quot;luxury market&quot;.

As of now though, something needs to be done to keep track of this needle in
the haystack. I can create a page with my personal watch recommendations 
(and probably will do), but I can only talk about what I can personally find 
where I am and fully test it in terms of daily usage. Not to mention that my 
current capabilities of extending the collection are quite limited (not 
financially but logistically), and I will mostly talk about the watches I 
already have in it. Nevertheless, I hope it will give you an idea where to 
look if you want a good yet really affordable watch. So, as always, stay 
tuned. For now, just remember: wristwatches are not dead and have yet to 
have a final laugh.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-06-05-on-the-perception-of-minimalism.txt</link>
<title>(2023-06-05) On the perception of minimalism</title>
<description><![CDATA[<pre>
(2023-06-05) On the perception of minimalism
--------------------------------------------
It just so happened that I&#x27;m into (cellular) phones as one of my main
hobbies. Not the touchscreen zombifiers but something that current folks 
call &quot;dumbphones&quot; or &quot;featurephones&quot;, which mostly have physical keypads and 
non-touch interfaces. And I do have some ongoing researches about this 
topic, most of which are unfortunately in a frozen state, but now I want to 
talk about something else. Whenever I hear someone chooses a &quot;minimalist 
phone&quot; instead of a smartphone in 202x, it makes me cringe. No, reducing 
smartphone dependency is always good and I fully support that. The cringe 
comes from calling the replacement of your choice &quot;minimalist&quot;. Especially 
if this replacement still has smartphone hardware inside and runs either 
KaiOS or a stripped-down Android version. But most people don&#x27;t understand 
that even if we take a true featurephone with no camera/memory card/GPRS and 
with a monochrome display, like Nokia 1100 or 3310-2000, it still is far 
from minimalism despite being old and basic. And most phones in my own 
collection can be called old but not minimalist. Let me explain why.

You see, even the most basic phone today, with no camera or Bluetooth or
expandable memory or 3G/LTE etc, still has a lot of things that we take for 
granted but they are really optional with regards to the established GSM 
standard and NOT necessary for the phone to perform its main functions. 
Today&#x27;s schoolboys might not know but not every GSM cellphone (not to even 
mention analogue standards like NMT or AMPS) even had a built-in clock in 
the early days. In some phones, like Motorola M3x88 series, the clock was a 
hidden option that you could only enable in the hidden test mode (and, 
unless you had a special cable and flashing box, activating this mode was 
another big story). Earlier phones didn&#x27;t have any clock at all. Same for 
calculator. Interactive USSD queries? SIM-specific menus from your carrier? 
If your phone is from 1998 or earlier (like Nokia 5110), forget them. You 
won&#x27;t have them. Tri-band? Dual-band? What are you talking about? Dual-band 
was a great feature that allowed M3x88 owners to feel superior to owners of 
5110 that only had GSM900, or, in case of 5130 (yes, the true 5130, not that 
colored pseudo-musical nonsense), only GSM1800. If we dive even deeper and 
look at the early GSM Nokias (and by that, I mean the models like 1611), 
we&#x27;ll find out that non-Latin SMS support is also something very-very 
optional. And no, I&#x27;m not talking about sending, I&#x27;m talking about 
reception. Also, be ready to receive compound SMS messages as separate ones 
and to not be able to send more than 140 characters at all. And if that&#x27;s 
not enough, early 1610 variants didn&#x27;t have SMS sending support at all, only 
1610 Plus and 1611 had. And even earlier phones might drop SMS/USSD support 
altogether, along with call divert menus (why would you need them when you 
have standard GSM codes anyway...) How do you like such minimalism?

Still, even the earliest GSM phones (like Ericsson GH172 from 1992, that had
a _segmented_ LCD) had a basic set of functions that is inseparable from any 
handset to these days. Which are:

1) phonebook;
2) call log;
3) keyguard;
4) PIN code entry and configuration (although, just like call diverts, this
can be changed via standard codes);
5) network selection (manual and/or automatic);
6) sending DTMF tones during the call and automatically;
7) auto-redial (a great legacy from desktop phones - more on that later);
8) battery charge indicator;
9) signal level indicator;
10) volume control;
11) ringtone selection;
12) microphone mute;
13) screen language selection;
14) last call timer and total calls timer.

You might think this list is trivial to implement, but, believe it or not,
every single item of it was a separate task to solve back then. Obviously 
they were reusing some solutions developed for the previous (analogue) 
platforms, but, for instance, PIN entry and SIM-based phonebooks are 
something that needed to be researched from scratch. Even such simple things 
already involved so much work under the hood that I wouldn&#x27;t dare to call 
_any_ GSM phone &quot;minimalist&quot;. Something like AMPS or NMT was a totally 
different story. A story for another time.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-31-why-danmakus.txt</link>
<title>(2023-05-31) Why I consider danmaku the best (graphic-based) game genre ever</title>
<description><![CDATA[<pre>
(2023-05-31) Why I consider danmaku the best (graphic-based) game genre ever
----------------------------------------------------------------------------
As someone who used to have a secondary YouTube channel with &quot;Shmup-o-mania&quot;
in its name, I might be somewhat biased in this topic, you might think. But, 
as usual, that superficial observation can lead to false conclusions. Yes, I 
do like shmups (or, as snobs like to point out, &quot;it&#x27;s shoot-em-up and I hate 
anyone who shortens this name!!111&quot;) and consider the entire genre heavily 
underrated nowadays. I won&#x27;t go into the details of how and why (probably 
will write about it later if I have enough inspiration) but the modern shmup 
scene is really nearing the underground. And its most successfully surviving 
subgenre actually never was _that_ popular among the general public, but it 
is the small but active and truly independent community of hardcore players 
and aspiring developers that still keep it alive and well. In Japan, this 
subgenre is known as &quot;danmaku&quot;, but everyone else in the world might be more 
familiar with the &quot;bullet hell&quot; term. To the unsuspecting folks, danmakus 
might look like normal 2D shmups, just with more acidic and/or fantasy-like 
aesthetics (although neither of that is always the case) and much more 
bullets to dodge which makes it look like it&#x27;s impossibly hard to play 
(hence the name &quot;bullet hell&quot;). However, there are several _fundamental_ 
differences between danmakus and other shmups.

And the biggest of them, in my opinion, is how collisions are handled. In
traditional shmups, all sprites are treated as geometric shapes, and, 
obviously, you take damage when an enemy bullet sprite or an obstacle sprite 
hits your sprite, and an enemy takes damage when your bullet sprite hits the 
enemy sprite or a vulnerable zone of it. So, the main collision rule of 
traditional shmups is: the hitbox must be equal to the sprite shape unless 
explicitly stated otherwise. As sprite shapes can be (and usually are) very 
irregular, this makes collision detection very computationally expensive and 
non-trivial in general. Danmakus, on the other hand, treat things very 
differently: what you actually see on the screen has nothing to do with what 
can be hit. Your player&#x27;s hitbox is usually a small circle or rectangle in 
the middle of the actual sprite. And all the particles flying around are 
similarly constructed circles or rectangles. This allows to generate much 
more of them at the same time on the screen and compute their positions and 
collisions much faster. And only then, after they are determined, the actual 
sprite graphics is drawn around those hitboxes. Such mechanics allow the 
player to perform maneuvers that look impossible to anyone just watching the 
gameplay. And the player&#x27;s hitbox is usually highlighted as a bright circle 
dot in the center, making it even easier to know when you&#x27;re about to 
collide with something.

Another thing very characteristic for danmakus is little to no randomness in
the bullet patterns and their sequences. Bullets are released in 
preprogrammed &quot;waves&quot; that one can learn how to dodge. While traditional 
shmups sometimes introduce random elements to diversify the gameplay a bit, 
danmakus can only change the order of these waves, but even that is rare. 
That&#x27;s why danmakus&#x27; main focus is not the player&#x27;s reaction speed and 
agility but solely seeking the correct tactics to further advance through 
the game. To some extent, I&#x27;d agree with the statement that danmakus, as a 
genre, are more like action puzzles disguised as shmups. This approach also 
makes danmakus more suitable for procedural level generation, especially for 
&quot;boss rush&quot; type games where you don&#x27;t have to design levels themselves, 
just bosses and their bullet patterns. Finally, there are some danmakus that 
can hardly be classified as shoot-em-ups at all, because you just can&#x27;t 
shoot anything in them, only dodge waves of bullets and survive through the 
levels this way. But this doesn&#x27;t make such danmakus less interesting to 
play, as, as I stated earlier, their focus is on learning tactics. 
Oftentimes, regardless of whether you can shoot or not, you also get 
additional points (called &quot;graze points&quot;) if you have dodged a bullet whose 
hitbox was very close to yours, and you won&#x27;t see such things in any 
traditional shmup, because, again, they are much harder to calculate for 
non-primitive hitbox shapes.

And the paradox of all this is that danmakus only look complicated, but in
fact they are much easier both to program _and_ to play. And if I, one day, 
create my own (graphic-based) game engine for anything beyond a 
point-and-click quest (will talk about this another time), it will be a 
danmaku engine. Like shmups themselves, this subgenre of them definitely is 
the most underrated today for what it actually can offer.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-25-domain-specific-languages-as-a-mindset.txt</link>
<title>(2023-05-25) Domain-specific languages as a mindset</title>
<description><![CDATA[<pre>
(2023-05-25) Domain-specific languages as a mindset
---------------------------------------------------
Little has happened during these 4 days, but I had implemented a TinyChoice
interactive fiction interpreter in POSIX AWK (available in the Downloads 
section) and fixed collisions in my BroPix engine (probably will rewrite 
AWPix accordingly when I have time too). Anyway, I want to continue my 
research in the area of this kind of &quot;no-coding development&quot;. Because, if we 
really think of it, both TinyChoice and even Pix64 are perfect examples of 
an entire artificial language class called domain-specific languages, or 
DSLs. And it doesn&#x27;t matter in this case whether or not these languages are 
text-based or graphical (e.g. OpenSCAD is text-based but describes 3D 
objects, and Pure Data is graphical but describes sound generation). What 
matters is that they reduce the _conceptual_ complexity of creating new 
things in the particular domain they aim for.

But... How do we distinguish between DSLs and just well-implemented
&quot;traditional&quot; programming language libraries that just offer the right level 
of abstraction? Well, to me personally, a DSL is something that doesn&#x27;t 
require the knowledge of _any_ aspect of the language it&#x27;s written _in_ to 
create things with this DSL. Like, you don&#x27;t need to know C to write SQL 
queries to the DBMS written in C, you don&#x27;t need to know JS or C# to draw 
Pix64 games that would run in the official Pix64 environment or my BroPix, 
you don&#x27;t need to know JS or AWK to write TinyChoice story games, and so on. 
But, when someone says that Lisp/Scheme/Racket/Rebol/Red/Haskell/ML/Nemerle 
are perfect for creating DSLs because of their homoiconicity and/or macro 
system, it doesn&#x27;t make any sense. When you &quot;create a DSL&quot; in them, you 
don&#x27;t write in your DSL afterwards, you still write in 
Lisp/Scheme/Racket/Rebol/Red/Haskell/ML/Nemerle, obeying the same syntax 
rules and everything¸ just on a much higher level of abstraction. Same with 
Forth - you can abstract everything away with clever word definitions, but 
it still remains Forth with its own rules, like significant whitespace and 
having to put the operands before the words, that is, unless you implement 
all sorts of crazy tricks just to make syntax nicer-looking for 
non-programmers. For me, this is not what a true DSL looks like. A true DSL 
is _fully_ implementation-agnostic and generally cannot just be rephrased as 
a series of high-level library calls in the underlying language, otherwise 
the entire purpose of creating it is defeated as it wouldn&#x27;t reduce the 
conceptual complexity, only the amount of code you have to type/draw to get 
things done.

Take, for example, the two most popular DSLs out there, HTML and SQL. And
then think how much computational work is actually done under the hood to 
display a complex page correctly (even a static and unstyled one) or to 
execute your DB query in an optimal way. You cannot, in general, rewrite 
this as a series of function calls. And even in a miniscule amount of 
particular cases where you can, you&#x27;ll have to fully rewrite your program on 
every significant structural change in your document or query. And again, 
you don&#x27;t care which language or platform is underneath, you don&#x27;t have to 
be a programmer to use HTML or SQL to do what needs to be done. This is the 
power of DSLs.

Now, why would I write about all these (mostly) obvious things here? Because,
well, people still think of &quot;I&#x27;m not a programmer&quot; as a universal excuse for 
not getting familiar with creativity tools, and programmers, in turn, cannot 
think out of the box to radically reduce the complexity of their tools for 
everyone else. Even at the expense of their universality. No one suffers due 
to HTML or SQL not being Turing-complete. Because no one needs them to be 
Turing-complete in the first place to do their job. To understand this, you 
need to switch your brain to the &quot;task-first&quot; mindset. Of course, even in 
this case your choice must pass some sanity checks (e.g. whatever the 
problem is, Faildows is not a solution but a source of more problems) but 
you should not focus on the tooling too much. On the other hand, if your 
tasks are relatively complex and you can&#x27;t find a ready-made tool to solve 
them, perhaps THIS is the chance to put some one-time effort into creating 
your own DSL in whatever shape you see fit. This way, you only spend time 
once to deal with that complexity and then it pays you back many more times. 
And if your DSL is high-level enough (as it generally should be), it would 
allow to involve non-programmers in solving your (or their) problems in the 
same domain too.

Domain-specific languages allow us to tell computers what we need them to do
with little to no low-level details getting in the way. The less low-level 
details we need to specify, the better. And the biggest paradox of all this 
is that, despite not always being Turing-complete and always being built on 
top of &quot;traditional&quot; runtimes, DSLs are much closer to the initial, possibly 
never fully reachable, idea of computer programming itself.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-21-pix64-in-the-browser.txt</link>
<title>(2023-05-21) Pix64 in the browser: it was boring but necessary</title>
<description><![CDATA[<pre>
(2023-05-21) Pix64 in the browser: it was boring but necessary
--------------------------------------------------------------
Yes, https://bropix.luxferre.top/ now has a port of Pix64 to HTML5/JS. Well,
algorithm-wise, it&#x27;s more of a port of AwPix than the original idea, and 
sometimes it even handles collisions MORE correctly than the original 
ZappedCow&#x27;s Pix64 v1.2 in .NET. I won&#x27;t get into the details of what changes 
I had to make to get it to work correctly with the keyboard, mouse and touch 
input, or how I made the cart information available in the URL themselves, 
because you can look all this up in the source code itself, I didn&#x27;t 
minify/obfuscate anything. I also won&#x27;t stress on the changes needed to 
create a working KaiOS port out of this (and I guess you know that its 
creation is inevitable at this point). What I do want to stress on though is 
the kind of impact that popularizing such platforms can have.

You see, game programming is generally a complicated thing, and always has
been. Especially if you want to write a good game, not some mediocre trash 
pumped out by schoolbois every day using ready-made frameworks. Writing a 
good game involves knowledge of mathematics, logic, programming itself and 
graphic design (unless the game is purely text-based, but even then you need 
to think of the UX). And even before all that, you need a well thought-out 
idea of what&#x27;s gonna happen in your game from start to finish on every 
possible path the player can take. An yes, nothing of this can be skipped, 
no shortcuts can be taken. Kinda. Because there are some tools that make 
basic logic and art design a bit easier, and there even are some tools that 
claim to ease the programming part of this by implementing &quot;visual 
programming&quot; paradigm. Well, guess what, most of them fail, because 
representing the concepts at the same level, just in a different form, 
doesn&#x27;t decrease the complexity. You might as well just put C or Python 
statements into rectangular blocks and connect them with a bunch of arrows. 
No effect on the result.

Pix64, on the other hand, does &quot;visual programming&quot; right. Unlike the
object-oriented approach imposed by most modern programming languages and 
game engines, Pix64 imposes the object-first approach. You don&#x27;t have a way 
and don&#x27;t ever need to describe an object and then determine how it would 
display, you just draw this object and then determine how it would behave by 
assigning a particular color and shape to it, and everything else is handled 
by the engine itself. Yes, this kinda limits the variety of interactions you 
can have in your game, but, at this cost, the complexity is drastically 
reduced down to the point that you no longer need to know a single 
programming language and, what&#x27;s even more important, you only need to think 
of the interaction logic purely visually. This fact makes game creation 
accessible to much broader spectrum of people and makes Pix64 much 
higher-ranked among other &quot;visual programming&quot; tools in terms of the ready 
implementation of any task being much closer to the initial description of 
the task. You draw the game and only alter some details to make it work. 
Heck, a properly drawn Breakout playfield turns into a playable 
Breakout-like game here! This alone was enough for me to see that the 
potential of this thing is enormous and to start all my porting efforts. 

If I were in charge of the further Pix64 specification development, how would
I improve it? Well, first and foremost, I&#x27;d lift the 64x64 limit on the 
playfield and would allow 128x128, 192x192, 256x256 and so on. This would 
allow to put much more logic into the carts. Second, I&#x27;d use the two 
remaining colors - blue (#0000ff) and magenta (#ff00ff) - for some new 
mechanics. For instance, blue would denote a sprite generator that would 
emit a new arrow (&quot;out of the blue&quot;, heh) from its side when being hit by a 
sprite from the opposite side, and the arrow would be of the same color that 
hit it, and magenta might be a &quot;sprite portal&quot; - when any non-wall sprite 
touches it, it disappears and appears from the other side of any _other_ 
randomly chosen portal, or the same one if it&#x27;s the only one. These two 
color additions would allow to implement shooter mechanics and more complex 
puzzles, as well as introduce more randomness to the games.

However, even in the current state, Pix64 is just fine and definitely has its
own niche and appeal. It&#x27;s much more honest than any other &quot;fantasy console&quot; 
out there, and much more high-level too. I hope my ports will help to spread 
this wonderful idea to an even wider audience.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-17-fixing-the-only-serious-flaw.txt</link>
<title>(2023-05-17) Fixing the only serious flaw in the only good "fantasy console"</title>
<description><![CDATA[<pre>
(2023-05-17) Fixing the only serious flaw in the only good &quot;fantasy console&quot;
----------------------------------------------------------------------------
It can become interesting how a great legacy of one platform can lead to the
development of totally unrelated ones. Following the success of CHIP-8, some 
gamedevs decided to capitalize on it 40 years later by releasing the first 
product that goes by the term coined by themselves, a so-called &quot;fantasy 
console&quot; PICO-8. While the looks were implemented in the spirit of CHIP-8 
(or rather its direct successors, S-CHIP and XO-CHIP), PICO-8 s not a VM in 
full understanding of this term, and heavily relies upon modern Web engine 
to execute the games&#x27; Lua code with all the API. Needless to say that the 
success of PICO-8 spawned a gazillion of those &quot;fantasy consoles&quot;, both 
open-source and proprietary, ranging from easier to program for to straight 
out ridiculous. Unlike CHIP-8 though, almost none of them, including PICO-8 
itself, cares about saving the computational resources of the platform it&#x27;s 
running on, and essentially offers nothing new to the table. Except this one.

Enter Pix64. It&#x27;s a &quot;fantasy console&quot; with a very original approach to game
programming itself. Instead of offering a traditional approach where code is 
code and data is data, Pix64 makes everything data, i.e. graphics. And the 
color of the shape of these graphics elements solely determine their 
behavior. Considering everything must fit onto a single 64x64 PNG image 
(hence the name), the concept is already mind-blowing. But let&#x27;s see how it 
plays.

Being an ordinary indie project, no wonder the original Pix64 is hosted at
Itch ([1]) and doesn&#x27;t contain full specification, only a manual which is 
quite limited, that is, until you look at the live examples. But, before I 
tell you what it took me to look at them, let me first share the missing 
specification compiled from the knowledge of the manual and what I saw.

So, every Pix64 &quot;cart&quot; is a 64x64 PNG image, and &quot;multicarts&quot; can be created
by naming the .png files with an underscore and an ordinal number, like 
mycart_1.png, mycart_2.png and so on, then they will be automatically played 
in sequence one after another. Only 6 colors are allowed to be used in any 
Pix64 image, and every color corresponds to a different type of object:

* black (#000000) - empty/background;
* white (#FFFFFF) - wall;
* cyan (#00FFFF) - player;
* green (#00FF00) - goal;
* yellow (#FFFF00) - trigger/barrier;
* red (#FF0000) - enemy.

Alas, that&#x27;s all that the manual says about how Pix64 works. Everything else
following below has been deduced from the live Pix64 behavior and its 
&quot;tutorial carts&quot;.

So, let&#x27;s continue with the semantics of those colors (and let me take the
liberty of reordering them for you to better understand what&#x27;s going on):

* Black pixels are empty, which means everything can move to them.
* Cyan pixels belong to the player and are movable with a D-pad (all at
once), and yes, D-pad is the only form of control in this console.
* Red pixels belong to the enemy. If any cyan pixel collides with any red
pixel, the game is over.
* White pixels are walls, and in this case a wall means an impenetrable
object. No other object can cross it.
* Yellow pixels are barriers/triggers/&quot;magic&quot; walls (as they are called in
one of the tutorials). When interacting with any other pixels, they act like 
white walls. But, additionally, when a cyan (player&#x27;s) pixel touches a 
continuous group of yellow pixels, this group disappears.
* Green pixels are the game&#x27;s goals. Their behavior is exactly like the
yellow pixels, but Pix64 reports the successful completion of the game/cart 
when there are no green pixels left.

Now, there is the last concept left: moving objects. Yes, it is possible to
create objects that are moving by themselves. The movement speed and overall 
console framerate, by the way, are not specified anywhere in the manual or 
&quot;tutorial carts&quot;, so I had to guess. And it definitely looks like 15 FPS. 
Anyway, a moving object of any allowed color can be drawn as an arrow, but, 
as far as I understood, not just any arrow, but an arrow of specific shape 
and size. If we assume &quot;x&quot; as a pixel, then the smallest of the allowed 
&quot;arrows&quot; look as follows.

Up-left, up, up-right, down, down-left, down-right:

xx  x  xx x x x   x
x  x x  x  x  xx xx

Right, left:

x    x
 x  x
x    x

You can make these &quot;arrows&quot; longer in any dimension by increasing the number
of pixels on the flaps. Not very convenient to implement, but I&#x27;m OK with 
this. But what about the semantics? Well, arrow objects are always processed 
as a group of pixels and obey all the semantics described above, except the 
following bits:

* Arrow objects drawn in cyan color are not controllable with the D-pad.
* When an arrow object collides with another object (arrow or non-arrow) and
this collision doesn&#x27;t lead to game ending or disappearance of this object, 
it changes its movement direction by being redrawn according to its new 
direction, only changing the axis where the colliding pixel has been found. 
I.e. if an arrow up collides with a wall directly up, it becomes an arrow 
down, and if a down-left arrow hits the corner wall, it becomes an up-right 
arrow. If, however, it only hits the left wall, it becomes a down-right 
arrow, and so on. Note that for both vertical/horizontal and diagonal arrow 
objects their linear dimensions are fully preserved upon transformation.

The final important remark is that any moving pixel, regardless of how it&#x27;s
being moved (automatically or manually), wraps around the screen if it 
doesn&#x27;t encounter any obstacles.

And yes, that&#x27;s it. That&#x27;s the entire specification as I understand it. Now,
even if all this sounds a bit complicated, it really isn&#x27;t once you take a 
look at it. And now, we return to the main and the biggest issue of Pix64: 
it&#x27;s written in .NET. On top of this, it&#x27;s written using the MonoGame 
framework. Why?.. Just fucking why?.. And yes, the author told that the 
Linux build is untested, and I totally felt that when I saw it trying to 
find &quot;soft_oal.dll&quot; instead of &quot;soft_oal.so&quot; it should look for. I ended up 
launching it under WineMono, which, to be honest, really pissed me off, 
since the author boasted about the &quot;cross-platformness&quot; of the 1.2 release. 
But still, now that we have sorted everything out on how this console works, 
can we just create a normal cross-platform port of it without all the .NET&#x27;s 
bloat and everything?

Yes, we can. At first, I was really tempted to do this in JS, but then I
realized this would put me into the league with all the dozens of other 
&quot;fantasy consoles&quot; out there that can&#x27;t work with anything but Web 
interfaces and depend on a no less bloated Web browser. Well, I still plan 
on a JS implementation (at least for the KaiOS porting purposes), but for 
now I decided the following: why not reuse my fresh experience gained with 
POSIX AWK when writing a CHIP-8 emulator? Especially here, where there is 
less routine opcode work and more creative thinking on new problems.

The first such problem is, of course, having to only use full blocks for
pixels and to render full 64 lines. We cannot use any half-blocks or 
quarter-blocks since any two adjacent pixels can easily have different 
colors. However, 64 lines at once definitely is a problem for most 
terminals. What do we do to reduce them to 32? Well, we still use an upper 
half-block and color the upper pixel as a foreground and lower pixel as a 
background, whatever they are. Easy!

The second problem is having to decode PNG using POSIX AWK (unlike JS where
we could directly read pixel data from the canvas we have written our image 
to). Well, I guess it&#x27;s fair to introduce the netpbm dependency here and to 
just convert the PNG file into a nice PPM pure-ASCII text with the png2pnm 
-n command, where the &quot;-n&quot; switch makes it generate the output in the P3 
format instead of binary P6. The P3 format is extremely simple to work with: 
the first line is the &quot;P3&quot; header, the second line consists of 
space-separated width and height decimal values, and the third line contains 
the maximum color value, which is usually 255. Afterwards, a raw RGB decimal 
number stream follows (normally, 4 pixels per line, but that&#x27;s not specified 
anywhere) where every number is also delimited with a whitespace. So, as you 
can see, this format is ideal for processing with AWK because it consists of 
whitespace- and newline-separated decimal values only.

The third and final problem, and perhaps the most difficult one, is to
properly handle the behavior of arrow objects, starting with their detection 
in the first place. They are like sprites, but with the rules for individual 
pixels still in place. And if a pixel finds its way into a larger arrow, 
what shall we do? I understand this is an edge case but we need to handle 
them somehow. Hence, we need to shape a stricter definition of what can and 
can&#x27;t be considered an arrow in our Pix64 engine. My most rational take on 
this problem is like this: we determine _all_ objects, moving and 
non-moving, as sprites by their initial positions of adjacent same-colored 
pixels, and if any two of them are placed close enough to not being able to 
detect an arrow, it&#x27;s the game author&#x27;s fault only. But then, during the 
gameplay, if such a situation occurs, all collisions are processed as usual.

Yeah, complexity has to live somewhere, remember that. And in an engine where
all the logic is defined by interaction of moving arrow-like sprites, it&#x27;s 
natural that the most complex stuff is going to be exactly there. First, I 
decided to move away from the naive approach of pure pixel-by-pixel 
recursive sprite detection and to use the &quot;cross method&quot; which allowed to 
somewhat reduce the amount of nested calls and make MAWK with its 
1024-element call stack a bit more happy. Second, shape detection is also 
multi-stage, but once the sprite box itself is determined and the positions 
are sorted in order, it&#x27;s rather easy to match the X coordinate pattern to 
determine a particular shape unambiguously, also considering that there must 
be a particular amount of active pixels in a valid arrow sprite. After all, 
the main issue with my POSIX AWK implementation was, as always, inability to 
get timely keyboard input, so some simplest games like race.png are really 
unbeatable on my current setup.

All in all, AWPix is a curious prototype, but that&#x27;s it, a prototype for now.
As usual, I&#x27;m sharing whatever I have in my Downloads section (which now 
lives on a separate Gophermap instead of the main page), and the next part 
of the Pix64 journey will be a JS implementation for browsers and, of 
course, KaiOS. So... to be continued! 

--- Luxferre ---

[1]: https://zappedcow.itch.io/pix64
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-13-changing-the-pace.txt</link>
<title>(2023-05-13) Changing the pace</title>
<description><![CDATA[<pre>
(2023-05-13) Changing the pace
------------------------------
From now on, after 40 days of continuous phlogging, I&#x27;m going to post here
once or twice a week instead of posting every single day.

No, I&#x27;m not abandoning the platform. In fact, I have nowhere else to go. I&#x27;m
going to continue developing this place as my primary alternative to the 
&quot;Big Web&quot;. New sections and articles are going to appear on the main 
Gophermap too. However, there are three things. First, I don&#x27;t want to turn 
this into a routine and force myself to write at least something even if I 
have nothing significant to write about at the very moment. After all, 
Gopher is often associated with the &quot;slow movement&quot;, and for a good reason. 
Second, I feel the need to concentrate more on the creation process itself, 
not on writing about it every now and then. I&#x27;ll gladly share any results of 
my work but I don&#x27;t want to speed myself up anymore just to have something 
to share here on the next day. Because, combined with a full-time job I have 
and the health and overall living conditions far from ideal, it can be 
exhausting, you know. Third, the summer is coming, and despite all the 
circumstances, I&#x27;m going to spend more time outside and often just won&#x27;t be 
able to physically update this phlog (theoretically, I have enough skills to 
automate posting from anywhere, but this kinda defeats the purpose, as my 
posts here are to be well thought-out and this phlog is not to be a twtxt 
clone).

Also, I don&#x27;t want to fixate on specific topics or post lengths. Yes, my
first posts were mostly about computing related stuff but that&#x27;s not the 
only type of content I&#x27;m going to share here. And don&#x27;t expect every post to 
be a long article or an essay. In fact, I realized that, from time to time, 
I am kinda violating some traditions of this network by writing too long 
posts that are hard to swallow by some implementations like my patched 
Pocket Gopher LX. But on the other hand, if I have a lot to say on a 
particular topic, I will. And no one can stop me.

So, just to let you know: if a new post appears here at least once a week,
then I am alive and well and didn&#x27;t lose access to my VPS. Viva la Gopher!

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-12-cp-mv-or-the-9001th-post.txt</link>
<title>(2023-05-12) cp != mv, or the 9001th post about copyright and copyleft</title>
<description><![CDATA[<pre>
(2023-05-12) cp != mv, or the 9001th post about copyright and copyleft
----------------------------------------------------------------------
I didn&#x27;t want to write about this. I really didn&#x27;t. But it looks like I have
no other choice.

As long as I am dependent on third-party hosting, I can&#x27;t, for instance, just
copy the binaries of the Timendus&#x27; CHIP-8 test suite 4.0 into my repo 
without having to rehost their Octo source code. Because GPLv3. And Drew can 
remove my repo because of this, since practical sense doesn&#x27;t matter to him. 
And the fact that Octo is an assembler-as-a-service (that can go down 
anytime, so any source codes written in it will become unbuildable unless 
someone makes a full backup of that webpage) doesn&#x27;t matter either. The 
question is: does this sound like freedom to you?

To me, it sounds even sillier than the copytards&#x27; statements that EULA offers
any protection. Let me tell what I think of them, by the way. First, being 
able to reverse-engineer the details that really matter had never been 
stopped by any EULA. Second, copying information doesn&#x27;t remove it from its 
original place. Third, if you need a large &quot;DO NOT COPY OR WE&#x27;LL PUNISH YOU&quot; 
sign to just be able to sell a single result of your work again and again, 
then your work is worthless in the first place and deserves to be pirated. 
Because, besides donations, really good product owners receive most of their 
money for professional support and regular updates that offer new useful 
features, i.e. for real work, not for selling thin air in the form of 
licensing keys and the right to copy once and launch on a single machine. 
The subscription model is an even worse form of the same slavery. Better to 
avoid such software altogether.

GPL neckbeards are on the other extreme end of the same scale. The
anti-freedom they introduce doesn&#x27;t mostly touch end users though, but it 
touches creators and modders. Again, if a GPL-based algorithm is used in 
some proprietary software, no one can prove its presence there because no 
one has the source code. And if a GPL-based algorithm, in a (slightly) 
rewritten form, is used in some public domain software where no one can 
claim the authorship (by definition of the public domain), what can they do 
with this? But the more important question is, why would they even attempt 
to do anything with this? Isn&#x27;t it better for users AND creators to have 
more freedom? If no, which agenda are they pursuing? Why are they forcing 
everyone to increase entropy and waste precious energy resources on 
rehosting gigabytes of the same source code every time they want to make a 
small modification in their derivative works? Does this really help to make 
the world a better place or just serves to scratch some leftist ego in the 
shape &quot;we suffered to host this, so must everyone else&quot;?

You know, there is a reason I put all my personal projects (the ones that I
can publish) into public domain, and also started learning programming 
languages that have non-GPL primary/reference implementations (preferably 
public domain or its equivalent too). Because I&#x27;m allergic to any freedom 
restrictions, whatever side they come from.

And yes, remember, cp != mv. Sell your real work, not air or egos.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-11-impossible-solutions-just-take-longer.txt</link>
<title>(2023-05-11) Impossible solutions just take longer</title>
<description><![CDATA[<pre>
(2023-05-11) Impossible solutions just take longer
--------------------------------------------------
They said AWK was not suitable for handling binary data.

They said AWK was not suitable for non-blocking single-character input.

They said AWK was not suitable for arbitrary UTF-8 output.

They said AWK was not suitable for realtime virtual CPU emulation.

Especially POSIX AWK.

For me, it&#x27;s not interesting to solve problems that already have solutions.
It&#x27;s much more interesting to solve problems that might not have a solution 
at all.

See for yourself: https://git.sr.ht/~luxferre/DALE-8A

Yes, this is a CHIP-8 emulator in POSIX AWK, you&#x27;re not dreaming. Yes, it can
run ROMs compiled with Octo. Yes, it supports five emulation quirks, unlike 
my first attempt in JS that supported only two. Yes, it displays the 64x32 
screen on an alternate terminal buffer 2 virtual lines per 1 physical line, 
using three Unicode pseudographic blocks.

And trust me, this is far from the limit of what can be done with this
programming language that almost hasn&#x27;t changed since 1988. Just give me 
more time.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-10-the-perfection-of-imperfection.txt</link>
<title>(2023-05-10) The perfection of imperfection</title>
<description><![CDATA[<pre>
(2023-05-10) The perfection of imperfection
-------------------------------------------
One of my upcoming pet projects, as crazy as it might look, will have a
profiling part to determine the approximate amount of cycles that we need to 
skip on a particular machine to fit into a single 1/60 second frame. And I 
want to stress on the word &quot;approximate&quot; here. First, the timing method is 
itself imperfect (the sacrifice needed for full POSIX compliance) and only 
returns two figures after the decimal point. Second, the calculated result 
may vary between different runs and really depends on the current system 
workload. Third, we don&#x27;t need the exact cycle count there, only the order 
of magnitude by which we need to slow down every frame loop iteration, and 
on the slower systems/runtimes, the amount of cycles we need to skip may be 
10 to 30 times less than on higher performance systems. Regardless of the 
project itself, this alone is a very interesting challenge. What&#x27;s more 
interesting is that the exact precision is not possible to achieve and not 
generally required.

As another example, I have a circular slide rule (model KL-1) in my pocket
(holstered in a small pouch that originally belonged to a cheap Chinese 
pocket microscope). Two figures after the decimal point is the precision 
limit it can operate with, and even that can&#x27;t be achieved in all possible 
conditions. Does this make it useless? Hell no! It&#x27;s an efficient tool to 
quickly estimate square roots, sines/arcsines, proportions, reciprocals and 
products. And if I can&#x27;t measure length with resolution less than 0.1 mm, 
weight less than 0.1 g and time less than 0.5 s anyway, why would I need to 
calculate anything using these measurements with the precision less than 
0.01? If I, for instance, use my scientific calculator and get the result of 
7.8378332, it will be of the same value to me as the 7.84 I&#x27;m gonna get on 
the slide rule. Because the source measurements are just as imprecise to 
make the rest of decimal digits insignificant.

What I&#x27;m trying to say is that there is a whole bunch of engineering tasks
that don&#x27;t require us to operate with exact numbers. Just because we got 
used to them in the today&#x27;s world doesn&#x27;t mean we need them every single 
time. And this has its own beauty in it, as well as the fact that more older 
and reliable tools are still relevant to this day.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-09-chip-8-revisited.txt</link>
<title>(2023-05-09) CHIP-8 revisited</title>
<description><![CDATA[<pre>
(2023-05-09) CHIP-8 revisited
-----------------------------
The first human-scale VM for home computers in history, CHIP-8, is something
I am a bit familiar with. To the extent of development of my own CHIP-8 
emulation engine for KaiOS, which I obviously had called DALE-8. I have 
always been sticking to the original specs and not quite liking all those 
extensions like SuperCHIP, XO-CHIP and so on. The most basic original specs 
- 4096 bytes of RAM, purely monochrome 64x32 bitmap graphics with 8xN 
sprites, 35 opcodes, 16 keys and a beeper - are perfect for implementing it 
on the most exotic hardware available. No wonder that even a Rockbox version 
for Sansa Clip players has a built-in CHIP-8 emulator (although, obviously, 
not all 16 keys are supported there). 

It is remarkable how CHIP-8 was initially called not a VM, not an emulation
engine, but an interpreter. Even Wikipedia refers to it as an &quot;interpreted 
programming language&quot;. Well, in 1977 when it was first introduced on a 
COSMAC VIP that had nothing but a 16-key keypad and a reset switch, it was 
your pretty much only alternative to direct machine language programmming, 
and yes, it was much simpler than the RCA 1802 machine language despite also 
being fully binary. Especially if you understand how expensive in 1977 were 
any expansions (pun intended) that would allow to run any Tiny BASIC or 
Forth or other &quot;real&quot; interpreters, and these expansions would at least 
include more RAM and, obviously, a full-size alphanumeric keyboard. What&#x27;s 
even more interesting (and what I didn&#x27;t know until this very day) is that 
RCA Studio II cartridges also contained games created for a VM with an 
instruction set similar (although not identical) to CHIP-8 as opposed to the 
machine language. Sic!

So, what&#x27;s so remarkable about this VM, besides being the first of its kind?
Well, at least two things. First, every single VM instruction is exactly two 
bytes long. No more, no less. This makes decoding them as easy as possible. 
Second, with sprites being first-class citizens in CHIP-8 graphics, XORing 
them on top of each other is a very clever trick one can pull off to 
interact with so little memory, considering the VF register is also set 
automatically on any collision of the active pixels. So, CHIP-8 
architecturally has collision detection out of the box, which is just great 
in my opinion.

When it comes to the original CHIP-8 variant support, I guess I have a lot to
improve and reimplement. Of course, it&#x27;s going to be done in POSIX AWK. If 
my algos work fine there, they will work fine everywhere else. So, as 
always, stay tuned!

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-08-making-posix-awk-great-again.txt</link>
<title>(2023-05-08) Making POSIX AWK great again: introducing The Great Library</title>
<description><![CDATA[<pre>
(2023-05-08) Making POSIX AWK great again: introducing The Great Library
------------------------------------------------------------------------
As I had already said, AWK is a heavily underrated tool available for pretty
much every normal OS out there. And even in its standard-compliant version, 
it is capable of doing a lot of useful things. However, I have some plans 
for some rather ambitious projects, but these projects would include several 
things POSIX AWK doesn&#x27;t offer out of the box, and most of them are not even 
offered by GAWK:

* single-character keyboard input (both blocking and non-blocking),
* ASCII and UTF-8 codepoint conversion for further output or processing,
* loading binary files into arrays and saving arrays into binary files,
* some math functions available in C standard library but missing from
standard AWK for some reasons (like sign, floor, ceil, tan, cotan),
* and, of course, cross-platform bitwise operations.

To think of it, implementation of all this stuff would put AWK on par with
Python or at least Lua when it comes to interactive apps and binary data 
processing. But, of course, not everything here can be implemented with AWK 
itself, and sometimes we have to rely on some external commands. If we limit 
these commands and their options to the ones required by POSIX as well 
though, we still should be fine. The most obvious examples are how we can 
implement single-character keyboard input and loading binary files 
(considering all the null bytes they may have): we just use the od command 
that can shape everything as decimals of the byte width we specify. To 
achieve unbuffered character input, we can also use od in conjunction with 
terminal modes set with the the stty command. Since both stty and od are a 
part of POSIX, we can, as long as we only use the options listed there, 
safely use them to extend AWK script capabilities. And this is what I have 
used in my tgl.awk, which, as you may have guessed, stands for The Great 
Library, as I have called it on my main hoi.st page.

So, how do we use it? Although AWK generally doesn&#x27;t have the &quot;include&quot;
feature (except the most recent GAWK), we can just use several -f flags one 
after another, which is fully supported by the standard. There also is a 
caveat when using this library with GAWK: in order for any string-related 
functions to work correctly, it requires LANG=C environment variable to be 
set. It won&#x27;t hurt, however, to set this variable for any other AWK version 
either. So, the most correct way of running any TGL-enabled AWK program in a 
POSIX environment is like this:

LANG=C awk -f tgl.awk -f your_prog.awk [...args...]

This library is subject to change/update in the future, although I believe
it&#x27;s already close to its final state. I&#x27;m not going to introduce any more 
feature creep unless it&#x27;s absolutely necessary to continue with active 
projects. And in regard of what we can do with the already existing 
functionality, I hope you&#x27;ll find this out very soon.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-07-choose-freedom.txt</link>
<title>(2023-05-07) Choose freedom</title>
<description><![CDATA[<pre>
(2023-05-07) Choose freedom
---------------------------
&quot;Choose freedom. Choose an open-source OS. Choose CLI. Choose Vim. Choose a
feature phone with a known way to edit IMEIs. Choose an anonymous SIM card. 
Choose Ungoogled Chromium. Choose incognito mode. Choose Nitter and 
Invidious, Whoogle and Searx, Tox and IRCS, Tor and I2P. Choose Gopher and 
Gemini. Choose paranoia as the only way of survival. Choose Bash, grep, sed 
and AWK for everyday tasks. Choose writing all small but important bits of 
software yourself. Choose stack machines and OISC architectures. Choose 
Forth, Uxn and Subleq, CHIP-8 and VTL-2. Choose emulation. Choose knowledge 
about how old CPUs worked. Choose a reliable solar-powered scientific 
programmable calculator. Choose a slide rule and learn how to use it. Choose 
an accurate solar-powered wristwatch. Choose an AM/FM receiver with a proper 
tuning knob. Choose a trusted CD/MP3/cassette player. Choose an SDR or ham 
radio. Choose to build an antenna. Choose to learn Morse and ITA-2 codes. 
Choose books and manuals. Choose to study another language. Choose to use 
alternative energy sources whenever possible.

Choose freedom... But why would I want to do a thing like that? I chose not
to choose freedom. I chose something else. And the reasons? There are no 
reasons. Who needs reasons when you&#x27;ve got an iPhone and TikTok?&quot;

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-06-user-friendly-is-wrong.txt</link>
<title>(2023-05-06) "User-friendly" is wrong</title>
<description><![CDATA[<pre>
(2023-05-06) &quot;User-friendly&quot; is wrong
-------------------------------------
For a brief moment, let us remember that the greatest evil on Earth is done
under the guise of good intentions. The generally approved concept of 
&quot;user-friendly&quot; interfaces/software/hardware/whatever is one of not so 
obvious examples of such evil. No, I&#x27;m not saying that being able to quickly 
pick it up and start using it is a bad thing, not even remotely. It&#x27;s just 
what the general uninformed and uneducated public considers &quot;user-friendly&quot; 
is the source of pretty much everything bad that has happened to the tech 
industry throughout the recent 35 years or so. 

Take a computer mouse, for example. It&#x27;s a useful addon to any PC, even in
the form of a touchpad or a trackpoint, although I personally prefer 
trackballs. It is useful when working with graphics software or playing 
point-and-click quests. And that&#x27;s what it has been for the entire time 
since its invention, an addon. However, it was the introduction of Faildows, 
and particularly starting with Faildows 95, that spawned an entire 
generation of mouse-movers who no longer could work with CLI at all, nor did 
they want to. Because, all of a sudden, CLI became less &quot;user-friendly&quot; for 
them. Well, I understand that the CLI in DOS is far inferior to the CLI in 
normal (Unix-like) OSes but it still is far more superior to any 
mouse-driven general system GUI. The CLI is still really friendly to any 
user who knows how to read and type. Similarly, capacitive touchscreens had 
been around for quite a long time, but it was the introduction of certain 
pseudo-smartphones that spawned an entire generation of screen-tappers. And 
again, all of a sudden, pressing physical keys and even using a stylus 
became too complicated for the brainwashed masses. Which, by the way, didn&#x27;t 
prevent the same global-scale scammers from reinventing a stylus and adding 
it as a &quot;pro&quot; option to their products. Because, you know, pointing 
precision still matters.

Now, even among those who know how to read and type, the amount of
&quot;user-friendliness&quot; bullshit still skyrockets. Almost all of them, for 
instance, become hysterical when seeing vi or any of its derivatives (like 
NeoVim that even has mouse support for some actions). Because &quot;oh boy, it&#x27;s 
modal! And it doesn&#x27;t have our beloved shorcuts we got used to! How do 
people even use this archaic thing today?&quot; It&#x27;s not an exaggregation, I 
heard real people (that I didn&#x27;t have any reason to consider stupid before) 
say this. Now, I&#x27;m not a vi expert in any way, and use almost no advanced 
features that (Neo)Vim offers (except line numbering and syntax 
highlighting), but I have been writing all my programs and posts like this 
in it for a long time and don&#x27;t feel any discomfort. Whenever I have to use 
some other, non-modal software with more popular shortcuts, I often find 
myself automatically typing Esc:w instead of Ctrl+S/Cmd+S. And yes, speaking 
of Macbooks, vi-like editors are the only type of editors I can comfortably 
use with that cut-down piece of a bitten apple of a keyboard. Yet they 
convinced everyone a keyboard without Home/End/PgUp/PgDn/Ins/Del is 
&quot;user-friendly&quot;... Indeed, why have advanced cursor manipulation with a 
single dedicated key for each action when you have Fn modifier and 
multitouchpad? It&#x27;s so much more convenient! (not really...) But yes, this 
is their entire ideology: &quot;if we didn&#x27;t put it here, you don&#x27;t need it even 
if you think you do, we know better than you what you actually need, start 
thinking different (but not different from us) and stop complaining&quot;.

When it comes to the notion of website &quot;user-friendliness&quot;, it&#x27;s a panoptikum
I won&#x27;t even try reviewing, but the overall principle is about the same: &quot;if 
it doesn&#x27;t look like mainstream resources, it&#x27;s already not very friendly, 
but if it requires us to manually enter something, it&#x27;s a UX catastrophe to 
us!&quot; There might be more to it though. Some time ago, I even had received a 
feedback about one of my BananaHackers subpages, I guess it was W2D, about 
being &quot;not very user-friendly&quot;. Damn, what?! It literally has a single 
button! And it even tells you what to do instead if you press this button on 
a non-KaiOS device! What else should I have done to satisfy whoever gave 
such a feedback? If that was ca/in and not W2D, then it has three main 
buttons to press, and again, I wasn&#x27;t even obliged to create this page as a 
corresponding CLI helper utility would be more appropriate. 

No one can make software friendlier to users who themselves don&#x27;t want to be
friendlier to the software to start with. And yes, please stop confusing 
&quot;user-friendly&quot; with &quot;illiterate dumbass-friendly&quot;, then every piece of the 
story will fall into its place.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-05-attempt-to-standardize-mmio.txt</link>
<title>(2023-05-05) An attempt to standardize memory-mapped I/O in M/OISC machines</title>
<description><![CDATA[<pre>
(2023-05-05) An attempt to standardize memory-mapped I/O in M/OISC machines
---------------------------------------------------------------------------
When it comes to interacting with the outer world, all authors of &quot;esoteric&quot;
ISAs (that don&#x27;t have dedicated I/O instructions, that is) have their own 
ways of doing it. Someone reserves the last virtual memory cell as a 
standard I/O cell, someone (like me with NRJ) reserves the first three cells 
for input, output and port context, someone allocates entire non-overlapping 
blocks or interrupt vectors... It definitely is hard to adapt to a new 
approach every single time you learn a new ISA or even a different variant 
of the same ISA. Like Subleq, for example - it&#x27;s obvious that the howerj&#x27;s 
variant of Subleq-16, under which eForth can be run, only provides the 
last-cell standard I/O option and it&#x27;s not sufficient for the purposes of 
Dawn OS that was based on the same Subleq instruction (although of a greater 
bitness) but had full-featured graphics, disk I/O, non-blocking keyboard, 
mouse and even touchscreen controls. And that&#x27;s just a single example, there 
are many others I haven&#x27;t even seen in action yet. But I guess you get my 
point already - with so many different approaches to the same dead-simple 
problem, it&#x27;s hard to focus on more essential stuff. So, while this attempt 
may be totally fruitless, I&#x27;d like to try and propose a bitness-agnostic 
spec of memory-mapped I/O in such machines. Let&#x27;s call it EsoIO.

0. Introduction

The EsoIO specification defines the rules to interact with five entities (if
any of them is present on the target system):

1) standard I/O (as/if defined by the host OS);
2) serial I/O (keyboard, mouse, touchscreen etc);
3) raw video memory;
4) block-oriented I/O (disk, sound, network etc);
5) timers.

While raw video memory and timers can just be considered special cases of
block-oriented and serial I/O respectively, these cases do have special 
requirements to processing speed, so it is recommended to implement them 
with separate logic as described below. Note that the system is 
EsoIO-compatible if and only if all the listed parts it actually implements 
are implemented according to this spec. If the target system doesn&#x27;t 
implement any of these five entities, corresponding memory blocks may be 
used for any other purpose and the system still will be EsoIO-compatible. 
Also, EsoIO tries to be as non-invasive as possible and is based on some of 
already existing conventions.

All the numbers and addresses refer to cells, not bytes, unless explicitly
stated otherwise. The size of a cell is one machine word, which can be 
anything by the VM author&#x27;s design. In this specification, addresses are 
always signed integers and the indexing is zero-based (the first memory cell 
is 0). Negative cell addresses refer to the cells at the upper part of the 
virtual memory space, e.g. cell -1 is the last cell in memory, cell -2 is 
the second last cell and so on.

1. Memory layout in EsoIO

Provided the entire memory space consists of N cells and a single cell
address itself takes S cells, the layout is as follows:

Entity | Address(es)           | Description
-------|-----------------------|---------------------------------------------
Program| 0..N/2 - 1            | Program/data space (all positive addresses)
Video  | -N/2                  | Screen width W
Video  | -N/2 + 1              | Screen height H
Video  | -N/2 + 2              | Color depth C (how many cells a pixel takes)
Video  | -N/2 + 3..BK- 1       | Video memory itself
Block  | BK = -N/2 + 3 + W*H*C | Block I/O port number
Block  | BK + 1                | Block I/O buffer size
Block  | BK + 2..BK + 1 + S    | Block I/O buffer address (S cells long)
Block  | BK + 2 + S            | Block I/O trigger
Unused | BK + 2 + S..-4        | Unused space, can be used for anything else
Timers | -3 - (S+2)*T..-3      | T timer vectors (address and delay value)
Ser/Std| -2                    | Serial I/O port number (0 for standard)
Ser/Std| -1                    | Single-cell standard/serial input/output

There might not be any &quot;unused&quot; part and the block I/O memory and timer
vector memory areas may be adjacent to each other but they must not overlap.

2. Standard and serial I/O

EsoIO views standard I/O as a special case of serial I/O. If the current
operation refers to standard I/O, the cell at the -2 address must be set to 
0. Otherwise, it&#x27;s set to an implementation-specific port number that 
defines which device to interact with. In case with standard or serial I/O, 
input operation is always triggered by reading from the cell at the -1 
address, and output operation is always triggered by writing to this cell. 
During a single serial operation, only a single cell of data can be 
transferred. The -2 cell should be set to 0 upon program start, but in 
general it&#x27;s recommended to explicitly set it before every input or output 
operation (unless it&#x27;s performed in a loop).

3. Video output

EsoIO specifies pixel-based video output with three mandatory parameters at
the start of the upper half of the virtual memory: screen width, screen 
height and color depth. Every one of this parameters must fit into a single 
cell. In this case, color depth parameter only serves the purpose of 
indication how many memory cells (again, not bytes, but cells) a single 
pixel would take. Thus, the video memory itself that starts immediately 
after these three parameters, is W * H * C cells long. It is fully up to the 
implementation on how the screen contents refresh is triggered after 
updating the video memory: it can be immediate, it can be an internal 60 Hz 
interval timer and so on. The semantics of the video memory cell contents is 
also fully on the implementation: whether it&#x27;s RGB, YCbCr, HSL or something 
else.

The rationale behind populating screen width, screen height and color depth
in the virtual memory is to give programs a possibility to distinguish 
between different graphical environments and adjust their logic accordingly.

4. Block I/O

For block I/O, there are three parameters to set: implementation-specific
port number, input/output buffer size and input/output buffer address. After 
these parameters are set, an read or write operation on the trigger cell is 
required, which starts block input or block output operation respectively. 
The result of this action is expected to appear in the buffer we have 
pointed to in our parameters. If there are more parameters to the block 
operation than can fit into a single trigger cell (like file name, sector 
number or network address and port), they must be encapsulated in the buffer 
itself.

5. Timers

In EsoIO, timers are defined at the end of the memory before the
standard/serial I/O cells. Every timer consists of S cells for target 
routine address and 2 cells for the delay and parameters. The timing units 
(seconds, milliseconds, nanoseconds, CPU cycles etc) and the parameters 
(interval/one-off etc) are fully implementation-specific, as well as the 
condition under which the timer is considered armed. When a timer is 
triggered, the control is expected to be passed to the routine at the 
corresponding address. EsoIO does not limit the amount of timers to be 
created, but a specific implementation can.

Now, that&#x27;s it. I hope it helps at least a bit to streamline all the
different models into something interoperable, and promise myself to develop 
the next ISAs in accordance to this proposal too. Have fun!

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-04-if-stuck-with-busybox.txt</link>
<title>(2023-05-04) If stuck with Busybox but need to run Forth, use... Subleq</title>
<description><![CDATA[<pre>
(2023-05-04) If stuck with Busybox but need to run Forth, use... Subleq
-----------------------------------------------------------------------
It might sound even crazier than running Brainfuck on top of VTL-2 on top of
AWK, but yeah, there is an implementation of eForth ([1]) for a specific 
variant of Subleq architecture, the details of which I&#x27;m going to share here 
shortly and programs for which are distributed as .dec files, where a .dec 
file is just a list of signed decimal integers separated by whatever 
whitespace delimiter. And yes, this file format looks like a perfect target 
for AWK, so I couldn&#x27;t resist writing my own Subleq implementation in this 
language. And, to be honest, considering all the quirks AWK requires where 
plain C offers straightforward solutions, 26 SLOC is not a lot for such a 
useful single-instruction computer architecture.

Now, I am going to share the ready-made .awk file as usual on the main page,
but I want to go through every significant line of it here to explain what&#x27;s 
going on. As I already said, this program accepts a single .dec file as 
input in the form [busybox] awk -f subleq.awk program.dec and processes it 
line by line. But first, we need to define two helper functions:

function L(v) { # cast any value to unsigned 16-bit integer
  v = int(v)
  while(v &lt; 0) v += 65536
  return int(v%65536)
}

function getchar(c, cmd) { # POSIX-compatible getchar emulation with sh read
  (cmd=&quot;c=&#x27;&#x27;;IFS= read -r -n 1 -d $&#x27;\\0&#x27; c;printf &#x27;%u&#x27; \&quot;&#x27;$c\&quot;&quot;) | getline c
  close(cmd)
  return int(c)
}

These functions are mostly self-explanatory, but I have something to add. The
L() function could be simplified if we could use bitwise operations, but I 
decided to stay on the POSIX side and emulated everything with conditions 
and the % operator. It also ensures the result stays integer before and 
after the conversion. The getchar() function is necessary to emulate 
Subleq&#x27;s input logic and, as you can see, unlike the C version, here it 
requires some external shell processing so it is quite slow already. I only 
used POSIX-compatible command options for read and printf though. Here, we 
read a single character (which can be a newline, hence the null delimiter) 
and then display its decimal character code, which is cast to integer at the 
AWK side and returned to the caller after the subprocess is closed. Now, we 
can initialize our 64K virtual Subleq memory: 

BEGIN {
  for(pc=0;pc&lt;65536;pc++) MEM[pc] = 0 # init the memory array
  pc = a = b = c = 0 # reset the program counter and other vars
}

Once we&#x27;ve done this, we can start matching on the integers within the file
and filling our memory with the actual values:

{ for(i=1;i&lt;=NF;i++) if($i ~ /^[-0-9][0-9]*$/) MEM[pc++] = L($i) }

Here, the logic is like this. We iterate over every single input line, which
can contain any amount of fields. The default delimiters are fine but we 
need to iterate over every field we encounter. If the field matches the 
regex for _signed_ integers (the first character can be either - or a digit, 
any next ones, if present, can only be digits), we cast it into a 16-bit 
unsigned value using our L() function, set it to the current memory cell and 
shift the pointer to the next one.

Finally, once the entire file has been read and parsed, we can start the
actual execution process in the END block:

END {
  for(pc=0;pc&lt;32768;) {
    a = MEM[pc++]; b = MEM[pc++]; c = MEM[pc++] # fill the cell addresses
    if(a == 65535) MEM[b] = L(getchar())
    else if(b == 65535) printf(&quot;%c&quot;, MEM[a]%256)
    else {
      MEM[b] = L(MEM[b] - MEM[a]) # subtract the first 2 cells and cast
      if(MEM[b] == 0 || (MEM[b] &gt; 32767)) pc = c # jump if result &lt;=0 
    }
  }
}

Here&#x27;s how it works. First, we reset our program counter PC once again to 0
and start sequentially reading three values in a loop: A, B and C. These are 
the cell addresses our OISC operates on every cycle. Now, and this is the 
first quirk of this particular Subleq variant, we have two special cases: if 
A is set to -1 (which is obviously cast to 65535 as unsigned 16-bit value), 
we input a character from standard input and set it to the cell at address 
B, and if B is set to -1, we output the contents of the cell at address A as 
a character to the standard output (which is done much easier in AWK and 
doesn&#x27;t need a special method). If none of this special cases is true, we 
run the general Subleq logic: subtract the cell at address A from the cell 
at address B and write the result to the cell at address B, then jump to 
address C if this result is less then or equal to zero. However, you may 
notice that the code doesn&#x27;t specify the condition exactly like this. What&#x27;s 
the matter?

The thing is, and here is the second quirk, that this specific implementation
does all the casting beforehands and requires to map all negative 
subtraction results (from -32768 to -1) to the upper half of 16-bit range 
(from 32768 to 65535). And programs like eForth actually do check this to 
determine whether they are running on the correct Subleq VM version. For the 
same reason, we only iterate our program counter from 0 to 32767, as the 
program can only be loaded into the lower 32K of virtual memory. Higher PC 
values would be internally treated as negative and thus invalid. So, we 
change our &lt;=0 condition to check if the result is zero or above 32767. This 
way, everything works as expected.

And guess what, the .dec file of eForth does run under Busybox AWK too.
Extremely slowly but surely. I recommend the version of subleq.dec found on 
the JS version of howerj&#x27;s project, because the one from the repo is even 
slower despite having smaller size. So, even if you can&#x27;t compile anything 
for the target system (whatever it might be) but have an awk command there, 
you can run eForth programs just via this Subleq emulator. Ain&#x27;t it 
wonderful?

--- Luxferre ---

[1]: https://howerj.github.io/subleq.htm
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-03-ode-to-64k.txt</link>
<title>(2023-05-03) Ode to 64K virtual machines</title>
<description><![CDATA[<pre>
(2023-05-03) Ode to 64K virtual machines
----------------------------------------
Many, if not most, VMs and interpreters that are meant to be implemented on
old hardware or just simulating old hardware experience anywhere else, are 
designed to work with 16-bit address bus and, as such, the maximum of 65536 
bytes of addressable memory. This started back in 1970s (when CPUs 
themselves had 8-bit buses) with Wozniak&#x27;s SWEET16 and continues to this 
day, including but not limited to Uxn, VTL-2, CHIP-8 with all its flavors, 
PICO-8, Minicube64, OK64, many Forth implementations, most Subleq and other 
OISC implementations, and even my Equi. So, why does this work and why do I 
personally consider this amount of RAM (and 16-bit environments as a whole) 
optimal for day-to-day low-power human-scale computing?

First, let&#x27;s get the most obvious thing out of the way. In order to be as
efficient as we can, we need the maximum address to be equal to the maximum 
value of the machine word. Hence, for systems with 8-bit machine words, 256 
bytes of RAM would be architecturally perfect, but, you guessed it, it&#x27;s way 
too little and we can hardly fit anything in there... and if we do, let&#x27;s 
remember how hard it was to program for Atari 2600 which did. So, we rather 
allocate two 8-bit machine words (or one 16-bit machine word) to store our 
address, which means 65536 bytes of RAM, which, you guessed it, has been 
enough for an entire generation of home computers and gaming consoles, 
especially considering that there was even less of the actual RAM and a good 
part of the address space was dedicated to video, input, sound and other 
ports and internal needs.

OK, so we have found out why 64K of addressed space is the smallest
comfortable amount. Now, why not more? Well, we can do more but the next 
comfortable stop is 24 bits or 16 MiB. To be honest, programs that occupy 
space this large are already far from human-scale. Of course there are 
legitimate scenarios for storing large amounts of data (and not code) in RAM 
all at once, like BWT-based compression (which is generally better when the 
block size is larger). Well, in this case, you can optimize your algorithms 
to use processing methods more suitable for working with storage (e.g. in 
case of BWT, replace quicksort with an external merge sort). The point is, 
there should be virtually nothing to fill that much RAM with, otherwise 
something else is definitely going wrong.

I&#x27;m not saying that memory usage over 64KB per application/VM should be
prohibited, but I&#x27;m saying it must be heavily motivated. Otherwise, DIY and 
LPC projects and platforms will eventually come to the state that mainstream 
software/platform development is in today.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-02-on-semicolons.txt</link>
<title>(2023-05-02) On semicolons in those C-like languages where they are optional</title>
<description><![CDATA[<pre>
(2023-05-02) On semicolons in those C-like languages where they are optional
----------------------------------------------------------------------------
I won&#x27;t write a long essay this time. Just gonna say a simple thing. If a
C-like programming language (like JS or AWK) has optional semicolons, no one 
has a single fucking right to mandate them. Less characters mean less 
eyestrain. Period.

&quot;Oh, but unobvious errors like the lines starting with opening parenthesis!&quot;
If this is the case, do put a semicolon at the end of the previous line or 
even at the start of this one. It doesn&#x27;t mean you have to put them 
everywhere else. Do not be afraid of ASI, learn how it works and use it to 
your benefit.

&quot;Oh, but code minifiers!&quot; Only the lowest quality code minifiers require
semicolons. Ditch them. Their developers don&#x27;t understand the proper 
language syntax themselves and don&#x27;t know what counts as a real statement 
delimiter. Luckily, it&#x27;s 2023 and there are normally functioning minifiers 
for every mainstream programming language in existence.

Anyone who puts semicolons where they are not absolutely required is mentally
a virgin despite having a wife and three kids.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-05-01-thoughts-about-stream-compression-2.txt</link>
<title>(2023-05-01) Thoughts about stream compression, part 2</title>
<description><![CDATA[<pre>
(2023-05-01) Thoughts about stream compression, part 2
------------------------------------------------------
Yes, finally, on the first day of May, I have something new to tell you. In
fact, since the first compression-related post, I also had experimented with 
BWT (Burrows-Wheeler transformation) and MTF (move-to-front transformation) 
along with my ultimate RLUE scheme in different combinations, and come to 
various interesting results, including being able to compress a 2-second 
static yellow frame video in the raw YUV4MPEG2 4:4:4 format that weighs 
86400399 bytes into 119 bytes and successfully decompress it back (and yes, 
RLUE+BWT+RLUE combo beats gzip, RLUE+gzip or bzip2 in this scenario), but 
then I felt something else was missing: the final step to compress the 
excess entropy. At first I really thought of using Golomb-Rice codes (the 
ones I mentioned and tinkered with a bit in the first part) but then, 
obviously, practical common sense won and I decided to not introduce any 
additional complexity by operating on individual bits. I wasn&#x27;t fully 
satisfied with ready-made easy solutions like LZP either (although both BWT 
and LZP are plausible when you aren&#x27;t too memory-restricted). But then, what 
to use instead? Well, I have devised an encoding scheme called 4PE, which 
conveniently means both &quot;four-pair encoding&quot; and &quot;4-bit pair encoding&quot;. And 
now you&#x27;re going to understand why.

The main idea of 4PE is very simple. All byte values are divided into two
categories: from 0 to 15 and from 16 to 255. The upper category is the 
boring one: if a byte is from 16 to 255, we do absolutely nothing with it 
during both encoding and decoding stage. We must, however, record this fact 
somewhere, and I&#x27;ll get to that in a moment. If there is a pair of bytes A 
and B which are _both_ from 0 to 15, we can encode this pair as a single 
byte: [4 bits of A][4 bits of B]. With both bytes being e.g. 1, the encoded 
value becomes, you guessed it, 0b10001 = 31. This is why, for any bytes 
starting with 16 in the encoded stream, we need a way to distinguish whether 
it&#x27;s a literal or an encoded value. And the most compact way to do this is 
with a header byte, which itself just contains the positions of the 
following (up to) 8 bytes in the encoded stream are to be treated as 
literals. As the header byte is mandatory for each 8-byte block, this means 
every 8 bytes of input can be encoded into 5 (in an ideal situation) to 9 
(in the worst case scenario) bytes of output. To mostly shift our input 
values into the lower spectrum, we can use MTF, deltas and so on, but I&#x27;ll 
review practical applications of this scheme in my next post.

By the way, what&#x27;s the position order of bits in the header byte?
Theoretically, it doesn&#x27;t matter as long as the encoder and the decoder 
agree on this, but I recommend LSB to MSB as it takes less CPU instructions 
to process. In other words, the recommended header-based stream block format 
is like this:

{lf7 lf6 lf5 lf4 lf3 lf2 lf1 lf0} {b0} [{b1}] [{b2}] ...,

where every lfN is a flag bit to tell the decoder whether or not to treat the
corresponding input byte bN as a literal.

Now that you know the format, here are the algorithms. Let&#x27;s start with the
4PE encoder.

1. Allocate two 8-byte blocks I and O.
2. Attempt to read 8 bytes from the input stream into the block I. Record the
actual amount of read bytes as IL. If no bytes are read (IL = 0), halt the 
algorithm.
3. Initialize the processed amount OL and the header byte H by setting them
to 0.
4. For each input byte B at position P in block I (total IL bytes), repeat
steps 5 to 9.
5. If P is less than IL - 1, also read the next byte C = I[P+1]. If both B
and C are less than 16, go to step 6. If any of these conditions is false, 
go to step 7.
6. Set the O[OL] element to the value of B * 16 + C and increment P by 1. Go
to step 9.
7. Set the OL&#x27;th least significant bit of H to 1.
8. Set the O[OL] element to the value of B.
9. Increment OL by 1.
10. If OL is above zero, emit the value of H and then OL elements of O in
their order, all as plain bytes into the output stream. Go to step 2.

Now, let&#x27;s take a look at the 4PE decoder.

1. Set the header expectation flag E to 1. Initialize header byte H, input
counter IC and output counter OC by setting them to 0.
2. Read a single byte B from the input. If unsuccessful, halt the algorithm.
3. If the flag E is 1, set the header value H to the value of B and go to
step 8.
4. If the IC&#x27;th least significant bit of H is set to 1, emit the value of B
and go to step 7.
5. Emit the four most significant bits of B as a separate byte value.
6. Emit the four least significant bits of B as a separate byte value.
Increment OC.
7. Increment IC and OC.
8. If OC is more than or equal to 8, go to step 1, otherwise set the flag E
to 0 and go to step 2.

Besides being elementary to implement and suitable for pure stream
processing, 4PE also has some interesting properties. For example, in 4PE 
output, you can visually determine how many bytes you&#x27;ll need to read to 
decode every block just by looking at the header byte and its binary 
representation from right to left: 0 means 2 bytes and 1 means 1 byte, and 
the limit is 8. That&#x27;s why, for instance, 0xA0, 0x70 and 0x00 are 
effectively the same header byte that means that four pair-packed values 
follow. You can even store additional information in the upper part as it&#x27;s 
totally unused... but only in this case. The more ones are there in the 
lower part of the header byte, the more significant bits the upper part has 
too. At the other extreme end, the header value 255, which is 0xFF or 
0b11111111, means that 8 bytes follow and all are literal, none of them is 
packed which is the worst case scenario. Fortunately, it only takes a single 
eligible byte pair to break even, which is why 4PE can really benefit from 
techniques like move-to-front transformation if the data is correlated 
enough.

Well... That&#x27;s all folks. Next time, I&#x27;ll show how we can tie all this
altogether to create real-life compression pipelines. Stay tuned!

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-30-the-problem-with-xy-problem.txt</link>
<title>(2023-04-30) The problem with "XY problem": why it might not be a problem</title>
<description><![CDATA[<pre>
(2023-04-30) The problem with &quot;XY problem&quot;: why it might not be a problem
-------------------------------------------------------------------------
As I mentioned in my previous post, I&#x27;m greatly annoyed when I look for an
answer regarding a specific tool and get the results about how to do the 
same with anything except this particular tool. I also said it&#x27;s probably 
because forum know-it-alls think it&#x27;s a kind of &quot;XY problem&quot; and don&#x27;t 
consider the fact you have a reason to only use the tool you&#x27;re asking 
about. Now is the time for me to tell what I think about this phenomenon and 
people who constantly talk about it.

First of all, in case you don&#x27;t know and didn&#x27;t visit any relevant &quot;homegrown
psychologist&quot; webpages (as I like to call them), and haven&#x27;t even read a 
corresponding Wikipedia article (Gopherpedia is nice btw), let me reiterate 
what this &quot;XY problem&quot; is. It&#x27;s when someone asks about how to do X while 
actually meaning to do Y, but not asking directly how to do Y. Regarding 
computing, a canonical example these homegrown psychologists like to display 
is when someone asks how to remove the last three characters from a string 
when in reality they want to remove a filename extension (which, surprise, 
can have more or less than three characters). I fully understand why they 
like this example: in this case, the &quot;XY problem&quot; is evident and actually 
does pose a problem. In real life though, things may not be so simple, and 
usually aren&#x27;t.

Imagine you&#x27;re working as a tech support engineer or, more realistically,
just are a forum volunteer who genuinely wants to help people. And you are 
faced with a question from someone where you spot an &quot;XY problem&quot;. But you 
are not sure about this yet. You really want to be helpful, so you start 
asking more questions to the asker, like, &quot;do you mean to do Y instead of 
X?&quot; or &quot;why do you have to use the X tool or approach instead of Y?&quot; and so 
on. And then the asker tells you that more details are unavailable because 
of NDA. Or that this particular set of tools is the only option available. 
Or that Y requires much more effort with X giving the same result in their 
case. Or something else. It&#x27;s an awkward enough situation already: you are 
the one who is supposed to give reasoning and explanations, not they, they 
already feel uncomfortable at this point. But, instead of simply giving up 
and responding that you don&#x27;t know how to do X or responding with the answer 
to the _initial_ question (if you know it), you continue to tell the asker 
how X is wrong and how to do Y. What reaction are you gonna get? Most 
likely, the asker will never come back to you. You didn&#x27;t help and 
discouraged this person (and, if the conversation was in a public place like 
a forum, anyone who saw it) from coming to you again anytime soon. All 
because you think you know everything better than everybody else.

You might suppose this example is exaggregated, but no, it happens all the
time on well-known tech-related online resources. I fully understand that 
lots of questions on those resources are naive and best and sometimes 
outright stupid (trust me, I&#x27;m a semi-active tech group moderator and a 
forum topic ex-curator myself), but it doesn&#x27;t give anyone the right to 
treat group members who just come for answers as total idiots every single 
time. They might have their reasons why they chose to do X and not Y in the 
first place, and they are not obliged to explain these reasons and 
everything behind them. Everything else except what they asked about is none 
of your fucking business. Not to mention there are users like me, who still 
might be noobs at some areas and still need to ask questions in these areas 
but, other than that, fully know what they are doing. Imagine yourself 
asking a fully motivated and correct question just for some arrogant dork to 
XY you away from the answer.

By the way, I also don&#x27;t like the name itself: why not &quot;A-B problem&quot; or
something else that doesn&#x27;t look like the problem is with males (who carry 
XY chromosome set)? I&#x27;m not _that_ paranoid, but why, among all other 
options, did that particular name stick around? Makes one think twice about 
who is actually benefitting from spreading this concept across the 
mainstream information sphere.

Anyway, moral of the story and my advice to anyone who already has embraced
this concept: stop thinking you&#x27;re smarter than everyone who asks you 
questions. At least stop assuming that by default. It is generally much 
better to only answer what you&#x27;re asked about, and if you don&#x27;t know the 
answer to this particular question, don&#x27;t be afraid to say this and move on. 
In any case, you won&#x27;t be able to help anyone who doesn&#x27;t want to help 
themselves, so don&#x27;t waste your energy on thinking for them.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-29-awk-is-underrated.txt</link>
<title>(2023-04-29) AWK is underrated, even in the POSIX variant</title>
<description><![CDATA[<pre>
(2023-04-29) AWK is underrated, even in the POSIX variant
---------------------------------------------------------
I want to take a little break from writing new stuff. But still, there is one
thing that bothers me a lot. Whenever I search any information on how to do 
this or that with AWK, especially on StackOverflow-like forums, I constantly 
stumble upon &quot;solutions&quot; using Bash, Coreutils, sed, Perl and even Python or 
Ruby. Anything but AWK the question authors initially ask about. I don&#x27;t 
know, maybe forum know-it-alls think it&#x27;s a kind of &quot;XY problem&quot; (which 
bears a bag of bullshit on its own, but that&#x27;s another topic) and whoever 
asked the question chose the wrong tool for the job and the tool they offer 
is better and so on, but damn! I&#x27;m fluent in Bash, Dash, Python 3.6+, JS 
(from ES3 to ES6 and whatever was next), C89 and VTL-2, and as such, I have 
a lot of options to choose from when writing new stuff, but I want to get 
fluent in AWK as well. So, if I (hypothetically) ask about how to do 
something in AWK, I want an answer about AWK, not about Bash or Python which 
I already can write just about everything in, or about Perl which honestly 
must already die. The know-it-alls can&#x27;t even consider the situation someone 
could be left with Busybox and nothing else, and that&#x27;s why they want to 
learn how to solve problems with AWK alone (which is the only proper 
programming language they can have on some systems, and Busybox sed is much 
more limited compared to GNU sed too), not because they don&#x27;t know Perl or 
whatever.

This is why I have given up on trying to find answers on forums and turned to
the sole point of authority: POSIX.1-2017, 2018 edition ([1]). It has some 
external links (e.g. for printf/sprintf format specifiers ([2]) or for 
extended regular expressions format ([3])) but this is where everything 
becomes crystal clear in terms of features we can use: anything not in there 
is some non-standard extension. Compared to the real-life AWK versions I&#x27;m 
using right now (Busybox and GAWK), I&#x27;m still missing bitwise operations 
but, to be honest, they are not necessary everywhere and can be emulated 
with normal integer arithmetics if required, although it would definitely be 
slower. To make sure you&#x27;re on the safe side (mostly), GAWK even has a 
--posix (or -P) flag to turn on the POSIX compatibility mode. I say &quot;mostly&quot; 
because no matter which options you set, different implementations handle 
null bytes in strings differently, and POSIX states the behavior is 
undefined in this case, so no one is to blame. For instance, in Busybox, you 
can&#x27;t have null bytes inside any string as they automatically truncate its 
contents, while in GAWK they are handled normally even if you don&#x27;t 
explicitly pass the -b flag (treat all characters as raw bytes regardless of 
locale). The POSIX specification is also missing GAWK&#x27;s epic TCP/UDP socket 
pseudo-filenames (starting with /inet) and bidirectional process 
communication operator (|&amp;). Yet, despite all this, I consider even the 
standard AWK criminally underrated.

Why? Well, think about how much programming around us really boils down to
processing text in one way or another. Rendering templates, parsing logs, 
scraping web pages, collecting reports, emulating terminals, marshalling 
objects between client and server, most popular client-server protocols and 
APIs themselves... Not even to mention how smaller Bopher-NG could become if 
rewritten in AWK, but first, it couldn&#x27;t be called Bopher anymore, second, I 
don&#x27;t have time for this effort for now. But you get the idea, right? 
Whatever task involving text where using C is too tedious, is a job for AWK 
with its record- and field-oriented engine with extended regular expressions 
available out of the box. And, if you really need it, basic math is already 
there too, up to square roots, logarithms, sines, cosines and arctangents, 
as well as your basic built-in PRNG with rand() and srand(). I don&#x27;t really 
know what prevented them to add bitwise operations to the standard but it&#x27;s 
already pretty functional for such a tiny package (and I already mentioned 
that even Busybox AWK that has them is just under 3K SLOC long). Of course, 
this tinyness comes at a cost of some sacrifice in convenience: no way of 
explicitly declaring variables as local (only implicitly, as unused function 
parameters), 1-based string indexing (as opposed to C-like languages where 
0-based indexing is commonplace), no multi-assignment in the initializing 
clause of for loops (although Busybox supports them but even GAWK doesn&#x27;t), 
a single format for numbers (stored as floating-point, even when explicitly 
cast to integers with int()), a single format for arrays (strictly 
associative and all keys are cast to strings), but all these are minor 
quirks compared to what this language is really capable of.

Another thing I&#x27;d like to mention is that AWK specification, while having
some minor updates to clarify things from time to time, has been staying 
like this for good 35 years or so, and this means as long as you adhere to 
POSIX, your programs will run on some ancient systems just as successfully 
as on the current ones. Yes, you may struggle to replicate the behavior of 
old C compilers and runtime libraries, you may find incompatibilities across 
various versions of Perl (not even to mention Bash, Lua and Python), you 
might have issues with compiling J2ME or other old Java 2/3 code on OpenJDK 
higher than 8 or running REXX on anything modern non-IBM, you can find your 
entire JS code not working on KaiOS 2.x because of some ES6 feature not yet 
present in Gecko 48 back then... but as long as you have an AWK there and an 
AWK here and you&#x27;re not using any non-standard extensions and null bytes in 
your strings, you can be sure your program will be fully portable to any 
standard-compatible implementation from 35 years ago and probably from 35 
years forth. And this is probably where the lack of big-market interest is 
even somewhat good: no one is going to try to shove in fancy useless 
&quot;features&quot; like OOP, template-based programming, decorators and other BS 
that breaks all compatibility and makes the codebase even slower and much 
bulkier.

And, as a good example of &quot;don&#x27;t try to fix what&#x27;s not broken&quot;, AWK is
definitely worth learning and using as an everyday tool.

--- Luxferre ---

[1]: https://pubs.opengroup.org/onlinepubs/9699919799.2018edition
     /utilities/awk.html
[2]: https://pubs.opengroup.org/onlinepubs/9699919799.2018edition
     /basedefs/V1_chap05.html#tag_05
[3]: https://pubs.opengroup.org/onlinepubs/9699919799.2018edition
     /basedefs/V1_chap09.html#tag_09_04
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-28-giving-ascii-art-the-third-dimension.txt</link>
<title>(2023-04-28) Giving ASCII art the third dimension</title>
<description><![CDATA[<pre>
(2023-04-28) Giving ASCII art the third dimension
-------------------------------------------------
I usually try not to remember my school years, but some cool things
definitely could be seen in that time. And one of them were stereograms, or, 
to put it more strictly, autostereograms. They were on the backs of some 
notebooks and day books, and to those schoolchildren who really could see 
them, they looked like magic or, to more tech-savvy ones like me, like 
holograms I had only read about in some old encyclopedias or seen in the 
hologram museum (yes, we did have one in our country). I had always been 
wondering, how does one draw such images? Little did I know back then that 
these graphical autostereograms (called SIRDS, single-image random dot 
stereograms, in case of randomized dots, or just SIS in case of hand-crafted 
image patterns) were preceded by SIRTS, single image random TEXT 
stereograms. I guess if I knew that it was possible to generate 3D images 
out of pure text abracadabra when I was a schoolboy, I... wouldn&#x27;t have made 
a lot of silly choices in my life that I did.

Anyway, now I understand the principle behind stereograms is very simple and
based on how our sight and brain perceive objects at different distance, and 
there also is a ton of software generating both text and pixel 3D images out 
of depth maps and some other parameters, but it&#x27;s baffling how few resources 
actually explain what&#x27;s going on and how to implement the same effect from 
scratch. The best one of the few, called &quot;SIRDS FAQ&quot; ([1]), contains 
probably the fullest explanation of the entire phenomenon and even has some 
code and pseudocode examples of the algorithm. Still, the best way to 
understand the algorithm is to code it up yourself, so I&#x27;ve created my own 
SIRTS implementation in AWK called Textereo and published it on the main 
hoi.st page as usual. It doesn&#x27;t use bitwise operations or any other 
non-standard extensions so should work on any AWK variant, but I only tested 
it on GAWK and Busybox. As an input, this script accepts a map file in the 
following format:

* line 1: space-separated (desired) image width and pattern length &gt; 8 chars
* line 2: entire alphabet of characters to build the image from
* depth map lines: either empty or a digit sequence from 0 to 7

Textereo follows the standard depth map convention that 0 is background and 7
is the highest level of embossment and visually appears the closest to the 
viewer. Normally though, SIRTS images don&#x27;t contain depth levels higher than 
3. The width is an important parameter that defines how your map will be 
positioned. All lines that have fewer depth digits than the width value are 
centered to fit the width from the first line. If the line is empty or 
contains only zeroes (fewer than the width), it&#x27;s fully filled with zeroes. 
This allows to adjust not only width but the height of the background 
canvas. The second number of the first line defines the base pattern length, 
and I&#x27;ll get to this parameter shortly.

Here&#x27;s an example of the 78x27 3D map in Textereo format using two depth
levels and 10-character base patterns:

78 10
abcdefghijklmnopqrstuvwxyz@/0123456789$%!#ABCDEFGHIJKLMNOPQRSTUVWXY



0000000000000000000000000000000000000000000000000
0000000000000000000011111111111111111111111111100
0000000000000000000011111111111111111111111111100
0000011111111000000011111222222222221111111111100
0000111111111110000011111222222222222211111111100
0001111110111111000011111222222222222222111111100
0011111000011111100011111112222212222222221111100
0011111000000111100011111112222211111222221111100
0000000000001111100011111112222211111122221111100
0000001111111111000011111112222211111122222111100
0000001111111111000011111112222211111122222111100
0000001111111111000011111112222211111122222111100
0000000000001111100011111112222211111222221111100
0011111000000111100011111112222211111222221111100
0011111000011111100011111112222211122222211111100
0001111111111111000011111222222212222222111111100
0000111111111100000011111222222222222211111111100
0000011111111000000011111222222222222111111111100
0000000000000000000011111111111111111111111111100
0000000000000000000011111111111111111111111111100
0000000000000000000000000000000000000000000000000



Note the blank lines before and after the digits. They allow to tweak the
height parameter to make sure the image is easier to visually perceive. And 
here is an example of what Textereo generates out of this map (sorry mobile 
users, you need to have 78-char width to see this properly):

Uc5rWnQtbQUc5rWnQtbQUc5rWnQtbQUc5rWnQtbQUc5rWnQtbQUc5rWnQtbQUc5rWnQtbQUc5rWnQt
3vB@n@ov0I3vB@n@ov0I3vB@n@ov0I3vB@n@ov0I3vB@n@ov0I3vB@n@ov0I3vB@n@ov0I3vB@n@ov
IF$fwYwBu9IF$fwYwBu9IF$fwYwBu9IF$fwYwBu9IF$fwYwBu9IF$fwYwBu9IF$fwYwBu9IF$fwYwB
EPI7KrhFWlEPI7KrhFWlEPI7KrhFWlEPI7KrhFWlEPI7KrhFWlEPI7KrhFWlEPI7KrhFWlEPI7KrhF
2@UK3A7OaB2@UK3A7OaB2@UK3A7OaB2@UK37OaB2@UK37OaB2@UK37OaB2@UK317OaB2@UK317OaB2
#uEyQB%R7V#uEyQB%R7V#uEyQB%R7V#uEyQ%R7V#uEyQ%R7V#uEyQ%R7V#uEyQ6%R7V#uEyQ6%R7V#
pcdoCJuBxgpcdoCJuBxgcdoCJuBxjgcdoCJBxjgcoCJBxjgcoCJGBxjgcoCJGBXxjgcoCJGBXxjgco
DzgMn#irjgDzgMn#irjDzgMn#irjDz@gMn#rjDz@Mn#rjDz@Mn#rjyDz@Mn#rjtyDz@Mn#rjtyDz@M
D!%3isSRtHD!%3isSRHD!%3ieSRHD!%53ieRHD!%3ieRHD!%3ieRHD!6%3ieRHyD!6%3ieRHyD!6%3
!MxGt/zF2N!MxGt/z2N!MxYGt/2N!MxYSGt2N!MxYSt2N!MXYSt2N!MXYbSt2NH!MXYbSt2NH!MXYb
%@ks3rLM$3%@ks3rL$3%@k8s3rL$%@k8ds3L$%@k8d3L$%@Rk8d3$%@RkO8d3$v%@RkO8d3$v%@RkO
Omq/ksansaOmq/ksansaOmq/ksasaOmqN/kasaOmqNkasaOJmqNkaaOJmhqNka7aOJmhqNka7aOJmh
xTbt9btvwoxTbt9btvwoxbt9btvwoxbpt9bvwoxbptbvwox%bptbvox%bputbv@ox%bputbv@ox%bp
kUCI4e18!skUCI4e18!skCI4e18!skCMI4e8!skCMIe8!skfCMIe8skfCMqIe8oskfCMqIe8oskfCM
rg6nETnT3Trg6nETnT3Tr6nETnT3Tr6%nETT3Tr6%nTT3Trt6%nTTTrt6%HnTTyTrt6%HnTTyTrt6%
c!bLbTPzqic!bLbTPzqic!bLbTPqic!bfLbPqic!bfbPqicY!bfbqicY!hbfbq3icY!hbfbq3icY!h
jXsOphd$RyjXsOphdRyjXsCOphdRjXsCJOpdRjXsCJpdRjXVsCJpRjXVsHCJpR6jXVsHCJpR6jXVsH
2Ft!Radwvo2Ft!Radvo2Ftm!Ravo2Ftmf!Rvo2FtmfRvo2F7tmRvo2F7ptmRvoA2F7ptmRvoA2F7pt
7HEsMg4E7R7HEsMg4ER7HEsMg4ER7HE%sMgER7HEsMgER7H9sMgER7H69sMgERu7H69sMgERu7H69s
7XcgSiygKP7XcgSiygK7XcgSiygK7mXcgSigK7mXgSigK7mXgSigKq7mXgSigK!q7mXgSigK!q7mXg
v2I8b%!eLbv2I8b%!eLb2I8b%!eLHb2I8b%eLHb28b%eLHb28b%eFLHb28b%eFnLHb28b%eFnLHb28
yB20POSRWCyB20POSRWCyB20POSRWCyB20PSRWCyB20PSRWCyB20PSRWCyB20P8SRWCyB20P8SRWCy
taBAzK7BGvtaBAzK7BGvtaBAzK7BGvtaBAz7BGvtaBAz7BGvtaBAz7BGvtaBAz67BGvtaBAz67BGvt
#hBD6s9Eko#hBD6s9Eko#hBD6s9Eko#hBD6s9Eko#hBD6s9Eko#hBD6s9Eko#hBD6s9Eko#hBD6s9E
FjcBa6w%6sFjcBa6w%6sFjcBa6w%6sFjcBa6w%6sFjcBa6w%6sFjcBa6w%6sFjcBa6w%6sFjcBa6w%
Iu#fzcAO4VIu#fzcAO4VIu#fzcAO4VIu#fzcAO4VIu#fzcAO4VIu#fzcAO4VIu#fzcAO4VIu#fzcAO
F$Kf!GtBvaF$Kf!GtBvaF$Kf!GtBvaF$Kf!GtBvaF$Kf!GtBvaF$Kf!GtBvaF$Kf!GtBvaF$Kf!GtB

If you look at this correctly, you should see a large 3 and a box on one
plane and then the D on top of that box on the second plane. The more 
distant-focused and relaxed your sight is, the better the effect. Don&#x27;t 
strain your eyes too much though.

Now that I have a working SIRTS generator written by myself, I can fully
explain how it works. Besides all the preparation of the alphabet and 
equally-wide map strings, the general algorithm is as follows (assuming all 
positions and indexes are starting with 0):

1. Determine the desired image width W and pattern length PL (in our case,
they are read from the first line of the input file). Select the alphabet A 
(in our case, it&#x27;s read from the second line of the input file).
2. For each line M of width W in the depth map, repeat steps 3 to 17.
3. Generate a character pattern string P of basic length PL. The characters
in P must be taken from the alphabet A. They can be chosen randomly or 
consecutively, they also can appear multiple times, but no _adjacent_ 
characters in the generated pattern P must be the same.
4. Shape a set of characters F consisting of all characters in the alphabet A
except the ones present in P (so that, in set notation, {F} + {P} = {A}).
5. Duplicate the initial pattern length PL as the current length L.
6. Set the pattern tracking pointer PP to 0.
7. For each depth value V in the current map line M, repeat steps 8 to 16.
8. Calculate the new pattern length NL: NL = PL - V.
9. Calculate the delta value D: D = NL - L.
10. If D &lt; 0, delete one character at the position PP from the pattern P (-D)
times and go to step 14.
11. If D &gt;= 0, perform steps 12 to 13 D times and go to step 14.
12. Retrieve a random character C from the set F and remove it from the set F.
13. Insert the character C into the pattern P at the position PP.
14. Copy the value of NL into L.
15. Emit a character from pattern P at the position PP mod NL.
16. Increment PP modulo NL: PP = (PP + 1) mod NL. End of iteration.
17. Emit a newline character. End of iteration.

You can find a bit different description of these steps at [1], but I think
my approach is more understandable. In short, we delete a character from our 
pattern at the current pattern position (before emitting anything at this 
position) if the depth value increases, and insert a new character from the 
set of unused characters if the depth value decreases. Since depth can 
change by more than 1 at a time, we must make sure the amount of 
deletions/insertions matches this delta. What all this does visually is 
creating a sharp boundary between layers where shorter patterns correspond 
to closer layers and vice versa. This is why we can&#x27;t make a base pattern 
length too short: we must make sure it allows us to distinguish between all 
depth levels.

A nice thing about this algorithm is that it also is fully line-oriented:
each line is processed independently and can have its own character pattern 
to draw with. That&#x27;s why it works so nicely with the AWK runtime. The only 
quirk I had to deal with was the fact that all string indexes in AWK start 
with 1 instead of 0. This is why several places in the Textereo code (which, 
by the way, is under 60 SLOC in total) have these 1&#x27;s explicitly added. But 
the algo itself is quite flexible and can be adapted to virtually any 
programming language, including...

Yes, you guessed it. But it&#x27;s definitely not for today.

--- Luxferre ---

[1]: https://the.sunnyspot.org/asciiart/docs/sirdsfaq.html
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-27-brainfuck-in-vtl-in-awk.txt</link>
<title>(2023-04-27) Brainfuck running in VTL-2 running in AWK. Why not?</title>
<description><![CDATA[<pre>
(2023-04-27) Brainfuck running in VTL-2 running in AWK. Why not?
----------------------------------------------------------------
Brainfuck is pure art. VTL-2 is pure constraint-driven engineering. Yet I
couldn&#x27;t find a single BF interpreter in VTL-2. Not even on the Rosetta 
Code. Probably because BF was created much later than VTL-2, in 1993, when 
almost nobody gave a damn about VTL and even about Tiny BASIC already. But 
now, in 2023, here it is - bf.vtl that I was able to run with lvtl.awk which 
in turn can run under the &quot;busybox awk&quot; command. I deliberately avoided 
using any bitwise operators in the bf.vtl code (although they might make it 
more efficient in some places) to make it portable to non-LVTL 
implementations. So, anyone who has access to any VTL-2 version should be 
able to pick it up from my hoi.st downloads and try it out.

In case you&#x27;ve stumbled upon my phlog by navigating here from some unexpected
place and still don&#x27;t know what Brainfuck is, well, it&#x27;s a programming 
language operating on an unbounded (although practically finite) virtual 
tape of integers (typically unsigned bytes from 0 to 255) that only has 8 
single-character instructions and ignores any other characters as comments. 
In the canonical BF, these instructions are:

+ increment current cell
- decrement current cell
&lt; move the cell pointer to the previous cell
&gt; move the cell pointer to the next cell
, read a character from stdin and save its code to the current cell
. write the current cell as a character to stdout
[ jump past the matching ] if the cell at the pointer is 0 
] jump back to the cell after the matching [ if the cell at the pointer is
non-zero

That&#x27;s it. These instructions (that, by the way, can be 3-bit-encoded in a
straightforward manner or, for instance, remapped onto phone keypad digits, 
oh shit, why did I think of this...) are everything that&#x27;s required to 
construct a Turing-complete language. Since its inception in 1993, Brainfuck 
gained really huge following, spawning countless variations, improvements 
and a whole &quot;esoteric programming&quot; subculture. But this very canonical 
variant of BF remains the most popular esolang to this very day. I think 
it&#x27;s safe to say it&#x27;s more popular now than VTL-2 itself, which is not an 
esolang but, as I said, a fine piece of constraint-driven engineering. Yes, 
I imagine that a BF interpreter for Altair machines could weigh much less 
than 768 bytes of VTL/VTL-2, but how useful would it be with programs that 
long and the RAM limit far less than the 30000 bytes of recommended minimum 
for BF? And that&#x27;s only operating RAM (the virtual number tape size), not 
counting the length of the program itself. I guess, when VTL was a 
necessity, it was too early for something like BF to even appear on the 
scene. But now, when RAM is usually not a problem even for the most embedded 
tech, we have a perfect opportunity to explore this alternative history 
timeline and see how an interpreter of the most popular esoteric programming 
language would look when written in the tiniest non-esoteric one. That&#x27;s how 
my first VTL-2 project began.

Yes, I finally started writing something new _in_ VTL-2, not just
interpreters _of_ VTL-2 in other languages. And you know what... I really 
like it. In fact, you get used to this weird assignment-driven syntax with 
strictly-LTR expressions so quickly that it might even become baffling to 
you how every other high-level language didn&#x27;t implement this and went with 
more verbose expression/statement syntax modeled after BASIC. And yes, I 
have the guts to say VTL-2 is pretty high-level and abstracts over a lot of 
machine-specific stuff, although it also requires you to be precise in your 
line number calculations. By the way, the whole idea of using #=0 as nop is 
indeed very clever and provides a concise way to do all branching and loops 
and everything. You just need to be good at math. Now I guess that BASIC, 
its expression model and all its IF/ELSE/FOR/WHILE/UNTIL had been invented 
for those who weren&#x27;t. No wonder that later BASIC generations ditched line 
numbering altogether.

But I digress, let&#x27;s go back to my BF-VTL. Just like with implementing BF in
any other language, the main culprit here were the [ and ] instructions. 
Because, as I already had written somewhere in this phlog, I hate 
parentheses matching, it slows everything down, especially here where you 
have to do code lookahead. Because if we imagine our data as a tape, we can 
also imagine our BF code as another tape, and every time the corresponding 
condition is triggered at one of these brackets, we have to fast-forward or 
rewind this tape to the matching bracket, stopping at intermediate brackets 
to increase/decrease the tracking counter. But here, brackets and their 
matching are a paramount part of the language, so they had to be implemented 
appropriately. At first, I just wanted to use a single subroutine that would 
be parameterized for each of the two cases and called from the instruction 
subroutines. This would allow to shorten their code significantly. But then, 
I remembered the limitation that we only have a single ! variable that 
already was being used, so I had to improvise and combine these two routines 
into one that returned directly to the original processing loop. 
Nevertheless, overall bf.vtl SLOC count turned out to be under 75 lines. Not 
bad for such a Very Tiny Language, don&#x27;t you think?

Of course, when run in LVTL-W, any program in BF-VTL runs extremely slowly.
Because that&#x27;s a triple interpretation layer. But the point is, it runs. And 
runs correctly. It&#x27;s a good benchmark of both AWK and VTL-2. For example, on 
busybox awk, I instantly got a segfault because the interpreter couldn&#x27;t 
allocate memory for all the operations. So, with trial and error, I found 
out that I had to tweak the L variable in line 12 and also delete lines 13 
to 18 to be able to run my Hello World. Still, on the second run, I could 
already see how much memory was leaking. Maybe AWK is not the right tool for 
the job, but it&#x27;s a lot of fun. No issues on GAWK though, maybe it has 
proper GC. But, for some factorial code like this one...

&gt;++++++++++&gt;&gt;&gt;+&gt;+[&gt;&gt;&gt;+[-[&lt;&lt;&lt;&lt;&lt;[+&lt;&lt;&lt;&lt;&lt;]&gt;&gt;[[-]&gt;[&lt;&lt;+&gt;+&gt;-]&lt;[&gt;+&lt;-]&lt;[&gt;+&lt;-[&gt;+&lt;-[&gt;
+&lt;-[&gt;+&lt;-[&gt;+&lt;-[&gt;+&lt;-[&gt;+&lt;-[&gt;+&lt;-[&gt;+&lt;-[&gt;[-]&gt;&gt;&gt;&gt;+&gt;+&lt;&lt;&lt;&lt;&lt;&lt;-[&gt;+&lt;-]]]]]]]]]]]&gt;[&lt;+&gt;-
]+&gt;&gt;&gt;&gt;&gt;]&lt;&lt;&lt;&lt;&lt;[&lt;&lt;&lt;&lt;&lt;]&gt;&gt;&gt;&gt;&gt;&gt;&gt;[&gt;&gt;&gt;&gt;&gt;]++[-&lt;&lt;&lt;&lt;&lt;]&gt;&gt;&gt;&gt;&gt;&gt;-]+&gt;&gt;&gt;&gt;&gt;]&lt;[&gt;++&lt;-]&lt;&lt;&lt;&lt;[&lt;[
&gt;+&lt;-]&lt;&lt;&lt;&lt;]&gt;&gt;[-&gt;[-]++++++[&lt;++++++++&gt;-]&gt;&gt;&gt;&gt;]&lt;&lt;&lt;&lt;&lt;[&lt;[&gt;+&gt;+&lt;&lt;-]&gt;.&lt;&lt;&lt;&lt;&lt;]&gt;.&gt;&gt;&gt;&gt;]

...you really want to use the compiled VTL-2 versions to run BF-VTL. By the
way, from now on, all existing LVTL versions except the initial A prototype 
(LVTL-O, LVTL-R and LVTL-W) are distributed in a single sharball, 
lvtl.shar.gz, and bf.vtl is included in the examples directory of that 
sharball.

And yes, since I already mentioned phone keypad mapping... Here&#x27;s how I&#x27;d do
this:

+ 2
- 8
&lt; 4
&gt; 6
, 7
. 9
[ 1
] 3
end-of-code #
run/halt 5
save *index#
load 0index#

One could even imagine using this for some sort of DTMF-driven programming.
For now though, I want this to just remain a concept.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-26-making-vtl-more-awkward.txt</link>
<title>(2023-04-26) Making VTL more AWKward</title>
<description><![CDATA[<pre>
(2023-04-26) Making VTL more AWKward
------------------------------------
First of all, I have created a restricted variant of my LVTL-O
implementation, called LVTL-R. It limits the amount of generic variables to 
A-Z, disables input evaluation (only numbers are accepted, as in VTL02sg) 
and makes the ! and # operators the only possible versions of &quot;or&quot; and 
&quot;xor&quot;. To be honest, it didn&#x27;t help with any SLOC count reduction compared 
to the full-featured LVTL-O, so I&#x27;m not even sure it&#x27;s worth publishing. If 
I do publish it, it will be in the same shar as the updated LVTL-O. Because, 
obviously, if I find any bug in LVTL-O, it must be fixed in LVTL-R as well.

All these C versions are cool and fast but there is a small problem: they
require a compiler to work. It&#x27;s not always possible to cross-compile LVTL 
from your host PC for some embedded system (especially if it&#x27;s something 
more exotic than a regular ARM), not even talking about trying to compile it 
on that embedded system itself. So you&#x27;re basically stuck with the Busybox 
and...

Wait. If this particular Busybox has awk command applet compiled in, it&#x27;s
something we can already live with. Because after I figured out all 
essential VTL-2 implementation details and wrote the reference code in C89, 
porting it to AWK turned out to be a piece of cake despite I don&#x27;t know AWK 
as much. AWK allowed to simplify a lot of things, including the overall 
program execution flow and memory management, of course. Not without its own 
quirks (like the fact that all arrays in AWK are associative and 
string-keyed) but it took several hours to port and test my restricted 
flavor of VTL-2 to this language **and** make it compatible with both 
Busybox and GAWK (btw, I&#x27;m not sure what other flavors support bitwise 
operations, but they are absolutely vital here). For instance, I was a bit 
surprised when I discovered that GAWK doesn&#x27;t support multiple 
pre-assignment in for loops while Busybox AWK allows them. I.e. a construct 
like for(i=0,l=length(s);i&lt;l;i++) {} will be valid in BB but raise a syntax 
error in GAWK. Well, the more you know.

Surely my scripted implementation of VTL-2, which I called LVTL-W and
published on hoi.st as lvtl.awk, is quite slow (especially on BB), because 
essentially it&#x27;s an interpreter running inside an interpreter. But it still 
is useful, and I, for instance, enjoyed playing Bulls and Cows on it as 
well. This way, I don&#x27;t even have to look for the native AWK implementation 
of this game because I already have the VTL-2 version up and running. You 
might ask a question like &quot;why implement an older scripting language in a 
newer scripting language that&#x27;s more superior by itself?&quot; Well, the answer 
is simple: to preserve the older language AND all the software written in it 
by making it possible to run on as much hardware and as many runtime 
environments as possible. Besides, it&#x27;s a great benchmarking tool as well: 
if some VTL-2 program doesn&#x27;t display any slowness when running in LVTL-W in 
BB-AWK, you can be sure it won&#x27;t be slow anywhere else.

Now, I have plans to port LVTL-R to some more platforms. But before I do
this, one more concept needs to be designed. And no, I didn&#x27;t forget about 
compression.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-25-i-wore-an-automatic-watch.txt</link>
<title>(2023-04-25) I wore an automatic watch for 30 days</title>
<description><![CDATA[<pre>
(2023-04-25) I wore an automatic watch for 30 days
--------------------------------------------------
In March 2023, I became an owner of Invicta 8926OB. Unlike the vast majority
of the watches in my collection, this one is mechanical, moreover, it&#x27;s 
automatic so it&#x27;s pretty thick for its 40mm diameter (not to mention the 
fact this diameter is hard to find in the lineup of this brand at all). But, 
like most other watches in my collection, it also has a Japanese heart, 
which is Seiko NH35A movement in this case. I put it on a NATO strap to make 
it not so heavy overall, started my measurement by setting the exact time at 
08:20 March 26, and fixed the deviation at 08:20 April 25. And it turned out 
to be +69 seconds. This means 2.3 s/day which is not at all a bad result for 
a mechanical watch that I never took off for overnight positional adjustment 
(because I almost never take off my watches for the entire period I wear 
them) and for ~$115 I paid for it. In fact, $120 is the maximum price any 
watch this inaccurate should cost in a healthy society, regardless of who 
made it and with which technologies and materials.

So, what can I say as a result of this testing? Can I live with it? Well, if
this were the only option I had, certainly. It performs much better than 
most autos, even much more expensive ones. However, with all the arsenal of 
longwave/BLE-synchronized mostly-solar-powered quartz masterpieces I have, I 
don&#x27;t really want to stick to something where I have to manually adjust the 
time every month and the date every two months, because it obviously has no 
auto-calendar, and fiddle around with manual hand setting every time we have 
the DST change. Sweeping second hand? I don&#x27;t give a single F because I got 
used to the jumping one too, and on top of that, some of my favorite 
analogue watches (like Casio GMB2100BD) don&#x27;t have it at all, resorting to 
mini-displays to show seconds instead. And even the simplest display already 
allows to greatly enhance the functionality without having to resort to 
various trickery pure-analogue models usually have. Although I actually have 
used the rotating bezel in this Invicta several times for timing things, at 
the end of the day, I&#x27;d much more prefer the timer I have in every ana-digi 
and almost every pure-digital Casio (and where there ain&#x27;t a timer, there 
surely is a simple alarm).

As a conclusion, while I think this 8926OB is outstanding in its class, I&#x27;d
rather have it stashed in the collection for the time no electronics can 
work anymore. Until then, I&#x27;ll stick to my Casios and Citizens and will be 
confident in their readings every time I look at them, and in their power 
reserve too, unlike any &quot;smartwatches&quot; which are mostly smarter than their 
average buyer. In fact, besides the calculators I already talked about, 
these solar-powered radio/BLE-controlled watches are the most useful pieces 
of LPC hardware that anyone can find and use in real life. Now, will any of 
them live as long as the fx-3400P calculator did, for instance? Only time 
will tell if they still will tell the time in 30 years.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-24-lvtl-as-the-starting-point.txt</link>
<title>(2023-04-24) (L)VTL as the starting point for LPC-oriented applications</title>
<description><![CDATA[<pre>
(2023-04-24) (L)VTL as the starting point for LPC-oriented applications
-----------------------------------------------------------------------
Today, I released a new version of LVTL, called LVTL-O. It&#x27;s still portable
and under 300 SLOC of C89, but optimized for both execution speed and memory 
consumption, as well as fixing some critical bugs. So, whoever uses LVTL-A 
(although I doubt many people have downloaded it over the course of two 
days), should switch to LVTL-O as a drop-in replacement. The main page at 
hoi.st no longer offers LVTL-A for download either, only the new LVTL-O 
version. The new sharball also has some more VTL-2 examples that I found to 
be working fine after all the bugfixes, including my all-time favorite game, 
Bulls and Cows. Although, of course, this B&amp;C variant is easier to play 
because zeros are not allowed anywhere in the number.

By the way, why did I find Bulls and Cows so amusing and why does this
particular game bear a special meaning when it&#x27;s run on VTL? Well, because 
it&#x27;s the first game I encountered that doesn&#x27;t need any real-time interface. 
It&#x27;s a challenge-response type game. You send a four-digit challenge and 
receive a two-digit response. In fact, some time ago, I coded up a JS 
function version of this game where you pass your digits as a hexadecimal 
BCD number (e.g. if your guess is 1234, you pass 0x1234) and receive the 
response in the format BxCy, e.g. B3C1 for 3 bulls and 1 cow. And the entire 
game &quot;interface&quot; was just a function call (and another call to reinitalize 
it). You can wrap it into anything: graphics, text, voice, radioteletype, 
Morse code... Imagine playing Bulls and Cows over Morse. That&#x27;s something 
you can&#x27;t do with Tetris.

So, why is it significant that we can run B&amp;C in VTL? Because VTL itself is
fully line-oriented, and all program editing flow is designed in a 
challenge-response fashion. In fact, it was designed with teletypes in mind, 
because not every computer hobbyist in the late 1970s could afford CRT 
displays, not to mention full-featured terminals. Home PCs with TV adapters 
that allowed to use home TV sets as displays would only appear a bit later, 
in the early 1980s. And, as even Tiny BASIC was so verbose it&#x27;s a bit unfair 
to call it &quot;Tiny&quot;, VTL allowed its users to save keystrokes, paper tape and 
teletype ink.

Speaking of paper tape... I don&#x27;t actually know if the original design of VTL
was to also be fully compatible with the 5-bit ITA-2 code, also known as 
Baudot-Murray code, in its US variant, but it&#x27;s quite close to that. The 
only exception is that ITA-2 doesn&#x27;t have a character for a percent sign (%) 
and * (asterisk), so, as far as I understand, the bell character (S in FIGS 
mode, for US variant) is used instead of the latter, and we can extend the 
encoding by adding % as FIGS+CR or something. So, ideally, a &quot;5-bit&quot; version 
of VTL only uses these characters:

Bits  Letters Figures 
----- ------- -------
00000 null 
00100 space 
10111 Q       1
10011 W       2
00001 E       3
01010 R       4
10000 T       5
10101 Y       6
00111 U       7
00110 I       8
11000 O       9
10110 P       0
00011 A       -
00101 S       *
01001 D       $
01101 F       !
11010 G       &amp;
10100 H       #
01011 J       &#x27;
01111 K       (
10010 L       )
10001 Z       &quot;
11101 X       /
01110 C       :
11110 V       ;
11001 B       ?
01100 N       ,
11100 M       .
01000 CR      %
00010 LF
11011 Shift to figures  
11111 Shift to letters

And, indeed, the original VTL-2 for Altairs only used these characters + the
percent sign, while LVTL extends the range of variables to most lowercase 
letters (except xyz) and some special characters like @[\]^_`, as well as 
introduces three new bitwise operators - &amp;, | and ^, two of which are not 
present in the ITA-2 set. Well, as the precedent of using * as both a system 
variable name and an operator was set by the original VTL-2, and &amp; is also 
used both as a sysvar and as an &quot;and&quot; operator in LVTL already, nothing 
prevents us from aliasing | with ! for &quot;or&quot; and ^ with # for &quot;xor&quot;. And this 
is what I also have done in LVTL-O as one of the improvements. As long as we 
only use all-capital letters and strings, we can write LVTL programs that 
still can be ITA-2-encoded and punched onto a 5-hole tape (provided we also 
have encoded % as something like FIGS + CR).

Now, where do we go from here? I&#x27;m planning to create a somewhat restricted
subset of LVTL: with only 26 uppercase letters and only ITA-2 characters + 
percent sign (so ! and # operators will be the only versions of | and ^), 
also without interactive input evaluation, and the ? sysvar in this version 
will only accept integers, like in VTL02sg. This restricted version also 
will have a reference implementation in C89 (based on the cut-down LVTL-O 
code, of course), but then I&#x27;ll try porting it to some other runtimes. Which 
ones, is a story for another time.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-23-yes-extensible-is-the-key.txt</link>
<title>(2023-04-23) "Yes - extensible is the key"</title>
<description><![CDATA[<pre>
(2023-04-23) &quot;Yes - extensible is the key&quot;
------------------------------------------
No.

It isn&#x27;t.

How are named words in Forth-alikes different from line numbers in VTLs? I
mean, semantically, they are just points in memory where control is passed 
to. Imagine being able to assign aliases to particular line numbers. Do they 
suddenly make your language more &quot;extensible&quot;? Or is the whole idea of 
&quot;extensibility&quot; just based upon syntax, with the main feature being your 
custom words look exactly like predefined ones? If that&#x27;s the case, that&#x27;s 
definitely convenient but kinda lame at the same time. In case you really 
need that kind of convenience, you can use VTL as a sort of &quot;meta-assembly&quot; 
that everything else compiles to. Kinda like what JS is these days.

Piling up libraries, which comes as a natural result of extensibility, is not
the answer. Composing your solution from tiny programs that each do one task 
only is more like it. At least when you&#x27;re able to organize proper IPC. If 
you&#x27;re limited to loading a single program at a time, you can recreate the 
same behavior within it, but it doesn&#x27;t have anything to do with 
extensibility of the language itself.

In case you&#x27;re wondering, I was responding to Ken Boak aka monsonite ([1])
who had created several stack-based programming languages that, among 
others, inspired my creation of Equi.

--- Luxferre ---

[1]:
https://retrocomputingforum.com/t/keeping-things-simple-tiny-languages/1441/3
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-22-lvtl-is-out.txt</link>
<title>(2023-04-22) LVTL is out</title>
<description><![CDATA[<pre>
(2023-04-22) LVTL is out
------------------------
Yes, I did it. An interpreter of VTL-2 in less than 300 SLOC of pure C89. And
you can get it from my downloads section at hoi.st, a shar.gz with the C 
file itself, the shell wrapper unbuf.sh to turn off input buffering for the 
programs that use $ for single-character input, and a bunch of example VTL-2 
programs taken from the official manual and Rosetta Code.

Note that I labeled this version LVTL-A to stress both on the fact that it&#x27;s
the very first published revision and the fact it&#x27;s made as portable as 
possible as it only uses the standard ANSI C library. As you may have 
guessed, some other, more environment-specific versions **may** appear soon 
enough. I won&#x27;t reveal which versions in particular for now, because I 
honestly don&#x27;t know for sure myself yet, but I do know it is going to be an 
interesting journey.

This particular version is also not very speed-optimized and mainly uses the
most naive approach possible to navigate through the loaded code: since 
lines might not be entered in order, every iteration finds the next valid 
line throughout the entire allocated block of memory. It has no garbage 
collector whatsoever either: if the line gets overwritten by its number, it 
gets invalidated by labeling it as the line 65535 which is never read. This 
is why, in addition to 0, 65535 is also not a valid line number in LVTL. 
Because zeroes in the line number positions are used to find the free space 
to place the new statement to. I&#x27;m not sure how the original VTL-2 
implementations did all this but I&#x27;m pretty sure no sort of GC could be 
implemented in 768 bytes of machine code.

Speaking of which, I wonder if there are any specifications or example code
left for the previous VTL version, VTL-1. If we search &quot;Very Tiny Language&quot; 
on teh webz, we either get info on VTL-2 or some Japanese resources 
(although some of them have example code which clearly is VTL-2) or Arduino 
ports. The only things we know about VTL-1 for sure are the ones we can 
deduce from the VTL-2 manual:

* it had less characters allowed per line (not sure how many exactly);
* it had 11 less variables available;
* expressions were not allowed as inputs, only numbers;
* looks like it didn&#x27;t allow parentheses at all, because...
* ...looks like it supported a single operation per line only;
* it had no array manipulation (*) syntax); 
* it had no sysvars for last byte (&amp;) and EOM (*);
* it had no PRNG access (&#x27; sysvar);
* it had no single-character input/output ($ sysvar);
* it had no &gt; and &lt; sysvars (well, LVTL doesn&#x27;t have them either);
* the # sysvar meant &quot;current line number + 1&quot;, not just current line number;

Note that I&#x27;ve only listed anything regarding the language features
themselves. Things like &quot;it displayed &quot;OK&quot; after entering numbered lines&quot; or 
&quot;it required setting line number to 0 on init&quot; or &quot;it allowed to enter 
control characters into lines&quot; are more like particular implementation 
details that can easily be altered. Still, there ain&#x27;t much info on what 
this language actually was. Yet, some Japanese guy had produced this wonder 
of C coding called vtl7.c (a bit sanitized and reformatted by me):

#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

char c[80];
int v[&#x27;Z&#x27;-&#x27;A&#x27;] = {0};

int getval(char* p) {
  char* q = p;
  int a = (int) strtol(p, &amp;p, 0);
  if(q == p) {
    p++;
    return v[*q-&#x27;A&#x27;];
  }
  return a;
}

int expr(char* p) {
  int a = getval(p);
  for(;*p;) {
    char op = *p++;
    int b = getval(p);
    if(op == &#x27;+&#x27;) a += b;
    if(op == &#x27;-&#x27;) a -= b;
    if(op == &#x27;*&#x27;) a *= b;
    if(op == &#x27;/&#x27;) a /= b;
    if(op == &#x27;=&#x27;) a = a==b;
    if(op == &#x27;&gt;&#x27;) a = a &gt;= b;
    if(op == &#x27;&lt;&#x27;) a = a &lt;= b;
  }
  return a;
}

int main() {
  printf(&quot;OK\n* &quot;);
  for(;fgets(c, 79, stdin);) {
    if(c[1] == &#x27;=&#x27;) {
      if(*c == &#x27;?&#x27;) printf(&quot;%d\n&quot;, expr(c + 2));
      else v[*c-&#x27;A&#x27;] = expr(c + 2);
    }
    printf(&quot;OK\n* &quot;);
  }
  return 0;
}

Now, this is zen. Of course, it&#x27;s missing a whole lot of features even
compared to the VTL-1 and can&#x27;t be used for anything beyond a simple 
calculator with variables, but it showcases the basic ideas of REPL and 
strict LTR expression parsing in less than 40 SLOC of C. To be honest, 
adding parentheses to this particular evaluator is not that hard, one just 
needs to recursively pass the rest of expression to the same function and 
note the amount of parsed characters to further skip on return. Anyway, if 
you want to start creating your own VTL flavor, this particular piece of 
code is a great starting point.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-21-why-vtl-is-still-relevant.txt</link>
<title>(2023-04-21) Why VTL/VTL-2 is still relevant</title>
<description><![CDATA[<pre>
(2023-04-21) Why VTL/VTL-2 is still relevant
--------------------------------------------
I won&#x27;t write anything too long this time since I really wanna go to sleep.
But...

The more I study this language, the more I realize how elegant it is. In
fact, it&#x27;s far more elegant than anything you can write a VTL interpreter 
in. Just think about it: every program statement is numbered, and every 
program statement assigns something to something. All flow control, I/O, 
system calls - everything happens via assignments. And all of it maps onto 
the machine memory even nicer than Forth. Two bytes for the line number, one 
byte for line length, and the line itself. And then the next line with the 
same structure, and the next... With no additional delimiters and other 
overhead. Yet these &quot;system variables&quot; also give the programs a potential 
for self-modification and advanced memory control, the features not every 
dialect of BASIC of the time had, especially if we&#x27;re talking about Tiny 
BASIC which everyone compares the VTL-2 to the most. Yet even the tiniest 
implementation of Tiny BASIC was much heavier than the 768 bytes of VTL-2 
(on Altairs).

If there is a single thing I don&#x27;t like in VTL-2, it&#x27;s infix expressions.
Despite strictly left-to-right precedence and all the operators and 
variables deliberately taking a single byte, they are still harder to parse 
and execute than the prefix notation like in Lisp or RPN like in Forth or 
dc. If I were to create an even more minimalist language myself, I&#x27;d 
definitely make expressions RPN-like. The only advantage the LTR infix 
approach has is that it doesn&#x27;t require a significant whitespace or other 
delimiter to separate the operands. But once again, matching parentheses and 
keeping track of what we expect now - an operand or an operator - is a real 
bitch when it comes to keeping things simple.

Nevertheless, I plan to create my own public domain VTL-2 implementation in
upcoming days, and keep it under 300 SLOC of ANSI C89. And then run all the 
example programs from the manual ([1]). What do I plan to do with it next? 
Just wait for some time and you&#x27;ll find out.

--- Luxferre ---

[1]: https://deramp.com/downloads/altair/software/roms/
     orginal_roms/VTL-2%20Manual.pdf
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-20-a-few-words-about-dos.txt</link>
<title>(2023-04-20) A few words about DOS</title>
<description><![CDATA[<pre>
(2023-04-20) A few words about DOS
----------------------------------
I don&#x27;t like DOS, its backslashes and interrupts.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-19-human-scale-programming-languages.txt</link>
<title>(2023-04-19) Human-scale programming languages and the problem with them</title>
<description><![CDATA[<pre>
(2023-04-19) Human-scale programming languages and the problem with them
------------------------------------------------------------------------
I started writing this post while looking at the source code of Equi ([1]),
my probably most ambitious stack-based VM to this day that isn&#x27;t _fully_ 
esoteric as it allows to write compact but still human-readable machine code 
that ideally would even work on Apple IIe. I remember I promised to write 
more about this VM, but not today. Today, I just want to mention the equi.c 
file now has 716 SLOC of pure ANSI C89 code. Is this a lot? Well, compared 
to most modern programming language implementations, it might not seem a lot 
(even busybox&#x27;s awk.c is currently about 2900 SLOC), but 700 SLOC already is 
around the upper limit of my comprehension. Of course I understand this code 
because I wrote and tested it, and I hope everyone else will understand it 
because it is well-structured and well-commented, but still, it&#x27;s just too 
much. And the sad truth is, nothing else can be taken away from there 
without sacrificing either compatibility or usability. I don&#x27;t want this VM 
to grow in size, but the only realistic way to further shrink it would be 
dropping multitasking support and returning to the old(-ish) memory 
structure which I spent so much time to move away from. And yet again, this 
would reduce the codebase by around 100 SLOC at most and wouldn&#x27;t 
fundamentally change the overall picture of things.

From this point of view, it&#x27;s interesting to analyze various programming
languages and their particular dialects or implementations that are usually 
presented as &quot;minimal&quot;. For instance, regardless of how small Lua, Red, 
Boron and MicroPython are, I wouldn&#x27;t consider them &quot;minimal&quot; because their 
codebase still is huge. As I have already mentioned, busybox awk doesn&#x27;t 
look minimal either. Well, what does? There seem to be just three major 
language families that do not belong to esoteric or narrow-spec (like dc) 
classes that, although not at all small in their canonical implementations, 
_can_ have really minimal flavors: Forth, Lisp and Tcl. I say &quot;families&quot; 
because the implementations themselves may be so different one couldn&#x27;t 
recognize the original concept in them. For instance, both MINT and my Equi 
are Forth-like although neither of them fully qualifies as a variant of 
Forth. Apart from these three families, there also are some long-forgotten 
specimens like Tiny BASIC (world&#x27;s first piece of software to popularize the 
word &quot;copyleft&quot;, by the way) and VTL/VTL-2, with the canonical 
implementation of the latter being famous for being able to fit into 768 
_bytes_ of Altair&#x27;s ROM. And, as some advanced versions of this language are 
still being developed for 6502- and Z80-based machines, with the latest 
Apple IIe compatible variant (VTLC02) having 644 SLOC of the **assembly** 
language and fitting into 962 bytes of machine code, this continues to be a 
textbook example of true programming minimalism.

Now, here is the main problem that is true for all minimal programming
languages to larger or lesser extent: the simpler you make the core 
interpreter, the harder you make programming in it. As someone said, 
complexity has to live somewhere. If it&#x27;s not in your interpreter, then it 
either resides at the lower level (OS, VM or even the Web browser runtime if 
your language targets such an environment), or the upper level (the standard 
library, as it usually is the case with Forths and Lisps), or you put all 
the extra burden on the programmers themselves and every one of them has to 
reinvent the same wheels. To me, the main challenge in picking or even 
designing such a language would not be in moving the complexity around but 
eliminating it altogether. How, may you ask? After all, it&#x27;s the tools we 
can change, not the tasks we must do with them... Well, here are three 
recommendations I could give about complexity reduction.

1. Adjust your requirements. This is much easier to do if it happens before
even picking the tools. Think on the lines of &quot;Do I, or the tools I choose, 
really need to be able to do X in order for me to do Y?&quot; Don&#x27;t be afraid to 
cut off unnecessary requirements with the Occam&#x27;s razor.

2. Decompose your tasks into a set of smaller ones and only pick the tools
necessary to do each part, not anything extra. A good example would be 
typesetting software: you could use all-in-one packages like Kile, Scribus 
or even some proprietary monstrosities from Adobe I even forgot the names 
of, or you could use something modular like troff with eqn, pic, bib, 
dformat etc., but only the parts you actually need. If you only need to use 
formulas in your documents, you don&#x27;t need anything except eqn with 
troff/groff/nroff. If you also need images, you add in pic, and so on. 
Although they both perform the same tasks, guess which approach is less 
complex? The second one. Same with software design, as well as programming 
language design itself. I always was amazed how Plan 9, that never gained 
any serious traction, was far more Unix-way than the actual Unix-like OSes 
that did.

3. Don&#x27;t assume growth. This is what I already wrote about in my DevOps
related rant: most of the complexity in the software world arises completely 
prematurely from the blind assumption that everything that starts small will 
grow large. Only focus on what you need to do right here and now. When your 
code needs to grow, refactor it accordingly. When, not before. Accordingly, 
not beyond the scale.

Now, how do these recommendations and the thoughts before them translate into
my vision of truly minimal programming languages? Well, there must be some 
kind of &quot;lowest common denominator&quot; both in terms of implementation 
complexity and in terms of usage complexity, as well as self-sufficiency. 
So, here are my criteria. To me, a particular implementation of a 
programming language is minimal if all of the following conditions apply:

1. Its full source, along with the standard library, must not exceed 500 SLOC
of well-formatted and readable ANSI C89 code. If the implementation is 
provided in another programming language, the SLOC count of a hypothetical 
ANSI C89 translation replicating identical behavior of the language must be 
estimated. If the implementation provides a VM and a compiler is used to 
compile the code for this VM, then both the VM&#x27;s and the compiler&#x27;s source 
code is counted.
2. The implementation must provide I/O. If it targets the platforms that
support standard input, it must support standard input too. If it targets 
the platforms that support standard output, it must support standard output 
too. If it only targets the platforms that have neither, it must provide a 
way to return the computation results without having to use any kind of 
debugger, tracer or monitor.
3. The source code in the language itself must be human-readable and only
consist of printable characters except whitespaces or tabs. Also, any 
whitespace characters used in the code must not differ semantically, i.e. a 
single whitespace 0x20, a Tab character 0x9 or any combination of them must 
serve as a single delimiter or bear no semantics at all. An exception could 
be Python-like languages where the amount of leading whitespace characters 
on each line is significant, but that must be clearly stated in the language 
specification.
4. The language in this particular implementation must be Turing-complete.
This might not be so obvious from the first glance, so it&#x27;s better to 
explicitly specify this requirement.

Now, I understand that languages like Brainfuck will also meet all these
criteria. Well, yes, Brainfuck is cryptic but still minimal. Its full 
implementation in C89 can fit into well below 500 SLOC, it provides standard 
I/O and its source code is human-readable. Whether or not you can understand 
it is another question for another discussion. But, on the scale of 
complexity, I&#x27;d put BF far lower than anything like modern Java. At least I 
can imagine how I could even integrate BF programs into my Unix pipelines 
for daily routines. With Java, I&#x27;m not so sure.

About 10 years ago (if not 15 already), I had read a quote by some anonymous
that reflects the overall situation described in this post pretty 
accurately: &quot;If everyone out there knew bash, find, vi(m), grep, sed and 
awk, millions of software products would never need to be created&quot;. Only 
fairly recently I started understanding how damn right he was.

--- Luxferre ---

[1]: https://git.sr.ht/~luxferre/Equi
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-18-shellbeat-is-possible-too-but.txt</link>
<title>(2023-04-18) ShellBeat is possible too, but...</title>
<description><![CDATA[<pre>
(2023-04-18) ShellBeat is possible too, but...
----------------------------------------------
...with many caveats. At least when we want to make it work in Busybox-like
environments.

Yes, we all know that even the ash version in BB supports bitwise operators
and we can wrap formulas almost as they are defined in C into something like 
$((255&amp;($OUR_FORMULA))) in a loop and be happy. However, as much as you&#x27;re 
tempted to use native loops to create the stream and and printfs to convert 
the decimal output of the formula into hexadecimal and then output as raw 
bytes, please don&#x27;t. Printfs will work but they are _extremely_ slow, 
especially in BB where printf isn&#x27;t a shell builtin. If you need to generate 
an endless byte stream at the 8000 Hz rate and higher, you have to take 
another way. Luckily, BB (and every other POSIX environment) contains all 
the commands we need to do this. There will be another caveat but we&#x27;ll get 
to it later.

So, let&#x27;s split our task of expression-based sound generation into several
parts:

1. Generating an endless integer sequence starting from 0.
2. Calculating the formula.
3. Converting the formula result into a raw byte.
4. Passing the byte into the playback program.

Step 4 is essentially the same as used in AwkBeat, so I won&#x27;t repeat myself.
Here, let&#x27;s assume we have SoX installed and exported the variables 
SAMPLERATE=8000 and PLAYCMD=&quot;play -q -tu8 -r${SAMPLERATE} -&quot;, so we don&#x27;t 
have to write this part fully in the further examples. Now, let&#x27;s start from 
the beginning.

To generate an endless integer sequence, we could use the seq command as I
had shown in my initial post about ByteBeat, but the issue is it&#x27;s not 
endless and we have to specify the maximum integer, which depends on the 
target platform and compile time configuration of the shell itself. 
Alternatively, we can find something that generates endless lines and 
something that numbers them. And the first thought would be to use something 
like yes &#x27;&#x27; | cat -n, but this also poses several problems: cat doesn&#x27;t 
allow us to select which line number to start from and smaller line numbers 
are preformatted. Luckily, BB/POSIX environments also offer us a special 
command to number lines, nl, which we can parameterize to not have 
preformatting and to start from 0. So, the endless integer sequence 
generation part ultimately looks like this:

yes &#x27;&#x27; | nl -ba -v0 -w0

Now that we have a stream of integers from 0 to whatever, what&#x27;s next? Next,
we need to calculate the formula. Let&#x27;s assume we have it saved into the 
${FORMULA} variable and it&#x27;s a string with a valid shell-compatible math 
expression that takes t as the single parameter. And how do we substitute 
the parameter in the string that came from the standard input? This is what 
the xargs command is for (that also allows us to supply the parameter name 
to substitute). A naive approach would be to directly write something like 
this:

yes &#x27;&#x27; | nl -ba -v0 -w0 | xargs -It echo &quot;$((255&amp;(${FORMULA})))&quot;

But, if you run this, you&#x27;ll find out that all the lines return a single
value (the result of the formula being calculated against 0). That&#x27;s because 
the shell we&#x27;re running this in does the substitution of the nonexistent 
variable t (cast to 0 in math shell expressions) even before it gets to 
xargs. That&#x27;s why we need to escape the dollar sign. But then, we&#x27;ll just 
get a bunch of shell expressions printed. In order to evaluate them, we need 
to turn them into commands that we can pass to the actual shell afterwards. 
So, the final variant of this stage will look like this:

yes &#x27;&#x27; | nl -ba -v0 -w0 | xargs -It echo &quot;echo \$((255&amp;(${FORMULA})))&quot; | sh

So, we&#x27;ve got our byte stream calculated and printed in decimal, which is not
quite what we want, right? We can use a printf command instead of the inner 
echo but, as I already said, it will be extremely slow. Instead, let&#x27;s use 
another commonly available command that will do the job for us, dc:

yes &#x27;&#x27; | nl -ba -v0 -w0 | xargs -It echo &quot;echo \$((255&amp;(${FORMULA})))P&quot; | sh
| dc

Here, we append the P instruction to every number, which tells dc to output
the value from the stack top as a string if it&#x27;s a string, or as raw bytes 
if it isn&#x27;t. Our case is the latter. Finally, we just pipe this raw byte 
stream into our player, and this is the ready &quot;ShellBeat&quot; one-liner:

yes &#x27;&#x27; | nl -ba -v0 -w0 | xargs -It echo &quot;echo \$((255&amp;(${FORMULA})))P&quot; | sh
| dc | $PLAYCMD &gt; /dev/null

But here is another caveat: the P command is non-standard for dc. Yes, it&#x27;s
present in GNU dc and properly configured Busybox builds, but there are some 
systems where it&#x27;s just not there. In this case, we have to adjust our 
one-liner to make dc output our numbers in hexadecimal and also use sed to 
left-align the output to 2 digits, after which it&#x27;s passed to our old friend 
xxd:

yes &#x27;&#x27; | nl -ba -v0 -w0 | xargs -It echo &quot;echo 16o\$((255&amp;(${FORMULA})))p&quot; |
sh | dc | sed &#x27;s/\&lt;[0-9A-F]\&gt;/0&amp;/&#x27; | xxd -r -p | $PLAYCMD &gt; /dev/null

Or, if you consider writing &quot;16o&quot; every time an overhead, you can use this
version:

(echo &#x27;16o&#x27;; yes &#x27;&#x27; | nl -ba -v0 -w0 | xargs -It echo &quot;echo
\$((255&amp;(${FORMULA})))p&quot; | sh) | dc | sed &#x27;s/\&lt;[0-9A-F]\&gt;/0&amp;/&#x27; | xxd -r -p | 
$PLAYCMD &gt; /dev/null

Regardless of which of the three one-liners suits you best, any of them is
much faster than using printfs to generate the output. But still, there are 
too many pipes and substitutions. Are they really necessary? Is there any 
way to just evaluate our formula directly in the current shell and then 
render its output with whatever tool we choose? Well, check this out:

(t=0; while true; do printf &#x27;%02X\n&#x27; &quot;$((255&amp;(${FORMULA})))&quot;; t=$((t+1));
done) | xxd -r -p | $PLAYCMD &gt; /dev/null

Wait, what? Didn&#x27;t I just say that printfs are slow? Well, yes, they are
terribly slow when generating bytestreams directly. But here, we are 
generating the source for xxd, and we&#x27;re doing it line by line. In this 
case, printf works just as fast as echo. And xxd then reconstructs the 
binary stream as quickly as it can. This is what I have actually used in my 
shellbeat.sh script published on the main hoi.st page along with AwkBeat.

Moral of the story: when there is more than one way to do it, keep exploring
every possible option until you find the simplest one.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-17-wait-what-ecosystem.txt</link>
<title>(2023-04-17) Wait, what ecosystem?</title>
<description><![CDATA[<pre>
(2023-04-17) Wait, what ecosystem?
----------------------------------
It&#x27;s funny how the look of things can change depending on the perspective. A
single binary file that weighs about 11 MB is somehow no longer perceived as 
something you can build your entire workflow on top of. Indeed, Busybox is 
still missing some essential pieces like iwutils, wpa_supplicant or 
modem-manager, i.e. anything that would allow you to achieve wireless 
connectivity - only Ethernet, only hardcore - and in this regard, it lags 
behind a bit. But as for everything else...

Let&#x27;s take a look at the current stable Busybox version at the time of
writing, 1.34.1, built with the default compile-time flags, and see what we 
can find there. I won&#x27;t list all the commands, but just the ones I consider 
significant.

* Init system: busybox init (good enough to use in distros like TinyCore)
* Shell: ash (actually, a variant of dash with some GNU extensions)
* File commands: ls, mv, cp, rm, mkdir, rmdir, ln, find, file, du, df,
realpath, mktemp, chroot, lsof
* Text editors: vi, ed
* Text processing languages: sed, awk (POSIX + bitwise operators)
* Other text processing tools: cat, less, tee, head, tail, cut, tr,
grep/egrep, sort, uniq, cmp, comm, shuf, seq, diff, paste, patch, split, ts, 
rev, wc
* Binary editors/viewers: hexedit, xxd, hexdump/hd, od, strings
* Binary serializers: base64, base32, uuencode/uudecode, makemime/reformime
* Checksums/hashes: sum, crc32, md5sum, sha1sum, sha256sum/sha512sum, sha3sum
* Calculators: bc, dc, factor, ipcalc
* Archivers: ar, tar, cpio, rpm2cpio
* Compression tools: gzip, bzip2, xz, lzma, unzip
* Network clients: wget, tftp, telnet, nc, ftpget, ftpput, rdate, ntpd,
ssl_client, sendmail (!), popmaildir
* Network servers: inetd, tcpsvd, udpsvd, nc, telnetd, httpd, ntpd, dnsd,
udhcpd, tftpd
* Serial device client: microcom

This list doesn&#x27;t include all the filesystem utils, service management
commands, network setup and a ton of other things also present there. Yet 
you can already see how one can live a full life inside this environment. Of 
course, most of these tools (or &quot;applets&quot; in Busybox terminology) offer less 
amount of features than their &quot;full-sized&quot; counterparts, but they can be 
efficiently combined together to get the job done. While Busybox itself, as 
a single program, could be viewed as the opposite of the Unix-way, it 
actually is more of a portable binary container for multiple Unix-way 
programs developed under the same umbrella. One way or another, it can be 
considered an ecosystem.

However, just like with almost any other popular project that, while itself
being non-commercial, had led some companies or individuals to commercial 
success, politics got in the way here too. Busybox isn&#x27;t suitable as a part 
of projects that you&#x27;re not ready to release under GPL, although my opinion 
about &quot;free software licenses&quot; vs. true software freedom is a topic for 
another time. Nevertheless, I salute any toolbox alternatives with a similar 
approach but with more permissive licensing terms, like Toybox (which, for 
the time being, is far behind Busybox in terms of features and 
self-containment: it doesn&#x27;t even contain its own shell yet). As long as 
you&#x27;re just an end user though, Busybox stack is just fine. Which brings 
another interesting question to the table: which commands out of the above 
list I&#x27;d personally consider absolutely vital for a complete and 
self-contained minimal Linux system? Again, omitting all the network 
configuration and partition setup, here are the top 64 that I consider the 
most important:

1. init
2. (d)ash
3. ls
4. cp
5. rm
6. ln
7. printf (might not be a part of the shell)
8. test/[ (might not be a part of the shell)
9. realpath
10. find
11. cat
12. tee
13. head
14. tail
15. sort
16. grep/egrep
17. diff
18. patch
19. less
20. vi
21. sed
22. awk (POSIX+bitwise)
23. xxd
24. hexedit
25. nc (any implementation with server mode)
26. tar
27. gzip
28. xz
29. unzip
30. bzip2
31. date
32. chroot
33. mktemp
34. base64
35. md5sum
36. sha1sum
37. sha256sum
38. cpio
39. split
40. xargs
41. strings
42. paste
43. ps
44. kill
45. killall
46. udhcpc
47. ping
48. lsof
49. time
50. dc
51. mount
52. umount
53. mkfifo
54. mdev
55. netstat
56. env
57. pwd
58. nohup
59. stty
60. dmesg
61. losetup
62. wget (with FTP support)
63. ssl_client
64. microcom

Note that I didn&#x27;t include the mv command because it easily can be emulated
with cp+rm. In the same way, we could theoretically substitute hexedit with 
vi+sed+xxd, but that might be too cumbersome. In case the selected nc 
version (like the one used in Toybox) also supports interacting with serial 
device files, microcom is also unnecessary, although that&#x27;s a matter of 
convenience. 

So, this is how my ideal &quot;Toybox&quot; would look like. Now, just think of what
could be done if we added something like Lua, Tcl or MicroPython into the 
mix... But at this point, I&#x27;m just dreaming out loud.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-16-awkbeat-is-now-a-thing.txt</link>
<title>(2023-04-16) AwkBeat is now a thing. Now what?</title>
<description><![CDATA[<pre>
(2023-04-16) AwkBeat is now a thing. Now what?
----------------------------------------------
Continuing the ideas of my previous post, I decided to create a helper script
that I could just feed an AWK expression to and get the sound playing. Seems 
like a simple task, right? Well, I was really overwhelmed by the amount of 
issues that I actually needed to figure out in order for all this to work as 
expected. Most of these issues arose because I chose for my script to be 
strictly compatible with Busybox while most of the AWK materials on the 
Internet are about the GNU version, GAWK. And GAWK really differs from 
Busybox AWK even more than Bash differs from Busybox sh. Up to the point of 
being able to easily establish bidirectional communication and internet 
sockets (yes, unlike Bash, GAWK even allows to listen on them). We really 
have to consider the level of compatibility here, so whenever you&#x27;re writing 
an AWK script, I&#x27;d give you a strong advice to test it on Busybox first. 
Then you&#x27;ll be sure that it runs on GAWK with no problems as well.

One thing that really got me puzzled for a long time, until I found out that
one can&#x27;t solve this even on GAWK unless passing the -b flag, is that all 
output in AWK, even the one you do with the %c specifier in its printf, 
cannot be considered binary-safe. In other words, without the (GAWK-only) -b 
flag, characters in AWK are not semantically equal to bytes, and with 
non-printable values like \0, as well as codepoints above 127, anything 
goes. In case of my system, I could get away with setting LC_ALL=C but this 
is not guaranteed to work as expected for everyone. This is why, just for 
the sake of not having my formula output mangled regardless of system 
locale, I had to limit the AWK part to only printing the hexadecimal stream 
to the stdout, and then pipe it into the xxd -r -p command that would 
reconstruct the binary stream before passing it into the player command with 
another pipe. Well, having to use xxd didn&#x27;t bother me much as it also is a 
Busybox applet, just like awk command itself. Besides, xxd itself is an 
extremely useful tool even in the Busybox variant, so I recommend everyone 
to learn it too. By the way, in case you didn&#x27;t know, xxd -g1 is almost 
identical to the default hexdump -C, except there is a colon after the 
offset and no extra space in the middle of hex lines. Since I don&#x27;t really 
use hexdump in any other mode, maybe it&#x27;s time to finally fully migrate to 
xxd. Note that some systems with limited Busybox/Toybox builds don&#x27;t even 
include xxd. Although they may *usually* include hd (which is a poor man&#x27;s 
equivalent of hexdump -C), so that&#x27;s already something. But, in order to be 
able to reconstruct binary streams from hex or anything without xxd, you 
might have to use much slower options like another shell script with read 
and printf.

Another thing is, of course, PCM sound output itself, i.e. the commands we
pipe our binary output to. For the SoX play command in the previous post, 
there actually is a shorter equivalent (provided your desired sample rate is 
in the SAMPLERATE environment variable):

play -q -tu8 -r${SAMPLERATE} -c1 -

Now, if you don&#x27;t have SoX and don&#x27;t want to install it for some reason, I
found some easy ways to pipe the output to some popular sound subsystems in 
GNU/Linux environment:

* Raw OSS (only 8KHz support): tee /dev/dsp
* ALSA: aplay -q -f U8 -r ${SAMPLERATE}
* PulseAudio: pacat --raw --format=u8 --rate=${SAMPLERATE} --channels=1
* PipeWire: pw-play --format=u8 --rate=${SAMPLERATE} --channels=1 -

I commented out these alternatives in the script (the definition of the
PLAYCMD variable). If you need them, just uncomment the corresponding line. 
Oh, and the script itself, awkbeat.sh, already can be found in the 
downloadables section on hoi.st. Feel free to try it out. So, this 
initiative is finally complete. Now what?

Now, starting from here, I can pursue two goals: porting as many Bytebeat
expressions as possible to (Busybox-compatible) AWK (maybe some cool 
formulas collection will also appear on my page) and also learning AWK to 
make the most of the ecosystem that Busybox itself provides. Not sure if 
there is any point in targeting even more limited environments at the 
moment, like Android 6 that has no AWK but has a Toybox binary with nc and 
sed... Oh well, that&#x27;s totally another story.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-15-on-sound-in-minimalist-computing.txt</link>
<title>(2023-04-15) On sound in the world of minimalist computing</title>
<description><![CDATA[<pre>
(2023-04-15) On sound in the world of minimalist computing
----------------------------------------------------------
Yes, I know, I know. I promised to continue the stream compression topic.
Well, I&#x27;m in progress of writing some C code to support my further research 
(if I can even call it so) in this area. And the results are to follow 
whenever this is complete. No rush, but I&#x27;m not going to abandon this in the 
middle, I&#x27;ve already written enough code to not just throw it away.

Today, however, I want to talk about computer audio in general and computer
music in particular. While I believe in the predominant role of fully 
analogue media to store sound and music in case our computers become small 
and weak and truly LPC devices, there still is some space for possibilities 
and exploration. Because as we know, the greatest hits among those fully 
created on computers were created on, or at least for, old 8-bit machines 
like Famicom and C64. The beauty of chiptunes was in pushing the capability 
limits of the soundchips of the time. While the soundchip essentially 
defined the basics of how you design your sounds, giving you a very limited 
amount of PSG channels, each with its own restrictions, you could unleash 
your creativity not only with the music itself but also with your own 
techniques to bypass these restrictions to not sound like everyone else. 
This is why, for instance, when I hear a music from some Famicom/NES game, I 
can tell for sure if it&#x27;s Sunsoft, Natsume, HAL or Konami, even if I hear 
this music or see the game for the first time in my life. Because you can&#x27;t 
confuse their sound engines, that&#x27;s how original the sound is that they 
produce on the same five standard PSG channels.

But then, not every machine of the time even had a soundchip or any sound
output capabilities at all, except a simple piezo buzzer or a single 
frequency speaker. Aha! What if we turn this buzzer on and off fast enough 
to simulate any frequency we want? This is how the PFM (pulse frequency 
modulation) technique was invented, now often referred to as &quot;1-bit music&quot;. 
The &quot;1-bit&quot; here merely refers to the fact that we operate on the output 
device that can only be in one of the two states, on or off. In reality this 
also meant that everything that soundchips did in hardware, here had to be 
done in software. That&#x27;s why all PFM music authors also had their own 
speaker drivers bundled within the album, the game or whatever their music 
was created for. 

Starting with Amiga computers and IBM PCs with external sound cards and to
this very day, all audio is now generally output using the technique first 
introduced for Audio CDs, PCM (pulse-code modulation), where the signal is 
quantized to some finite amount of levels and sampled at some rate per 
second. The bigger the rate and the amount of levels are, the better the 
sound quality, but the more processing power is required too. Nowadays, 
sound generation is fully abstracted from the hardware layer and composers 
don&#x27;t have to adapt to the chips of the machines they work with anymore. 
They just output sound in some format that can be represented as PCM data at 
the end of the day, and the system then takes care of the rest. Most of them 
don&#x27;t even create music and effects in the form of pure PCM data - it is 
either decompressed from a more compact source (like MP3, OGG or FLAC) or 
synthesized (in which case the composer only deals with the notes, 
instruments and effects that a particular DAW or tracker software can offer, 
not to mention even higher layers of abstraction like Web Audio API). So, 
even the pure PCM, despite its ubiquity, isn&#x27;t something that everyone 
touches directly these days.

It is, however, still there. And every modern OS in existence offers a more
or less straightforward way to output raw PCM data directly to the audio 
device (if you can&#x27;t find a more straightforward way, you can always use 
less straightforward ways like the aforementioned Web Audio API, at the cost 
of higher resource usage, of course). A special case can be noticed when 
your signal is quantized to 256 levels and every level is represented with a 
single integer from 0 to 255 (or from -128 to 127, depending on how you look 
at it, but usually it&#x27;s treated as unsigned), that is, a byte. Combined with 
the default PSTN-compatible sampling rate for most sound adapters, that is, 
8000 KHz, we get the default &quot;raw&quot; PCM mode, which is unsigned 8 bit 8KHz 
PCM, that requires no additional preconfiguration of the adapter to emit 
sound, at least in Linux-based OSes. Which is why, when OSS was a major 
thing and the audio device was represented with a single file in /dev/ 
(/dev/audio or /dev/dsp), people had fun by redirecting the contents of 
various (non-sound) files directly into this file and listening to what came 
out as the result. With ALSA, PulseAudio or whatever else abstraction layer 
you have, you still can pipe the output to something like properly 
parameterized sox-play to achieve the same effect:

cat somefile | play -traw -r8000 -b8 -e unsigned-integer -

Some files gave interesting sound patterns, others just gave noise, so
experienced listeners could even guess the file type by what they heard. But 
then, some people started thinking a step forward: &quot;What if, instead of just 
catting existing files, we generate the raw sound data programmatically?&quot; 
Among those people was the Finnish guy I already mentioned on this phlog, 
viznut. He had compiled a bunch of short C programs where he defined every 
sound output byte as a function of a single eternally incrementing integer 
variable t, and the standard output of these programs was the unsigned 8-bit 
8KHz PCM data to be redirected to the /dev/audio, aplay or sox-play. He 
called this concept &quot;Bytebeat&quot;. And, if you read my previous explanation 
carefully, you can see where this name came from.

Originally, Bytebeat gained popularity in this very form, but then, as more
bytebeat players got ported to different environments including 
browser-based ones (with Web Audio API), it wasn&#x27;t limited anymore to 
outputting single bytes or even integers (so, the so-called &quot;floatbeat&quot; 
spawned as well), it wasn&#x27;t limited to only using the bitwise and basic 
arithmetic operators (because JS has trigonimetry and all other stuff in its 
standard library already) and it wasn&#x27;t limited to just output samples at 
8KHz. On one hand, the removal of this limitations allowed people to 
transcend to other spaces of exploration, on the other hand, the same people 
started abusing JS&#x27;s capabilities and built entire trackers inside the 
bytebeat expressions, returning to the traditional and not generative music, 
which kinda defeated the whole initial purpose and the idea to find music in 
purely mathematical expressions, preferably as short as possible.

I understand why the general public went the WAAPI route. Browsers are easy,
they allow for quick exploration and on-the-fly change of sound, not even 
having to compile the expression every time you change it. Less getting in 
the way between you and the formula, more possibilities for customization 
and general convenience. However, using a whole friggin&#x27; browser for this 
task is as far from the KISS way as possible. I&#x27;d rather use pure shell 
scripting but I know the bitwise operations in Bash are a bitch, it&#x27;s a 
command language, not a general-purpose programming language, after all. So, 
we need something no less ubiquitous that would ideally be present even in 
Busybox but would save us from all the hassle with compilation.

Enter AWK. Yes, it&#x27;s a full-featured programming language and yes, it&#x27;s
present in Busybox, although probably not as powerful as GAWK. I don&#x27;t have 
any good knowledge of AWK right now, I probably need to learn it at least as 
well as Bash and start using it on a daily basis. But, you know what, this 
example worked on my Arch:

seq 11111111 | busybox awk &#x27;{printf(&quot;%c&quot;,and($1,rshift($1,8)))}&#x27; | play -traw
-r8000 -b8 -e unsigned-integer -

Which means that yes, we do have bitwise operators in Busybox AWK and we can
port any classic C bytebeat formula to this language. I&#x27;ll definitely 
experiment with this more and probably will create a separate document 
linked from the main hoi.st map that contains ports of some music formulas I 
liked or created myself. A synthesizer and a tracker, all in a tiny formula, 
now in AWK.

If AwkBeat isn&#x27;t yet a thing, now is the time.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-14-some-thoughts-about-stream-compression.txt</link>
<title>(2023-04-14) Some thoughts about simple stream compression</title>
<description><![CDATA[<pre>
(2023-04-14) Some thoughts about simple stream compression
----------------------------------------------------------
This blog already contains my opinion about archive formats and why they must
not be confused with compression formats. Well, now it&#x27;s time to talk about 
the latter. But, unlike archive formats, I won&#x27;t point to ready-made or 
established solutions or algorithms this time, and just will express some 
thoughts on how I would approach the problem of stream compression myself 
using the simplest techniques available.

I&#x27;m emphasizing on the stream part because I want to stress the mandatory
condition that the compressor or decompressor can only operate on the data 
it already encountered when reading the stream, and the fact the set of data 
it can operate on is limited. I.e. it cannot read all the data at once into 
memory to create symbol statistics tables, nor it can allocate unbound 
memory chunks during the process to adapt the already compressed or 
decompressed data. Stream compression always operates on what we have right 
here and now, plus maybe some small buffer whose size is always fixed 
regardless of the input length.

I&#x27;m also emphasizing on the simplest techniques. That is, nothing more
complex to implement than LZW or even LZP. And keep in mind that even these 
two can be hard to do in really memory-constrained conditions. Think on the 
scale of around 4096 bytes max of available RAM for both your program itself 
AND the buffers it may allocate. We are, however, allowed to run the data 
stream through any amount of these tiny encoders/decoders to get the final 
result. So, it&#x27;s fine to use various simple techniques and combine them at 
the higher level.

Let&#x27;s start with the most obvious one - RLE, run-length encoding. The idea
here is as old as this world: if the byte is the same as the previous one, 
don&#x27;t write it to the output stream and just increment the counter, and as 
the byte changes, write the counter and the previous byte to the output 
stream. When it comes to real-life implementations, however, several 
questions arise, the first couple of them being: how do we store the length 
itself and how do we reliably separate the length from the payload so that 
the decoder won&#x27;t confuse the two?

Most RLE implementations I&#x27;ve seen just limit the maximum length of runs to
128, 256 or some other value that would conveniently fit into the fixed 
amount of bits. This is straightforward but quite inefficient. Suppose we&#x27;re 
processing a 800x600 video frame with a static color, this means every 
channel in this frame (R, G, B or Y, Cr, Cb - doesn&#x27;t matter) has 800*200 = 
160000 identical bytes. If we adopt the simplest RLE compression scheme 
[len][byte] where [len] is also a byte from 0 to 255 (encoding the length 
from 1 to 256), this means we can split our 160000 bytes into 625 256-byte 
chunks, and this means we have to write 1250 bytes of output data (625 
identical pairs of 0xff and whatever the actual value is) because we have 
limited the maximum run length to 256 by design. Now, if only there was a 
way to encode the length of the length itself, so that we could still 
separate it from the data byte, wouldn&#x27;t waste the extras but also wouldn&#x27;t 
have the upper limit...

And it turns out there is a way to do this, moreover, the most popular and
versatile text encoding in the world - UTF-8 - also uses it. This technique 
is called unary coding. In unary coding, the amount of leading 1 bits before 
the first 0 encodes the value, so zero bit itself acts as a delimiter here. 
In UTF-8, this amount of leading ones in the header byte indicates how many 
bytes the entire character will take. For our purposes, this amount would 
indicate how many extra bytes the length field will take. On top of that, we 
could borrow the principle from UTF-8 and only use a single byte for 128 
byte long runs maximum instead of 256. I.e. the length of 128 would be 
encoded (in binary) as 01111111 (0 as the delimiter and 1111111 as the 
value), while the length of 129 would already be encoded in two bytes as 
10000000 10000000 (1 as unary-coded extra length, 0 as the delimiter, and 
the value minus 1 aligned to the end of the sequence). The maximum run 
length one could encode in two bytes would then be 10111111 11111111, or 
16384. The length of 16385 would be then encoded as 11000000 01000000 
00000000, and three bytes would fit in the length up to 2097152 (11011111 
11111111 11111111), and so on. Although, in practice, I doubt one would 
encounter over 2 megabytes of a single repeated byte, but nevertheless it is 
infinitely scalable if you need to. We might make the encoding even more 
efficient by considering the offsets from the previous maximums (e.g. 129 
would be encoded as 10000000 00000000 instead of 10000000 10000000), thus 
increasing the range of values a little, but the practical effect would be 
negligible compared to the amount of operations we&#x27;d need to add every time 
to do this.

So, for the case of our static frame, how well could we compress a single
channel with the unary-coded RLE (or should I call it RLUE - run-length 
unary encoding?) instead of fixed length RLE? Let&#x27;s encode the length of 
160000 with our unary coding. First, we need to subtract 1 from the value 
itself (159999) and convert it into binary, and we get 100111000011111111. 
Nice. Then, we split it into bytes starting from the LSB: 10 01110000 
11111111. So, we get three bytes, which means we need to use two ones and 
the zero delimiter at the start. So, our RLUE value of 160000 becomes 
11000010 01110000 11111111, or C2 70 FF in hex. Now, add the color byte 
itself, and we get 4 bytes per channel. We have compressed an entire 
480000-byte static frame into 12 bytes with this improved RLUE technique 
that, if you think about it, doesn&#x27;t really introduce any additional 
complexity into the encoding or decoding process compared to the fixed RLE 
variant.

Of course, this is an ideal situation. In reality, the runs normally will be
much shorter (although they may be longer than the usual 128 or 256 bytes, 
so the RLUE approach is still good to use). But what&#x27;s going to happen even 
more frequently is that every following byte of your data will **slightly** 
differ from the previous one and that&#x27;s why a perfect run will not be 
shaped. So, the next natural question would be: how can we make use of the 
fact that the bytes are close to each other in order to compress them 
better? Well, the answer is delta encoding. Instead of writing the current 
byte value to the output stream, we write the difference between the current 
and the previous bytes (at the beginning of the stream, we assume the 
previous byte is always 0). Now, in case of equal or strictly sequential 
data (like 1, 3, 5, 7, 9, 11...) delta encoding turns most of it into the 
perfect run (1, 2, 2, 2, 2, 2...), but in case of slightly varying data 
we&#x27;re interested in (like 77, 78, 75, 81, 80, 77, 76, 79... ) delta encoding 
alone (77, 1, -3, 6, -1, -3, -1, 3...) won&#x27;t be sufficient enough to achieve 
any compression. So, we must combine deltas with another simple code that 
represents integer numbers closer to zero with less bits than the numbers 
further from zero.

Since it&#x27;s just single bytes we&#x27;re dealing with and, while encoding deltas,
we always subtract the previous value from the current one, wrapping it 
around if necessary, the numbers we must be able to encode with whatever we 
choose range from -128 to 127. Since most compression codes were designed 
for non-negative numbers, a more practical approach would be to remap our 
delta d to the 0..255 range using the following formula: 2d if d&gt;=0, -2d-1 
if d&lt;0. This way, deltas with small absolute values will still be closer to 
zero. Some codes also don&#x27;t like encoding zeros, so we add 1 and get the 
following code points: 2d+1 if d&gt;=0, -2d if d&lt;0. So now, once we have 
converted every delta to an integer from 0 to 255 or from 1 to 256, what do 
we choose?

After a long train of thought, I&#x27;m leaning towards the Golomb-Rice code. In
the variant I know it, it&#x27;s a clever application of our old friend unary 
encoding. Here, it doesn&#x27;t apply to the entire number though, but only to 
the upper part of it. I.e. we split the bit representation of a number into 
the quotient and remainder, where remainder is the lower m bits (usually 
these codes are named after M = 2**m, e.g. GR8 for 3-bit remainder, GR16 for 
4-bit remainder and so on), and we encode the quotient in unary and then 
write the remainder as is. Decoding is also straightforward here: we read 
the unary quotient representation until the first zero bit, then the known 
amount of bits in the remainder, and reconstruct the number by decoding the 
quotient from unary and just concatenating the remainder. Compared to the 
pure unary and some other encoding schemes (like Exp-Golomb or Elias code 
family), Golomb-Rice code allows us to adjust the tradeoff between fixing 
the bit length of the smaller values and expanding the bit length of the 
larger ones. This is why this kind of encoding is especially good for 
encoding deltas: the smaller, the better.

Again, remember that we aim for simplicity here, so we can afford losing some
compression ratio but we must produce byte-aligned output. So, if we, for 
example, read four bytes of input from the stream, we can only encode them 
to 1, 2, 3 or 4 bytes of output, nothing in between. This is why, to make 
use of GR codes, we still have to read input stream data into some buffer. 
As you can see from the following example, it may be small but still needs 
to be present. Note that for the decoder to know how many Golomb codewords 
to read next, the buffer length must be passed beforehands as well, and we 
can do this in the same encoding. Note that byte alignment doesn&#x27;t hamper 
the code readability because we know how many bits to read at the beginning 
of every codeword: everything up to the first 0 and then exactly 3, 4 or 5 
bits if we&#x27;re using GR8, GR16 or GR32 respectively.

Example:

Input stream buffer data: ababagalamaga
Data length in bytes: 13
Data length encoded in GR8: 10101
Data length encoded in GR16: 01101
Data length encoded in GR32: 001101
Total source bit length: 104 bits
Data ASCII: 97 98 97 98 97 103 97 108 97 109 97 103 97
Data deltas: 97 1 -1 1 -1 6 -6 11 -11 12 -12 6 -6
Absolute deltas (0-based): 194 2 1 2 1 12 11 22 21 24 23 12 11
GR8 bitstream: 10101 1111111111111111111111110010 0010 0001 0010 0001 10100
10011 110110 110101 1110000 110111 10100 10011
GR8 bit length: 94 (90.4% of original)
GR8 bit length, byte-aligned: 96 (92.3% of original)
GR16 bitstream: 01101 11111111111100010 00010 00001 00010 00001 01100 01011
100110 100101 101000 100111 01100 01011
GR16 bit length: 86 (82.7% of original)
GR16 bit length, byte-aligned: 88 (84.6% of original)
GR32 bitstream: 001101 111111000010 000010 000001 000010 000001 001100 001011
010110 010101 011000 010111 001100 001011
GR32 bit length: 90 (86.5% of original)
GR32 bit length, byte-aligned: 96 (92.3% of original)

What conclusion can we draw from this? Actually, none, except that delta+GR16
encoding performs better at compressing ababagalamagas, that we know for 
sure. The longer buffer we can capture and the closer its data bytes are to 
each other, the better the results will be.

Now, a really good question is: which order should we apply all these
techniques in? The most obvious answer is that it fully depends on the 
nature of your data. But what if we don&#x27;t know? Like, in our case with video 
frames, we can estimate their length but that&#x27;s it, we don&#x27;t know if they 
are going to be mostly static colors or not. If we are compressing text data 
(like source codes in a shar archive), there is absolutely no sense in even 
trying to apply any RLE compression, while GR-encoded deltas still could be 
of some use. If we&#x27;re working with something like UTF-encoded text or some 
data that already has undergone fixed-sized RLE, it might make sense to not 
process single bytes, but pairs or even triplets of them instead. But that&#x27;s 
something I&#x27;ll tell you about next time, when all these thoughts are sooner 
or later going to take some practical shape.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-13-on-serialization-of-binary-chunks.txt</link>
<title>(2023-04-13) On serialization of binary chunks</title>
<description><![CDATA[<pre>
(2023-04-13) On serialization of binary chunks
----------------------------------------------
Today is a Clean Thursday, and my post will be much shorter and cleaner than
any of the recent ones.

If you need a really safe way to store or transfer arbitrary binary data
chunks, just use djb&#x27;s netstrings ([1]) which are constructed like this:

[length]:[content],

Here, the trailing comma is mandatory and the [length] is decimal, with
starting zero prohibited unless the length is 0 (so, an empty string is 
encoded into three characters &#x27;0:,&#x27;).

The decoding algorithm is dead simple as well:

1. Read everything until the first : byte as a decimal number L. If L is not
a valid non-negative decimal integer number (or starts with 0 while being 
positive), report an error and halt.
2. Allocate a buffer B which is L bytes long. If allocation is impossible,
report an error and halt.
3. Read the next L bytes after the first : byte into the buffer B. If reading
L bytes is impossible, report an error and halt.
4. Read the (L+1)-th byte. If it&#x27;s not equal to the &#x27;,&#x27; byte, report an error
and halt.
5. Return the buffer B as the result. End of algorithm.

Why is the trailing comma needed if we already specify how many bytes to
read? I asked this question myself some time ago. The comma being in the 
expected place after the chunk is an additional (although not fully 
surefire) marker that no bytes were lost from the chunk during the transfer 
and no extra bytes were added, and the actual chunk length is exactly as 
declared.

I really can&#x27;t praise this format high enough. I recommend using it as a
top-level wrapper for whatever message format you employ in your scripting, 
desktop, mobile, server, embedded applications. Even if your messages are in 
plaintext, they will remain in plaintext, but now much safer. Netstrings 
introduce so little overhead while adding so much value that they really 
help to make the world a bit better place. Just use them wherever you can.

And no, I&#x27;m not confusing marshalling with serialization. And neither should
you.

--- Luxferre ---

[1]: https://cr.yp.to/proto/netstrings.txt
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-12-on-structured-text-data-formats.txt</link>
<title>(2023-04-12) On structured text data formats</title>
<description><![CDATA[<pre>
(2023-04-12) On structured text data formats
--------------------------------------------
From time to time, I come across various articles about why this or that
format rulez or suxx, endless debates like XML vs. JSON vs. YAML vs. TOML 
and so on. I&#x27;m astounded about the fact that these debates are being held in 
all possible seriousness. As if the readability of the format is solely 
determined by the syntax itself and not by the way you choose to write your 
own documents in it. In my practice, I had seen a lot, including nearly 
unreadable YAMLs and perfectly readable JSONs and XMLs, although, of course, 
the opposite can be seen a bit more often. For me, any of these discussions 
just don&#x27;t make sense. When choosing an export/configuration format for your 
project, you must only answer three simple questions:

1) Does the data need to be readable/editable by humans?
2) How critical is the performance of saving/loading the data by the machine?
3) What is the deepest level of hierarchy that _really_ needs to be stored?

Trust me, everything else is not as important as you think. Moreover, once
you answer these questions, the results might surprise you as they most 
probably wouldn&#x27;t match your first assumptions. But it&#x27;s not about the 
format, it&#x27;s about how you arrange your data. And even then, you can make an 
informed choice towards BOTH readability AND ease of parsing. Here, I&#x27;m 
going to overview two easily most overlooked and underrated structured text 
data storage formats that would make our lives so much easier if everyone 
was using them for appropriate situations instead of all this zoo.

The first format is something that you, since you&#x27;re reading this on Gopher,
must already be familiar with: TSV, tab-separated values. Yes, Gophermaps 
are just TSV files and nothing else. The name looks like the format was 
derived from CSV, but in fact it&#x27;s far more ingenious in its simplicity. 
Unlike CSV where you have to escape commmas (and don&#x27;t even get me started 
on the fucking M$ that actually allowed _semicolon_ instead of comma as a 
delimiter for some locales, and still calls that abomination CSV) and 
ideally quote all strings with whitespaces and commas and escape all the 
quotes inside and so on, TSV allows you to just write things as is. Because 
no one uses the tab character, CR or LF in their tabular data or 
configuration values anyway. If they really have to be used there, they are 
just replaced with \t, \r and \n respectively, with the backslash itself 
being escaped as \\ in this case. But that&#x27;s really it. This format is 
extremely easy to parse and write, and probably offers the highest 
machine-friendliness to readability ratio. The only precaution you must take 
if you write TSV files manually in a text editor and not programmatically is 
to make sure tabs are saved as tabs and your editor doesn&#x27;t automatically 
convert them to spaces. Other than that, it just works. Nothing to complain 
about, really.

Except one thing. TSV format, exactly because of being so simple, doesn&#x27;t
offer a straightforward way to store hierarchical data of deeper levels than 
just &quot;list of key-value objects&quot; or &quot;key - fixed-size list of values&quot;. Sure, 
just like with Gophermaps, you can use the first field to store the value 
(and optionally the type) and all the subsequent fields to store keys and 
subkeys, but variable amount of fields in each row would greatly reduce both 
readability and parsing efficiency, and that&#x27;s not what we want. So, in case 
these levels of hierarchy are not enough, we must find something even more 
ingenious to describe more complex structures while not introducing another 
JSON, YAML or, gods forbid, XML level of complexity for machine parsing and 
still keep the format entirely humanly readable AND writeable.

Enter Recfiles. This is a format created under the GNU umbrella to describe
relational database-like structures using simple plaintext files. By the 
way, I won&#x27;t talk about using Recutils here, I don&#x27;t care much about them. 
For now, I&#x27;d like to focus on the format itself. As far as I understood, 
it&#x27;s about as simple as Gemtext.

1. Comments:
* Any line starting with the # character is a comment
* Comments can only be written on a separate line, # must be the first in it

2. Fields:
* They are name-value pairs separated with colon and space (&quot;: &quot; or &quot;:\t&quot;)
* Field names are case-sensitive
* Any field name must match this regexp: ^[a-zA-Z%][a-zA-Z0-9_]*$
* Field names starting with % denote metadata, not data
* Any field value must be terminated with LF (except \LF or LF+ cases)
* If a line ends with \LF instead of LF, the next line continues the value
* Newlines in values are encoded as LF+ and a single optional whitespace
* Fully blank lines are allowed and not counted as fields
* In all other cases, the line after LF must begin with a valid field name

3. Records:
* A record is a group of fields written one after another
* Can contain multiple fields with identical names and/or values
* Records are separated by one or more blank lines (like paragraphs in MD)
* Record size is the number of fields it contains

And this is where the syntax itself ends. Everything else documented about
Recfiles, including the notion of record sets and how we describe them using 
record descriptors (which are just records containing metadata fields only, 
something like database table schemas) is completely optional, built upon 
this syntax and constitutes implementation details specific to a particular 
set of tools (GNU Recutils). If you&#x27;re interested in diving deeper into the 
canonical implementation that GNU Recutils are, I recommend their full 
manual ([1]) for further reading. It really is fascinating. However, with 
Recfiles being a fully open format, its implementations are not limited to 
just one, and some other tools adopt simpler modes of operation. The 
reference recfile parser in Tcl ([2]), for instance, only recognizes the 
%rec metadata field in the descriptor to turn its value (record type) into a 
top-level key in the output dictionary.

I really like this format for the same reason I like Gemtext among others:
because it is fully line-oriented. That is, after splitting your text by LF, 
you can unambiguously determine the type of each line based on the character 
it starts with. In fact, a single record parser can be defined with a very 
simple informal algorithm:

1. Initialize an empty string buffer BUF. Set the literal reading mode to off.
2. Read the next line L.
3. If the literal reading mode is on, append the contents of L to BUF and go
to step 9.
4. If L is empty, go to step 11.
5. If L starts with #, go to step 2.
6. If L starts with +, skip an optional whitespace after it and append an LF
and the further L contents to BUF, emit the flag to update the previously 
emitted field, then go to step 9.
7. Read all characters until the first colon (:) in L as NAME. If NAME
matches the ^[a-zA-Z%][a-zA-Z0-9_]*$ regexp, then save it, otherwise discard 
it.
8. Clear the BUF value. Read all characters after the first colon (:) in L
into BUF. If BUF now starts with a whitespace (0x20 or 0x9), remove this 
whitespace.
9. If BUF ends with a backslash, then remove it from BUF, turn on the literal
reading mode and go to step 2.
10. Turn off the literal reading mode and emit NAME as the current field name
and BUF as the current field value. Go to step 2.
11. Report the end of the record. End of algorithm.

Note that the algorithm doesn&#x27;t tell us that to do with the duplicate field
names. We determine this ourselves, as well as how to contatenate the + 
lines.

Now, even if we aren&#x27;t aiming for a full SQLite3 or TextQL replacement, what
can we use this bare format for?

Tabular data? That&#x27;s the native mode of recfiles, although just using them as
a drop-in replacement for CSV or TSV probably won&#x27;t showcase their full 
potential. Every record with the same set of unique field names naturally 
corresponds to a table row. For example, any Gophermap line has a 1-to-1 
mapping to a Recfile record.

INI/TOML/.properties-style configuration? Easy! Just use the metadata fields
like %rec to name your sections and unique field names in each record. 
Everything else is the same key-value structure.

JSON/YAML-style configuration of any depth? Also easy:
* all objects and lists of objects are named via the %rec descriptor;
* objects with primitive values are just records with unique key fields;
* lists of primitive values are just records (or parts of records) with
non-unique key fields;
* nesting is done by referencing something like &#x27;%rec/[name]&#x27; instead of the
primitive value.

Let&#x27;s take a look at the example YAML from some CloudBees tutorial:

---
doe: &quot;a deer, a female deer&quot;
ray: &quot;a drop of golden sun&quot;
pi: 3.14159
xmas: true
french-hens: 3
calling-birds:
  - huey
  - dewey
  - louie
  - fred
xmas-fifth-day:
  calling-birds: four
  french-hens: 3
  golden-rings: 5
  partridges:
    count: 1
    location: &quot;a pear tree&quot;
  turtle-doves: two

Now, here&#x27;s how it might look as a Recfile (note that&#x27;s just one option out
of many):

# top record descriptor - may be omitted
%rec: top

doe: a deer, a female deer
ray: a drop of golden sun
pi: 3.14159
xmas: true
french-hens: 3
calling-birds: huey
calling-birds: dewey
calling-birds: louie
calling-birds: fred
xmas-fifth-day: %rec/xmas-fifth-day

# subrecord descriptor
%rec: xmas-fifth-day

calling-birds: four
french-hens: 3
golden-rings: 5
partridges: %rec/partridges
turtle-doves: two

# another subrecord descriptor
%rec: partridges

count: 1
location: a pear tree


Another example might be this highly nested JSON that contains some arrays of
objects:

{
  &quot;id&quot;: &quot;0001&quot;,
  &quot;type&quot;: &quot;donut&quot;,
  &quot;name&quot;: &quot;Cake&quot;,
  &quot;ppu&quot;: 0.55,
  &quot;batters&quot;: {
    &quot;batter&quot;:
      [
        { &quot;id&quot;: &quot;1001&quot;, &quot;type&quot;: &quot;Regular&quot; },
        { &quot;id&quot;: &quot;1002&quot;, &quot;type&quot;: &quot;Chocolate&quot; },
        { &quot;id&quot;: &quot;1003&quot;, &quot;type&quot;: &quot;Blueberry&quot; },
        { &quot;id&quot;: &quot;1004&quot;, &quot;type&quot;: &quot;Devil&#x27;s Food&quot; }
      ]
  },
  &quot;topping&quot;: [
    { &quot;id&quot;: &quot;5001&quot;, &quot;type&quot;: &quot;None&quot; },
    { &quot;id&quot;: &quot;5002&quot;, &quot;type&quot;: &quot;Glazed&quot; },
    { &quot;id&quot;: &quot;5005&quot;, &quot;type&quot;: &quot;Sugar&quot; },
    { &quot;id&quot;: &quot;5007&quot;, &quot;type&quot;: &quot;Powdered Sugar&quot; },
    { &quot;id&quot;: &quot;5006&quot;, &quot;type&quot;: &quot;Chocolate with Sprinkles&quot; },
    { &quot;id&quot;: &quot;5003&quot;, &quot;type&quot;: &quot;Chocolate&quot; },
    { &quot;id&quot;: &quot;5004&quot;, &quot;type&quot;: &quot;Maple&quot; }
  ]
}

And here is how I&#x27;d represent it as a Recfile (omitting the toplevel
descriptor and comments for brevity this time):

id: 0001
type: donut
name: Cake
ppu: 0.55
batters: %rec/batter
topping: %rec/topping

%rec: batter

batter: %rec/batterlist

%rec: batterlist

id: 1001
type: Regular

id: 1002
type: Chocolate

id: 1003
type: Blueberry

id: 1004
type: Devil&#x27;s Food

%rec: topping

id: 5001
type: None

id: 5002
type: Glazed

id: 5005
type: Sugar

id: 5007
type: Powdered Sugar

id: 5006
type: Chocolate with Sprinkles

id: 5003
type: Chocolate

id: 5004
type: Maple


Although this example and even more complex ones are obviously
machine-generated (i.e. the data came as a result of calling some API) and 
it doesn&#x27;t resemble anything to be used for configuration purposes, these 
data still are more human-manageable in this format (which is still easy to 
write or read programmatically) instead of trying to guess where the closing 
bracket was missing and which one exactly, curly or square, it was. It also 
doesn&#x27;t cause eyestrain from the abundance of quotation marks and the fact 
that you need to escape them if they are encountered inside your string 
values. I also could provide an example of how to handle XML-structured 
data, but I hope you already get the idea.

Regarding non-Recutils implementations of Recfiles, it was also interesting
for me to find out that Bash itself, being a GNU project, was supposed to 
include a readrec builtin command that would facilitate reading a whole 
record from a file as opposed to parsing the lines obtained via the read 
builtin. In fact, however, this &quot;builtin&quot; never became a real builtin 
shipped within Bash. For it to work, you still need to install Recutils 
separately (and on my Arch, I had to do this from AUR) and then plug the 
readrec.so library like this:

enable -f /usr/lib/readrec.so readrec

Even without the entire package overhead, this particular library, on x86_64
architecture, weighs about 14K. Not really sure whether all this is really 
necessary to just parse a simple record format and handle special newline 
cases within field values, especially that the command itself doesn&#x27;t do 
much else. Also, contrary to GNU&#x27;s own specification, this command doesn&#x27;t 
enforce whitespace characters after the colon to delimit field values from 
their names (in the input, but does insert them in the output). That&#x27;s why I 
created a sourcable script for modern Bash versions (4.3 and up) with my own 
version of the command, readreclx, that mimics readrec&#x27;s behavior (although 
doesn&#x27;t set the REPLY_REC variable) and weighs under 3K bytes. You can 
consider it a reference implementation of the algorithm mentioned above. And 
it looks like it deals with edge cases just fine, although more thorough 
testing might be required. As usual, I have published this script in my 
downloads section on hoi.st.

Why did I do this? Because such formats really deserve more attention, more
love and more independent implementations.

--- Luxferre ---

[1]: https://www.gnu.org/software/recutils/manual/index.html
[2]: https://wiki.tcl-lang.org/page/recfile
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-11-sharballs-better-than-tarballs.txt</link>
<title>(2023-04-11) Sharballs > Tarballs</title>
<description><![CDATA[<pre>
(2023-04-11) Sharballs &gt; Tarballs
---------------------------------
Nowadays, when people hear the words &quot;archive file&quot;, they usually think of
something like .zip or .7z (or, if they are completely braindead, something 
like .rar) files, that contain some directory structure in compressed form. 
Most of them associate archiving with compression and don&#x27;t have a slightest 
clue they are completely different processes, and that even the Info-ZIP 
format supports the &quot;store&quot; method that doesn&#x27;t compress any data. Those who 
live in a more healthy kind of environment, surely know about tarballs, but 
still not every one of them might understand or intuitively get why these 
tarballs often have two separate suffixes in their names, like .tar.gz, 
.tar.bz2, .tar.xz and so on. And only those who have seen and worked with 
.cpio.gz files definitely know the truth about this, because otherwise they 
wouldn&#x27;t be able to create a single file in this format.

And the truth is that compression algorithms don&#x27;t work with filesystem
structures like directories and files themselves. They only work with 
streams of continuous data. Turning the former into the latter is the sole 
task of an archiver. There is a whole lot of software that only archives 
files and directories whithout any compression, with tar, ar and cpio being 
the most famous and popular examples. Yes, modern GNU tar can automatically 
call the compressor (gzip, bzip2, xz) if we tell it to, but it still is a 
fully separate stage. We can gunzip a .tar.gz file and still work with the 
bare .tar file as if it wasn&#x27;t created with the gzipping option. This is why 
archive formats are NOT the same as compression formats, and are an 
interesting topic on their own.

By the way, I&#x27;m not really sure why tar took over cpio for general usage. The
cpio format itself is more straightforward and doesn&#x27;t require 512-byte 
block alignment, and now allows (and even recommends) to use plain 
ASCII-based file headers, making it a fully plaintext format in case your 
files are also plaintext. The only _major_ difference is that cpio command 
itself (that, in GNU version, even supports tar/ustar format creation!), in 
the archival mode, only accepts the _full_ list of files/directories from 
the standard input and outputs the resulting stream to the standard output. 
In the extraction mode, it accepts the stream from the standard input. I 
like this behavior more than tar&#x27;s, because it is implemented in the true 
Unix way and serves the initial purposes of cpio (passing complex directory 
structures over the network or between linear-access storage devices like 
tapes) much better. The tar command always could simulate this experience, 
but its default mode is accepting the flags and the archive file first, and 
files/directories to add (in the archival mode) afterwards. And, unlike 
cpio, if you add a single directory to the list, tar will automatically add 
all the underlying elements recursively. Maybe not having to use find 
command for this purpose made tar more appealing to noobs, as well as not 
having to pipe the output to gzip of whatever for further compression, as 
it&#x27;s just a matter of a single letter flag you pass to the tar command. 
That&#x27;s probably why tarballs, whether compressed or not, became a de-facto 
standard in the modern Unix-like ecosystem, despite cpio being much more 
suitable for backup-restore scenarios.

But what if I told you that there exists an archive format that&#x27;s even more
noob-friendly in terms of unpacking (kinda like SFX-type archives in 
Faildows), has minimum dependencies to create the archives and zero 
dependencies to unpack them, and is portable across all POSIX-compliant 
systems? Interested? Well, this format is called shar (SHell ARchive) and it 
doesn&#x27;t have a single specification... well, because it&#x27;s just a generated 
shell script that recreates initial files from the data stored in it. So, 
there is no separate shar unpacker, the files unpack themselves when passed 
to sh. And all major differences between various shar flavors are in the 
following things: how they store the data internally, what are the 
dependencies required by the archiver and what are the dependencies required 
by the self-unpacking script. Historically, shar archiver was a shell script 
too but most current shar versions are written in C, and shell scripts 
generated by them depend on the echo and mkdir commands and some kind of 
monsters like sed and uudecode. I personally don&#x27;t support this approach, as 
echo can have some caveats in different OS implementations, uuencode and 
uudecode might not be installed at all and sed is a Turing-complete language 
by itself. That&#x27;s why I naturally decided to create my own version of shar. 
As a shell script, of course, but, for the first time in all these years, 
not a Bash-specific one.

Writing a shar clone may seem a very straightforward task until you start
thinking about minimizing external dependencies as much as possible. I 
decided that the minimum requirement for my shar is that it must work at 
least on Busybox and on the bare KaiOS 2.5.x/Android 6 ADB shell with Toybox 
or whatever it has there. By &quot;work&quot; i mean both archiving and extraction. 
The questions that I had put before myself were:

1) What to use instead of echo?
2) What to use instead of uuencode to pack binary files?
3) How to read binaries in a non-Bash-specific way?
4) How to ensure we don&#x27;t have duplicate input files and directories?
5) How to ensure we don&#x27;t have EOF markers in our packed content?

And the answer to the first two questions came almost instantly: printf.
Alas, POSIX printf doesn&#x27;t have the wonderful %q specifier that would solve 
90% of my problems, and I didn&#x27;t want to make Bash a dependency even for 
packing only. As for the EOF markers all current shell-only shar versions 
use, we could use them with variable reading and some end-of-line 
manipulation, and this is what I tried first. But Android&#x27;s shell reminded 
me that this is the case when dumber is smarter. So instead of using EOF 
markers, I ditched this approach altogether and wrote a function to 
serialize any file into a series of shell printf calls by a fixed chunk 
length (because we don&#x27;t want to overflow the 130K command buffer, do we?). 
And this function also addresses the question number three: use as standard 
version of read builtin as possible with an empty IFS value, and read the 
file byte by byte. It is slow but reliable. Then, using another printf call, 
the ASCII code of the byte is retrieved, and then, depending on its value, 
it is output &quot;as is&quot; or as a \x-sequence, hex-encoded. With what? With 
printf, of course! Now, since the final printf call in the shar file that 
actually unrolls the bytes will be called with %b specifier, we must also 
make sure that all single quotes and backslashes are also passed in there 
hex-encoded. That&#x27;s another two conditions added into our loop.

Once a proper serializer is created, that already is 80% of success. Now, as
shar traditionally accepts the exhaustive list of input files from the 
command line arguments only, and they can come from various sources, there&#x27;s 
no guarantee that the input won&#x27;t contain duplicates, and, of course, we 
don&#x27;t want duplicates in our archive. This is where we must make use of an 
external command dependency, namely, sort -u. Some might argue that 
sort|uniq might be more portable but I&#x27;ve actually never seen any sort 
command version - in GNU/Linux, macOS, Busybox or even Toybox - that 
wouldn&#x27;t support the -u flag. Looks like portable enough to me, at least 
from the archive creation standpoint. Apart from that, I&#x27;d like to make the 
shar script create the entire directory structure _before_ writing any files 
into it, so a separate loop to do this was implemented.

And that actually is it. The entire archiving script, lshar.sh, that I have
published in the downloads section of my main hoi.st Gophermap, is exactly 
60 SLOC of simple and well commented code that is portable across various 
shells. And, just like the very first versions of shar, this script also is 
released into public domain. I guess this will be my primary tool for 
publishing new code and the code migrated from Git repos (something I 
already suggested in my previous post, by the way). Obviously, just like 
with any other shar, Lshar can be combined with gzip or a similar tool to 
achieve compression. Examples:

Archive and compress:
find my-dir | xargs sh lshar.sh | gzip -9 &gt; my-sharball.shar.gz

Decompress and unroll:
gzip -d -c my-sharball.shar.gz | sh

Note that, due to the script nature, unrolling is always fast but archiving
isn&#x27;t. Since the serializer processes one byte at a time, it&#x27;s not the 
fastest thing in the world (and on KaiOS phones, it&#x27;s very noticeable), so 
I&#x27;m probably going to walk the path of the original shar creators and write 
a portable ANSI C89 version of the same tool at some point in the future. 
For the time being though, it serves its purpose and also is a cool example 
of working with individual bytes in shell scripts in a way that isn&#x27;t 
Bash-specific.

Using tarballs to show respect to the Unix way? Switch to sharballs if you
truly love it.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-10-2-rethyncing-devops.txt</link>
<title>(2023-04-10) ReThyncing DevOps</title>
<description><![CDATA[<pre>
(2023-04-10) ReThyncing DevOps
------------------------------
This summer, I&#x27;m about to reach the point where I have already been working
for 7 years as a DevOps engineer, whatever it means to you. Yes, the term 
&quot;DevOps&quot; still doesn&#x27;t have a commonly understood definition, which is a 
problem by itself because customers, vendors and contractors often think 
about different things when they even hear this term, not even to mention 
when they actually try to build any kind of business relationship on this 
ground with each other. Some folks perceive DevOps as a bunch of sysadmins 
on steroids doing cloud deployment automation and/or SRE kind of support, 
some view them as the guys who actually help development teams organize 
CI/CD and release management processes, some even think that there&#x27;s no such 
thing as &quot;DevOps engineer&quot; at all and it is just a set of common practices 
and methodologies used by whatever specialists the teams already have, and a 
new job position isn&#x27;t necessary to implement them. Well, the truth is, as 
IRL I had to do all this stuff for different customers, I have to agree with 
all these points, even with the last one stating we don&#x27;t exist. Why?

Because in an ideal world, we wouldn&#x27;t. In an ideal world, developers would
know how to write code that&#x27;s easy to deploy and scale, and sysadmins would 
know how to offer developers a way to deploy and scale this code with little 
to no human intervention. But we don&#x27;t live in an ideal world either, that&#x27;s 
why DevOps engineers exist. And the practices we employ are also far from 
ideal. There are, however, several major issues we could have avoided en 
masse (and on the individual/team level, anyone still can) that have deeply 
rooted into the DevOps discipline as a whole during all these years as this 
discipline started to encompass more and more practices from different areas 
of IT, and these issues just won&#x27;t go away unless a conscious collective 
effort is made to eliminate them entirely.

The first and probably the biggest major issue is some totally blind
assumption that everything that starts small _will_ grow large. Hence all 
the premature project complexity growth for the purposes of scalability that 
might never become necessary. Writing a simple network service script that&#x27;s 
reasonable enough to put behind a properly configured xinetd and that&#x27;s it? 
Hold up a second:

- &quot;No, you don&#x27;t know where you&#x27;re going to run it from, containerize it!&quot;
- &quot;No, you don&#x27;t know what the workload is gonna be, put your service
container into a Kubernetes cluster!&quot;
- &quot;No, you don&#x27;t know how many machines you&#x27;ll need, put your cluster into an
autoscaling host group and configure load balancing on the cloud side too!&quot;
- &quot;No, it&#x27;s considered bad practice to do it manually, use IaC tools like
Terraform or CloudFormation!&quot;
- &quot;Oh, and don&#x27;t forget to use Helm charts instead of bare Kubernetes
manifests because the latter will drive you insane!&quot;
- &quot;No, you can&#x27;t just maintain your service code and IaC files at one place,
they need to be in separate repos, and... oh, why didn&#x27;t you already put 
everything in Git, it would make your life so much easier!&quot;
- &quot;Fine, you have everything in Git, but where&#x27;s your CI/CD process? Write a
Jenkinsfile or .gitlab-ci.yml or something already...&quot;
- &quot;What do you mean you&#x27;re running Jenkins directly on the host? You have to
create a corresponding autoscaling cluster for its worker nodes, and then 
also maintain it within your IaC repo, and make two jobs where one builds 
your service and puts it into the artifact storage (try Artifactory), and 
the other deploys it onto your cluster from there, and...&quot;

Fuck. Off.

I&#x27;m rsyncing my script onto the server along with the xinetd.conf. And it
works without any of your resource-hogging bullshit. IF and when I need to 
build a scalable system, I&#x27;ll build a scalable system. I know how to build 
them without your &quot;best practices&quot;.

The second major issue arises as a direct consequence of the first one:
making everything VCS-centric instead of human-centric. Despite the name, 
version control systems are now being used for every single aspect of 
development except the actual version control. Now, Git itself is still 
great for the purposes it was initially created for: to simplify 
collaboration on the project code for entire teams. And for these purposes, 
it&#x27;s easy to see why it surpassed CVS, Subversion, Mercurial etc. And I 
might not like every aspect of it (&quot;you made a typo? screw you, you can&#x27;t 
override your already pushed commit unless you have an explicit permission 
to force-push, so everyone will see from now on that you made a typo&quot;), but 
for TEAM collaboration on SOFTWARE code, I guess nothing better exists yet. 
However, there are cases where I consider Git (or any VCS, for that matter) 
a bit overkill:

1. Personal projects. If you work on the code alone, rsync over SSH is fine
enough for both remote storage and deployment. If you need to preserve the 
version, just make a tarball of your code tree before further editing, name 
it accordingly and back it up in a reliable place. Anyway, lots of small 
files with unreadable metadata in .git must die. I publish my projects on 
various Git repos (started with GitHub long ago, then moved to GitLab, now 
on SourceHut) more out of courtesy to those who might be ready to fork them 
than out of personal necessity. And mind you, I started doing this when I 
was too poor and uninformed to order a VPS I can trust at least to some 
extent. However, now that I have it, with all the recent repo service 
owners&#x27; craziness (like &quot;no-crypto&quot; policy on SourceHut and all the SJW 
madness on GitHub/GitLab), instead of increasing the entropy by installing 
Forgejo/Gitea or whatever on my VPS, I&#x27;m actually considering to stop doing 
this altogether and provide a simpler access interface to all of my source 
code trees or tarballs instead. Maybe even via Gopher, why not?

2. Documentation. Unless we are talking about code comments or docstrings
that various languages support, any other documentation must be separated 
from the code entirely. And the process to collaborate on it is much better 
done on wiki-type resources or real-time collaborative editors. I won&#x27;t 
endorse anything of particular here, but there are several cool FOSS options 
to do this.

3. Infrastructure as code. Again, there is nothing wrong about describing
infrastructure with nice, readable, declarative syntax. I fully support this 
movement. What I don&#x27;t support is putting it onto Git. Repeat after me seven 
times: infrastructure is not software. Being able to describe it as code 
doesn&#x27;t make it software. Software development practices don&#x27;t apply to 
infrastructure. Files that describe infrastructure can never be subject to 
large team collaboration, or you WILL run into a disaster sooner or later.

The third major issue is trying to containerize everything. Containers are
not a bad thing per se, on x86 systems I use them for a lot of things 
myself, and even this host actually runs many containerized web 
proxy/gateway services behind Traefik. However, in my case it was a 
necessity because I probably couldn&#x27;t have gotten these services to work all 
at once on the host system otherwise. And this is the problem: 
containerization discourages service developers to write simple, 
interoperable, portable and secure code. They no longer have to test it 
anywhere except a single version of a single Linux distro of their choice. 
Instead of trying to reduce external dependencies as much as possible, they 
encapsulate all of them into the image and tell us &quot;here, you&#x27;re not going 
to have any hiccups if you run this as a whole thing&quot;. Instead of providing 
command-line options for us to be able to specify where to look for all the 
configuration and data, they gladly hardcode these paths because all the 
customization can be done via volume mounts anyway. Instead of learning to 
work within limited user permission boundaries, they get root access to 
everything within the container and manipulate various directories they 
don&#x27;t really need to touch, not to mention opening privileged network ports 
they don&#x27;t need to use. Containerization is great if you know what you&#x27;re 
doing, and only if you can write a proper service without it, but you just 
need to isolate all internal networking (if your services, for some reason, 
can&#x27;t be designed to conduct all internal communication via Unix domain 
sockets instead) and/or ensure particular versions of particular 
dependencies you really cannot get rid of. In any other cases, it is the 
opposite of efficient and also creates totally unnecessary entities ready to 
be shaved off with the Occam&#x27;s razor.

Finally, the fourth major issue is a neverending stream of buzzwords aimed to
conceal the truth. I could babble about them all day, but my most favorite 
example is &quot;serverless&quot;. Lolwut? Serverless? Does it mean any peer-to-peer 
architecture? No, it&#x27;s a glorified name of a fucking function-as-a-service 
(yes, FFaaS, now THAT should be the term!) that still runs on servers and 
consumes extra resources you&#x27;re paying for. And no, the AWS Lambda name is 
in no way better. Historically, the word &quot;lambda&quot; came from lambda calculus 
where it denoted _anonymous_ functions, and it has been meaning anonymous 
functions henceforth. In AWS though, every &quot;lambda&quot; has a name, well, how 
would you run it otherwise? 

I love my job, and if I quit it, I think I&#x27;ll quit the sphere of IT entirely.
But, after seven years of experience in the DevOps world, I fully and firmly 
understand that the only buzzword and methodology it desperately needs right 
now is the KISS principle. Keep it simple, stupid.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-10-1-about-http-0.9.txt</link>
<title>(2023-04-10) Why HTTP/0.9 sucks but still matters</title>
<description><![CDATA[<pre>
(2023-04-10) Why HTTP/0.9 sucks but still matters
-------------------------------------------------
Among various posts and discussions around some &quot;big Web&quot; proponents
criticizing Gopher and Gemini, sometimes the HTTP/0.9 abbreviation comes up. 
This is the original specification of HTTP ([1]) back from 1991, which is 
notably similar to Gopher protocol in some aspects: first, the server 
accepts a single CRLF-terminated request string (where CR is optional), 
second, the server returns the response body immediately with no headers or 
status messages and closes the TCP connection on finish. All this sounds so 
familiar that you might think &quot;wow, the only HTTP version that could be 
integrated within Gophermaps!&quot; Well, yes, but actually not quite.

First, the request string we send to the HTTP/0.9 server really can only
refer to the pathname similar to the selectors in Gopher, but it must be 
prepended with the &quot;GET&quot; method name and a whitespace. There actually are no 
other methods in this HTTP version, just &quot;GET&quot;, but it must be specified. 
This already makes building a selector not so straightforward but still 
doable. Well, the second problem is far more paramount. The standard 
explicitly says that the response must be returned in the HTML format. It is 
the only content type allowed. There is, however, an interesting remark in 
the original statement:

&gt; The format of the message is HTML - that is, a trimmed SGML document. Note
&gt; that this format allows for menus and hit lists to be returned as
&gt; hypertext. It also allows for plain ASCII text to be returned following
&gt; the PLAINTEXT tag.

Dafuq is &quot;the PLAINTEXT tag&quot;? As someone who had known HTML for about 15
years (and its basics for like 20), I had never heard of one. But I looked 
it up on MDN, and indeed, here it is ([2]):

&gt; The &lt;plaintext&gt; HTML element renders everything following the start tag as
&gt; raw text, ignoring any following HTML. There is no closing tag, since
&gt; everything after it is considered raw text.

Wow. Just wow. This is even cooler than &lt;xmp&gt; and &lt;marquee&gt;. This tag had
been deprecated since HTML 2 (!) and the MDN page is half-red from trying to 
tell you that you should not use it, but all major browsers still support 
it. What does all this mean to us? It essentially means two things:

1) we still can view/download HTTP/0.9 documents directly from Gopher clients
by shaping special GET-prefixed selectors in the maps or addresses;
2) we still can serve plaintext documents to Web browsers supporting HTTP/0.9
requests from a Gopher server under special GET-prefixed selectors by 
automatically prepending &lt;plaintext&gt; before our response.

Unfortunately, case 1 is much harder to try out nowadays because more and
more servers are dropping HTTP/0.9 support and treat single-line requests as 
malformed HTTP/1.0 or HTTP/1.1 requests instead. For instance, anything 
behind Cloudflare or Traefik can&#x27;t be accessed using the 0.9. Unsurprisingly 
though, textfiles.com and frogfind.com worked, and you actually can shape 
URLs like gopher://frogfind.com:80/0GET%20/?q=example or 
gopher://textfiles.com:80/0GET%20/internet/acronyms.txt - although, of 
course, Textfiles has a direct mirror on Gopherspace, anyway, you get the 
idea. Well, no one prevents you from appending &quot; HTTP/1.0&quot; to the selector 
if you want to, but then you&#x27;ll have to deal with all the header 
parsing/skipping yourself.

For case 2, there also might be another issue with Chromium-based browsers
dropping HTTP/0.9 support for non-default ports (and for insecure 
connections, this only leaves port 80 for us) but Gopher, thankfully, 
doesn&#x27;t care which port to work on, and we can proxy all requests coming to 
port 80 by removing the &quot;GET &quot; part, and then prepending &lt;plaintext&gt; to all 
the responses. Of course, it won&#x27;t do any good for gophermaps and binaries, 
but will at least make any plaintext documents accessible from the browser 
with little to no overhead. Or, we can add a bit of overhead to generate a 
valid HTTP/1.0 response by prepending the following instead of the 
&lt;plaintext&gt; tag: HTTP/1.0 200 OK&lt;CRLF&gt;Content-Type: text/plain&lt;CRLF&gt;&lt;CRLF&gt;

For experimental purposes, I created a &quot;GET &quot; directory in the content root
of this server and placed an index.map file with my &lt;plaintext&gt; message 
there. If you visit http://hoi.st:70/ from a Web browser that still supports 
HTTP/0.9 _requests_, you should be able to see my message about HTTP 0.9. 
Otherwise you&#x27;ll see a message about HTTP 1.0 or 1.1, depending on which 
version you&#x27;re trying to access the server with. Their sources are located 
in the &quot; HTTP&quot; directory in the corresponding files.

As you can see, some interoperability still can be achieved, but of course,
it should only be viewed as a temporary measure. A proper Gopher client is 
something no Web browsers and proxies can ever substitute.

--- Luxferre ---

[1]: https://www.w3.org/Protocols/HTTP/AsImplemented.html
[2]: https://developer.mozilla.org/en-US/docs/Web/HTML/Element/plaintext
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-09-on-timekeeping.txt</link>
<title>(2023-04-09) On reliable timekeeping on slow networks</title>
<description><![CDATA[<pre>
(2023-04-09) On reliable timekeeping on slow networks
-----------------------------------------------------
I am very passionate about timekeeping. I have a nice collection of watches,
some of which are capable of syncing via longwaves or even Bluetooth LE, and 
the protocol to do this already is reverse-engineered, at least to the 
extent of performing basic time synchronization tasks ([1]). I&#x27;ll probably 
write a non-GUI tool for it as well once I figure out what the most optimal 
stack is to write it on top of. But, of course, these tools also need some 
source of truth. Something needs to set the time on our own client devices 
before we can pass it further or display it to the user.

Nowadays, all time synchronization and coordination over the Internet is
usually done via the NTP protocol. It&#x27;s really well-engineered and takes 
into account a lot of factors and allows to receive accurate time all over 
the world. Can&#x27;t really complain about that. One thing I can complain about 
though, is that it&#x27;s too complex to reimplement from scratch, and some CVE 
reports about NTP server or client vulnerabilities just confirm that. On top 
of that, it involves a lot of overhead data in every synchronization packet, 
which might not be a lot in modern conventional networks, but pose a 
significant problem once we&#x27;re talking about something like GPRS at 30 Kpbs, 
PSTN or CSD dialup at 9600 bps, AX.25 at 1200 bps or even slower 
transmission modes at 300 bps. In these conditions, every extra byte 
matters. The solution? Ye goode olde Time protocol (RFC 868), which some 
timekeeping networks (like time.nist.gov) still gracefully run on the port 
37 of their servers. It returns a 4-byte (32-bit) timestamp on a TCP 
connection or as a response to any UDP datagram, and that&#x27;s it. The 
timestamp is expected to represent the number of seconds since the beginning 
of the year 1900 UTC, and is supposed to roll over every 136 years.

Now, I encourage you to only use the UDP mode of this protocol whenever you
need it, as TCP connections made to just retrieve a 4-byte payload both pose 
significant overhead for our purposes and don&#x27;t make server admins happy 
either. And, just like with NTP, you still need some way to measure elapsed 
time locally with around a millisecond precision. Once it is sorted though, 
the algorithm to get more or less accurate time (although with a whole 
second resolution) is very simple and straightforward:

1. Prepare a tool for time measurement of steps 2 and 3 combined.
2. Send a random 32-bit datagram to the Time server.
3. Receive the 32-bit timestamp datagram FTIME from the Time server.
4. Record the execution time (in milliseconds) of steps 2 and 3 as ETIME.
5. Obtain the true Unix timestamp (in seconds) using the following formula:
   TRUETIME = |(1000*FTIME + ETIME/2 - 2208988799500) / 1000|
6. Emit the TRUETIME value for further processing. End of algorithm.

There is a couple of things that might need explanation here. First, the RFC
says the Time protocol expects an empty datagram in UDP mode, but IRL most, 
if not all, implementations accept any datagram and just discard its 
contents. The 4-byte length of the outgoing datagram was chosen to make the 
resulting IP packet have exactly the same length as the one we&#x27;re going to 
receive, so we can then safely divide the elapsed time by 2 to get more or 
less accurate correction. Second, the constant 2208988799500 is the number 
of milliseconds between the start of the year 1900 (where Time protocol 
timestamps start) and the start of the year 1970 (where the Unix epoch 
starts), minus 500 milliseconds used for rounding the final result properly. 
So, starting with the year 2036 when the 32-bit Time protocol counter rolls 
over, we will be adding 2082758400500 here instead of subtracting 
2208988799500. And this is something that we need to know before applying 
the formula, but I hope no one will ever finds themselves in the situation 
they don&#x27;t know whether or not the year 2036 already has come. But just in 
case, here is a more future-proof version of the same algorithm with a 
larger safety margin (until the year 2106):

1. Prepare a tool for time measurement of steps 2 and 3 combined.
2. Send a random 32-bit datagram to the Time server.
3. Receive the 32-bit timestamp datagram FTIME from the Time server.
4. Record the execution time (in milliseconds) of steps 2 and 3 as ETIME.
5. Obtain the true Unix timestamp (in seconds) using the following formula:
   TRUETIME =
     |(1000*FTIME + ETIME/2 - 2208988799500) / 1000| if FTIME &gt; 2208988800,
     |(1000*FTIME + ETIME/2 + 2082758400500) / 1000| otherwise.
6. Emit the TRUETIME value for further processing. End of algorithm.

Somewhere between the year 2036 and 2106, you can switch to using the second
formula unconditionally, and this will prolong the algorithm to the year 
2172. Afterwards, you adjust the offset accordingly (500 + the amount of 
milliseconds that actually passed between the start of 1970 and the start of 
2172), and so on. This way, the 32-bit second counter can be reused forever.

This approach has the following advantages: only a single send-receive round
required, no transactional overhead thanks to UDP (4 bytes out, 4 bytes in), 
not having to rely on the local clock for anything but elapsed time 
measurement, and simple but accurate enough compensation for the roundtrip 
time. For really slow networks, I can&#x27;t think of anything better at the 
moment. This is why I hope that even when NIST stops serving time using this 
protocol, someone still will. Maybe, I&#x27;ll set it up right here on hoi.st, 
who knows.

--- Luxferre ---

[1]: https://git.sr.ht/~luxferre/RCVD
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-08-on-buzzwords.txt</link>
<title>(2023-04-08) On permacomputing, frugal computing and other buzzwords</title>
<description><![CDATA[<pre>
(2023-04-08) On permacomputing, frugal computing and other buzzwords
--------------------------------------------------------------------
First off, I was delighted to know that there already is an IRC client
written in pure Bash, so that I don&#x27;t have write it myself. It&#x27;s called 
Birch and I gave it a shoutout on my main hoi.st page. It&#x27;s a nice proof 
that everything is possible if you&#x27;re involved enough. It&#x27;s great. I&#x27;m proud 
of whoever made it. It&#x27;s cool to know I&#x27;m not alone in it. Let BashNet 
flourish!

Now, onto the today&#x27;s topic. One of the many things that eventually brought
me here was some (not big, but some) practice in contribution to the 
computing minimalism world: reading about VTL-2, CHIP-8, BytePusher, Uxn, 
creating a CHIP-8 emulator for KaiOS, creating a 999-byte BytePusher engine 
in JS, implementing a KaiOS-compatible Uxn core, SPARTA/ESOP specs that 
probably may never be finished (although ESOP is closer to a working model, 
but that&#x27;s it, a model), and finally, inventing my own Equi and NRJ-OISC VM 
architectures (I guess I&#x27;ll dedicate another post to them someday soon) that 
I still have to write most of the standard library for, and implementing 
them in pure C. Again, it&#x27;s not much, and I don&#x27;t even try to compare my 
experience to the creators&#x27; of Uxn/Uxntal or even BytePusher, but I dove at 
some depth into all this and naturally have read some supporting material.

So, among this material, I frequently encountered terms like
&quot;permacomputing&quot;, &quot;frugal computing&quot;, &quot;salvage computing&quot;, &quot;sustainable 
computing&quot;, &quot;degrowth computing&quot; and even &quot;vernacular computing&quot;. While the 
overall idea behind all this looks simple, I really was a bit dazzled by 
this amount of buzzwords coming at me to highlight this idea and different 
aspects of it. But if we remove all this fancy cover, what&#x27;s left that 
actually matters? If you look up any of these buzzwords on the &quot;big Web&quot; 
(which, ironically enough, doesn&#x27;t have a place in any of these models that 
would truly work), you&#x27;ll see all the history of the terms and who coined 
them, but again, it&#x27;s not so important. What is important is that they all 
point to the same single fundamental fact:

1. We consume much more resources than we really need to fulfill our
day-to-day computational tasks.

And that is the essence everything boils down to. Now, different authors may
go differently about what kind of resources we&#x27;re talking about. All of them 
agree that it&#x27;s about energy resources users themselves consume, most of 
them also add the energy required to manufacture the computing devices and 
also to maintain all the networking infrastructure and so on. Some authors 
even count all the material resources as a whole, from start to finish. 
Like, if your ancient Spectrum consumes more electrical power than your 
(still old but newer) Symbian phone but you got the Spectrum for free and 
it&#x27;s still working but you had to spend some money on the phone, then your 
Spectrum is considered more frugal overall, especially when considering all 
the resources required to manufacture that phone and deliver it to you as 
well. Now, that&#x27;s a radical kind of vision even to me but I even know some 
people IRL who stick to it. Then, the &quot;permacomputing&quot; and &quot;sustainable 
computing&quot; notions also bring us to another point that is no less 
fundamental for them:

2. Computational hardware and its software must be designed in a way that
makes repairing it, replacing its parts or, when it&#x27;s necessary, recreating 
it entirely from scratch as easy as possible.

Now, if we return to our example, that&#x27;s where your Symbian phone definitely
loses to the Spectrum. I knew some folks who soldered Spectrum clones 
themselves, but I know no one who built their own GSM cellphone. With their 
own baseband and everything, I mean, not some ready-made baseband module 
boards like SIM800C (although I think a phone with pluggable SIM800&#x27;s would 
be a great start). Because, despite its size, it&#x27;s much more complicated 
inside. If you were building your own cellular radio module from scratch, 
then AMPS (not even DAMPS) or basic unscrambled NMT would be your best bet. 
They were simple and reliable enough to be reimplemented by anyone, maybe 
that was the _real_ reason why they were phased out. But for the time being, 
I guess, real wireless permanetworking lives on the amateur radio bands with 
all their stuff like AX.25, APRS and other data transfer protocols. With the 
initiatives like HSMM, if you&#x27;re careful and skillful enough, you can even 
repurpose spare WLAN routers to create independent networks on a longer 
distance. Also, for shorter transmissions, don&#x27;t forget about FM repeater 
satellites ([1]). Why? Just because.

But I digress. Let&#x27;s go back to the buzzwords. For my own vision, I&#x27;ve
decided to stick to the term &quot;low-power computing&quot; that reflects what&#x27;s most 
important to me in all this, and, I believe, doesn&#x27;t require any explanation 
to anyone new to the scene. This term encompasses both low wattage 
consumption, which automatically means energy footprint reduction, and low 
processing speed, which automatically means keeping old devices usable these 
days and designing software to be able to run on as many of them as 
possible. I guess I just can&#x27;t find a better term for my ideal model. 
Low-power computing. LPC.

Of course, not all LPC hardware that suits my model necessarily suits
viznut&#x27;s &quot;permacomputing&quot; vision. A bright example would be an old 
scientific+programmable calculator given to me by a close friend of mine, 
the Casio fx-3400P, which I consider a near-perfect piece of LPC hardware as 
it runs off a single battery for over 5 years provided you calculate 
something every day for an entire hour, and also has a solar panel enough to 
power all the calculations without a battery. I don&#x27;t know how old this 
particular calculator is (although they say the model was introduced in 
1988, so probably older than me) but it still works like new. But the thing 
is, everything is so thin inside that I really was afraid to damage 
something when replacing the battery. Yes, it still works and I hope it will 
for a long time, but when it doesn&#x27;t, it doesn&#x27;t forever. No repairability 
that we can talk about here. On the other hand, there is a soviet 
Elektronika MK-52 programmable RPN calculator, the one I had been using for 
at least 2 years before getting my first PC. It&#x27;s a perfect example of fully 
repairable hardware with full internal schematics available, and I guess it 
fits the &quot;permacomputing&quot; vision, but in no way is it an LPC device, as, for 
its capabilities, it was a powerhog that quickly ate four AA batteries or 
had to be powered via the mains adapter with a proprietary connector.

Over time, at the beginning of the 2010s, the EEPROM block in that MK-52
started malfunctioning and the adapter also went dead. And guess what - it 
was manufactured after 1990. Yes, I could repair it now, but why would I do 
this if I got an irrepairable programmable Casio that still works in 2023 
like it did in 1988 or 1989 or whenever it was manufactured, with its only 
theoretical point of failure being the LCD screen (which still is fine 
though), and it has a completely autonomous hybrid power circuit that lasts 
virtually forever with my usage rate and its overall some-dozen-microwatts 
energy consumption? What I&#x27;m trying to say is that repairability is cool 
(and, under some circumstances, even essential), but overall build and 
(internal) design quality that doesn&#x27;t ever give you the need to repair your 
device is even cooler. And, as long it doesn&#x27;t involve any planned 
obsolescence or any other sort of pushing you to buy a new product to 
fulfill the same functions every now and then, it&#x27;s entirely up to you which 
side of the scale to be closer to.

And again, this is where &quot;stop babbling, start acting&quot; principle still holds.
Posting anti-consumerism and frugality ideas solely on the commercialized 
Web that requires a ton of resources to browse and maintain it is akin to 
trying to find a virgin in a brothel. Meanwhile, I have yet to see a single 
viznut&#x27;s page on the Gopherspace or even on Gemini, as opposed to the 
Facebook, Twitter and YouTube links he posted in the footer of his website.

--- Luxferre ---

[1]: https://www.amsat.org/fm-satellite-frequency-summary/
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-07-modern-instant-messaging.txt</link>
<title>(2023-04-07) Modern instant messaging solves non-existing problems</title>
<description><![CDATA[<pre>
(2023-04-07) Modern instant messaging solves non-existing problems
------------------------------------------------------------------
Alright, enough of this meta-content (posting on Gopher about Gopher), now we
can talk about real stuff.

Not so long ago, I was advocating that everyone should move to Matrix from
whatever proprietary crap they&#x27;re using right now: Discord, Slack, WhatsApp, 
Viber, Telegram etc. Yes, everyone with a slightest bit of intelligence 
understands why Matrix really is orders of magnitude better than any of 
those things. But I have stopped advocating for it. For everything users 
already got used to see in modern messengers, it just makes it more 
open-source, secure and distributed. Sorry, federated. That&#x27;s an important 
difference. With a distributed system, I still would be able to connect to 
the KaiOS-themed Matrix room on mozilla.org when my homeserver on 
anonymousland.org went down. But with the federated system that Matrix is, I 
couldn&#x27;t. And that&#x27;s just the first issue with Matrix I spotted right on. 
Tox, for instance, doesn&#x27;t have it, as it is truly distributed. Just saying.

The second issue is that Matrix is just as overengineered as just about
everything else these days. It works over HTTPS. With JSON transport. 
Sometimes even with WebSockets. That&#x27;s an enormous amount of transactional 
overhead, parsing and transport-layer cryptography. It is, in fact, a 
glorified federated webchat system that makes even XMPP (which is already 
bloated beyond repair with all its XML) look like Gopher in comparison to 
Web 2.0. I can&#x27;t imagine running a Matrix client on a 
MIDP2.0/CLDC1.1-enabled phone with GPRS-grade connectivity, 16MHz CPU and 
32KB of RAM. Yes, not megabytes, but KILObytes. And let me tell you 
something right away, to send a message to all Internet communication 
protocol developers of current and future generations: if a client cannot be 
made to work under these specs directly with your protocol, it already sucks 
by default.

The third, and IMO the most important issue, is that Matrix doesn&#x27;t really
offer anything that would facilitate its mass adoption. For noobs, any 
simple but reasonable argumentation like &quot;it&#x27;s just like your favorite 
crapssenger with all the same features, but doesn&#x27;t spy on you by design&quot; is 
definitely not enough for them to switch, especially that they, to support 
their inherent laziness, often employ the chicken-and-egg fallacy like 
&quot;everyone around me doesn&#x27;t use it, so why should I, who will I talk to if I 
join there?&quot;. For tech-savvy people, Matrix doesn&#x27;t bring back simplicity 
and control, as no one can single-handedly recreate a basic chat app over a 
weekend from scratch, starting with bare TCP sockets. Security-wise, it 
makes life a bit easier, but that&#x27;s about it, and that&#x27;s nothing IRC/XMPP 
over TLS or Tor/I2P couldn&#x27;t achieve.

Speaking of XMPP, I see a stunning similarity. XMPP was initially created as
a response to the most popular crapssenger of the time, the father of all 
crapssengers: ICQ. Its protocol was so simple though that it quickly became 
reverse-engineered and alternative clients appeared at the speed of light. 
Whoever owned ICQ (at different times, it was different companies) didn&#x27;t 
like the situation and constantly changed the official protocol, and these 
changes got reverse-engineered and the loop repeated. It was a kind of an 
arms race between official and unofficial client developers. XMPP/Jabber 
protocol was aimed at creating a fully open and federated messaging platform 
as an alternative to ICQ - doesn&#x27;t this remind you of anything? And, just 
like with Matrix, Jabber got much more complicated than the protocol it was 
rivaling. Over the time, it turned into a heap of RFCs and XEPs no one knew 
for sure which to implement, and caring about the userbase just shifted 
somewhere far away. In case you&#x27;re wondering, this is the future I predict 
for Matrix as well.

On a larger scale of things, XMPP ultimately failed, along with ICQ. Why?
Well, as I said, most people are lazy. And someone very clever decided to 
prey on their laziness. Someone, and I don&#x27;t really know who did this first, 
told them: &quot;you don&#x27;t need to think up your user ID or remember the one we 
gave you, you also don&#x27;t need a password: your phone number already is both 
your ID and the method to authenticate you&quot;. And people really started 
falling into this trap, and the brave new world began. Later on, most 
mainstream crapssengers returned to the notion of passwords but (just think 
about it for a sec) as an additional security measure, and they still use 
your phone number as your sole ID and optionally abuse your contact list 
data. Even the seemingly open-sourced Signal does this, which in fact tells 
a lot about who might be really interested in developing this &quot;most secure 
messenger in the world&quot;.

See what I&#x27;m saying? Invent a problem people don&#x27;t have, convince them they
do have it and it really is a problem, and offer a solution for it. That&#x27;s 
how mainstream software development has been working for at least the recent 
25 years (maybe more but I don&#x27;t have enough visibility on things that 
happened before). And even beyond software, that&#x27;s how changes we don&#x27;t need 
are imposed on us in general.

So, what are my proposals instead of using inherently broken &quot;modern
messengers&quot; and their open-source counterparts like Matrix?

- use Tox if you&#x27;re really concerned about privacy more than about resource
consumption;
- use SMS/RCS (where available) if you still want to use your phone number as
the sole identifier;
- use IRC (+bouncers if necessary) for team-wide or individual instant
messaging and its SDCC extension for direct file sending;
- use plain old good email for non-instant messaging with attachments (and,
whenever you don&#x27;t want to invent an ID for some other service, using your 
email address as an ID is _much_ more preferable to your phone number);
- use SIP+ZRTP for voice/video calls over the Internet, or friggin&#x27; Jitsi
Meet if you really need some Zoom-like presentation experience (although Tox 
also has got you covered here, it&#x27;s just a browser-only alternative option);
- use BitTorrent to share large files.

Although, BitTorrent and other simple P2P protocols (simpler than Toxcore)
and what one actually can do with them is a topic for another time.

--- Luxferre ---

P.S. Switching to 78-char width from now on. Will explain later why. Probably.
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-06-why-i-patch-pocket-gopher.txt</link>
<title>(2023-04-06) Why I have patched Pocket Gopher</title>
<description><![CDATA[<pre>
(2023-04-06) Why I have patched Pocket Gopher
---------------------------------------------
So, my ambitious plan to actually make an already wonderful client for J2ME
by Felix Plesoianu, Pocket Gopher, even more usable finally come true. 
Although I really didn&#x27;t expect that many obstacles besides just rewriting 
some essential parts of code. But first, let me tell you why I did all this 
in the first place.

Where I live, second hand phone market is huge. J2ME is almost anywhere
besides the most basic models. And every MIDP2.0/CLDC1.1-enabled phone, 
despite how old it is, can be made a pretty usable Gopher terminal. I have 
already written about it here. But the problem with the original Pocket 
Gopher is, it doesn&#x27;t use screen space too frugally. It inserts odd newlines 
where they shouldn&#x27;t be, it uses whopping 5 bold characters for each 
directory item label (and for 20-character wide screen like on Samsung 
SM-B312E, 5 characters is a real waste), and on top of that, it uses too 
inefficient and unnatural model of pagination that requires users to make 
much more keystrokes than they really need to. No, I didn&#x27;t radically change 
any controls here (although maybe someday I&#x27;ll come up with something 
Kopher-like) but all the issues with newlines and pagination were addressed 
in my fork, as well as some optimization of the rendering part of the code 
had also been done.

Now, apart from &quot;why&quot;, there always is &quot;how&quot;. And that&#x27;s a completely
different story. Working with J2ME in 2023 is incredibly hard, ESPECIALLY if 
you don&#x27;t trust emulators (and rightly so). First of all, you need JDK as 
old as version 8, nothing newer will work. Second, you need a set of 
corresponding CLDC and MIDP libraries from somewhere, along with optional 
JSR APIs if you plan on using them (that&#x27;s something I collected during my 
ReloadME effort in 2019). And then, you need to know how the build process 
is properly organized, because when I spent half a day figuring out what 
wasn&#x27;t working on my Nokia 3310 3G while everything was working on the 
SM-B312E, and then realized it was the lack of _class preverification_ that 
caused all the troubles... And it turned out the only modern, cross-platform 
and open-source way of doing MIDlet class preverification was to use 
Proguard, which I ended up compiling with OpenJDK 8 and also including in my 
source tarball along with the required CLDC 1.1 and MIDP 2.0 jars. You can 
use this tarball as a reference starting point if you plan to write anything 
for J2ME these days by yourselves.

The greatest and also the weakest point of Pocket Gopher, as well as my
Pocket Gopher LX (by the way, LX stands for Luxferre&#x27;s eXtensions and 
doesn&#x27;t really mean anything), is that all rendering is done with StringItem 
elements and their derivative DirectoryItems (actually, in my LX version, I 
unified it to only use the custom DirectoryItem type). Hence all the visual 
focusing gimmicks and overall strange look on some phones. It&#x27;s easy to 
implement, it takes less space and processing time, but it is bound to look 
_very_ different on different devices and sometimes uglier than Opera Mini. 
But then again, Gopher is not about visual appeal, it&#x27;s about getting to the 
desired content as fast as possible. And this client, that weighs 12013 
bytes (the size of my current PGLX 1.0.1 version), really is fast and 
simple. This is why I don&#x27;t regret any minute of effort already put into it, 
and will plan on using it myself whenever I happen to use a MIDP2-enabled 
phone in my daily life.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-05-why-is-gopher-important-to-me.txt</link>
<title>(2023-04-05) Why is Gopher important to me?</title>
<description><![CDATA[<pre>
(2023-04-05) Why is Gopher important to me?
-------------------------------------------
If you end up here somehow, chances are that you already had read a famous
text hosted on Floodgap, named relevance.txt and titled &quot;Why is Gopher Still 
Relevant?&quot; I&#x27;m not going to discuss all the points made in that document and 
whether or not I agree with each of them, but I want to share my own vision 
on the topic and tell you why it is not only relevant, but important to me 
personally.

You see, I&#x27;m now working (as a contractor) for a multinational BigTech
company that, among other things, constantly babbles about transition to 
sustainable energy. The thing is, when their employees actually find 
themselves under power outages, it looks like an apocalypse to them as it 
turns out every single time they weren&#x27;t prepared for this themselves. We 
here, on the other hand, have survived over half a year of bombings of our 
power grid infrastructure, yet I was able to work remotely almost as if 
nothing happened. Because my own peak energy consumption never exceeded 
~80W, and I was able to provide this power with several portable charging 
stations during the scheduled power downtime periods. Now, if only I had _a 
bit more_ solar panels than one and at least one wind generator here, I&#x27;d 
cover my own autonomous electrical supply needs by 100% even during the 
winter time.

But what does Gopher have to do with all this? Well, it&#x27;s about reducing
energy consumption. It doesn&#x27;t take a friggin&#x27; Ryzen to serve hundreds of 
gopherholes from a single embedded node, and it doesn&#x27;t take a latest NVidia 
to create a functional browser to correctly and neatly display them. Unlike 
Web, Gopher is just as usable on the 1993 hardware as it is on the current 
one. It is, in fact, usable on anything that can talk plain TCP/IP and parse 
tab-separated strings. It just works there. Works without the TLS overhead 
(hello Gemini), without complex XML parsers (hello WAP), without client-side 
scripting, same-origin policies, cookies and other nonsense that made Web 
engines impossible to reimplement from scratch (if you want to support 
current standards, that is) and killed all competition. Meanwhile, a 
barebones Gopher client, Bopher, was written in pure Bash for educational 
purposes within half a day, using just Bash&#x27;s own /dev/tcp pseudo-devices. 
Everything else written on top of it under the Bopher-NG umbrella, was 
created to adapt this educational prototype to the actual day-to-day usage. 
And it, while being written in a purely interpreted command language, still 
consumes much less RAM, CPU and disk space than any Web browser in existence 
for the same OS. Needless to say, Gopherspace can be browsed with no trouble 
on platforms like DOS, Win3.1, OS/2, Classic Macintosh (with MacTCP), 
Commodore 64 (with Turbo232), AmigaOS, Acorn RISC OS, VAX, NeXT, PalmOS... 
and even J2ME. Yes, if you still have J2ME-enabled phones that support 
MIDP2.0 profile or higher (i.e. expose the raw socket API), you can install 
a wonderful 15KB-sized PocketGopher.jar on them (btw, I have some plans to 
modernize it a bit, and if I do, there&#x27;s going to be a separate post about 
this effort) and have just as much of fun experience as with browsing 
Gopherspace from your PCs or modern smartphones. Unlike the difference 
between HTTP and WAP where everyone served different content for one and 
another, here the content is absolutely the same regardless which client is 
requesting it. And, since the content is basically either a binary you 
directly download or a text file you display (probably with some additional 
TSV parsing if it&#x27;s a map), you don&#x27;t need to separate the PC and mobile 
versions anyway: the client itself decides how to display it best.

Speaking of mobile experience and continuing the old cellphones topic, I
won&#x27;t stop repeating one thing: Gopher actually is what WAP should have 
been. It offers the same content delivery and basic interactivity features 
while not requiring any transcoding servers, any complex XML markup or 
scripting. I actually have the first WAP-enabled phone in the world, Nokia 
7110, and one of the first GPRS-enabled phones in the world, Nokia 8310, in 
my collection. And I can&#x27;t imagine how much faster the 8310&#x27;s browser would 
have been and how much less battery charge it would consume if it just used 
Gopher with its dead-simple request-response flow instead of 
WML/WMLC/whatever else it has there. Same about the plague of the following 
decade, Opera Mini. One could just avoid so much trouble with that 
transcoding overhead and everything. Besides, having all my internet traffic 
fully processed unencrypted with a centralized third-party proxy makes me 
feel much more uneasy than knowing that only my mobile carrier can see it. 
But then, WAP was just gone with (almost) all its WML sites and carrier 
transcoders. And the number of WAP/WWW sites I can currently browse on my 
Nokia 8310 (which is still working) is next to zero. With Gopher, this 
planned obsolescence can never happen. My point is, any client (or even 
server) device already considered useless for Web is still useful for 
Gopherspace. 

But why, may you ask, is Gopher the primary focus of my attention here, and
not FTP, IRC and other similarly ancient protocols? Well, unlike IRC and 
Telnet, it doesn&#x27;t require us to constantly maintain the TCP connection. 
And, for instance, whenever GPRS disconnects on the phone I use Pocket 
Gopher on, I don&#x27;t need it until I make the next request (again, saving 
precious battery life). Unlike FTP, it doesn&#x27;t require setting up two 
different ports for control and data transfer. Unlike email, it doesn&#x27;t 
require setting up a store-and-forward facility that provides two completely 
different services to send and receive messages. The only other protocol on 
par with Gopher I can think of is Finger... well, because technically it is 
absolutely the same protocol, and any Finger query can be represented as a 
Gopher selector. To me, Gopher looks like the perfect balance between the 
human-scale level of implementation from bare TCP stack, frugal computing 
and functionality for everyday usage under even the harshest conditions of 
network connectivity.

This is why I consider Gopher important to me, as well as to everyone else
who wants to be prepared and to actually make some move towards reducing 
their own energy footprint and overall consumerism. If you&#x27;re reading this 
via a webproxy, go ahead and install a proper client. Stop babbling, start 
acting.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-04-to-wrap-or-not-to-wrap.txt</link>
<title>(2023-04-04) To wrap or not to wrap? (just my opinion)</title>
<description><![CDATA[<pre>
(2023-04-04) To wrap or not to wrap? (just my opinion)
------------------------------------------------------
DO NOT hard-wrap your text if you&#x27;re serving or creating:

* program code or markup documents of any sort
* configuration or log files of any sort
* technical documentation of any sort
* generally any text content that&#x27;s not specifically designed to be only read
via Gopher

DO hard-wrap (and optionally decorate) your text if you&#x27;re:

* generating Gophermap infolines
* writing an article-like post or an essay
* writing an information digest or an e-zine
* generally creating any content to be exclusively read via Gopher by living
humans in its entirety, that&#x27;s not meant for copying, parsing or processing

I felt the need to publish my own opinion on the topic after having read two
opposite writeups:

Pro-hard-wrap (by Wandering Geek):
gopher://gopher.unixlore.net/0/glog/gopher-annoyances.md
Anti-hard-wrap (by Magical Fish):
gopher://magical.fish:70/0/phlog/thoughts-on-text-formatting

Also, I have developed some algorithms to wrap soft-wrapped texts and to
unwrap hard-wrapped texts.

### The Phlow algorithm ###

This algorithm reflows a line to fit the target page width with no word
breaks.

1. Accept the line L and target page width W as parameters.
2. Get the line length LL: LL = len(L).
3. If W is 0 or LL &lt; W, emit L and quit.
4. Allocate a variable LWS to track the last whitespace position. Set to 0.
5. Allocate a variable CPOS to track the current relative position. Set to 0.
6. Allocate a variable BPOS to track the current base position. Set to 0.
7. Allocate an empty output string buffer OUT.
8. For every index I from 0 to LL, perform the steps 9 to 19:
9. If the CPOS value is less than W, go to step 10, otherwise go to step 13.
10. Fetch the current character C from the line L at position I.
11. If C is a whitespace character (0x20), set the LWS value to CPOS.
12. Append the value of C to the output buffer OUT. Go to step 19.
13. If LWS value is 0, set it to W.
14. Emit the value of the output buffer OUT truncated to LWS characters.
15. Empty the output buffer OUT.
16. Set BPOS to BPOS + LWS.
17. Set CPOS and LWS to 0.
18. Set I to BPOS.
19. Increment CPOS. End of iteration.
20. If the output buffer OUT is not empty, emit its value. End of algorithm.

&gt; Y U NO use fmt?

Thanks for asking. First, this is an external dependency on GNU coreutils,
and the main conceptual objective of Bopher-NG is to have as few external 
dependencies as possible. Second, fmt messes up lists, treating everything 
not separated by two newlines as a paragraph. The -s parameter of fmt solves 
this problem but look where it wraps a line if it was started with some 
whitespaces. I don&#x27;t like tools that try to appear smarter than the user, 
and especially those that utterly fail to.

### The Unphlow algorithm ###

This algorithm removes all hard breaks from the text. The only limitation is
that input paragraphs must be separated with (visually) empty lines. Kind of 
what fmt does without the -s parameter.

1. Read the input text into an array A of lines, separating them by LF (line
feed, 0x0a) character.
2. Allocate an empty output string buffer BUF and an empty output string
array OUT.
3. For each line L in the array A, perform the steps 4 to 10:
4. Remove from the line L all leading and trailing occurrences of the
following characters: whitespace (0x20), TAB (0x09), CR (carriage return, 
0x0d).
5. If the line L is empty, go to step 6, otherwise go to step 10.
6. Remove all trailing occurrences of the whitespace character (0x20) from
the buffer BUF.
7. Append the value of buffer BUF to the array OUT.
8. Empty the buffer BUF.
9. Append an empty string to the array OUT. End of iteration.
10. Append the value of L and a whitespace character (0x20) to the buffer
BUF. End of iteration.
11. If the buffer BUF is not empty, perform the operations described in steps
6 and 7.
12. (Optional step) For each line OL in the OUT array, replace all sets of
consecutive whitespace characters listed in step 4 with a single whitespace 
(0x20).
13. Emit the array OUT. End of algorithm.

I hope my reasoning and algos finally will help solve this long-standing
debate.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2023-04-03-intro.txt</link>
<title>(2023-04-03) Finally here</title>
<description><![CDATA[<pre>
(2023-04-03) Finally here
-------------------------
So, it finally happened. I&#x27;m really starting my journey to Gopherspace.

I have a bad habit of writing all the tooling I can&#x27;t find myself, and for
this journey, I came prepared - several authoring scripts, another script to 
put it all together and... two fucking DIY Gopher clients, one for Bash and 
another for KaiOS. And I can proudly say that Bopher-NG is currently more 
comfortable to use than Lynx, and that Kopher is much more usable than the 
stock KaiOS 2.5.x browser.

I also happen to have a few J2ME-enabled phones so I tested Pocket Gopher on
them as well. Although obviously not thoroughly designed as Kopher, my user 
experience with PG definitely beats Opera Mini or whatever else &quot;large Web&quot; 
capabilities they tried to shove into such phones. I liked this piece of 
software very much, it really shows the potential of Gopher becoming the 
next WAP if someone wanted to.

For the server, I&#x27;m planning to write my own someday too. In Bash + xinetd,
of course. But for the time being, I&#x27;m using gofor-lx.py, a modified version 
of gofor.py by yam655. Why modified? Because in my coordinate system of 
values, the directory Gophermap should be called index.map and not just 
lowercase gophermap, so I made gofor.py look for that name instead. I also 
removed all the &quot;smartness&quot; from the map generation process, I want it to be 
served exactly the way I prepare it. I can look after fields, CRLFs and 
trailing dots myself.

That&#x27;s it for now. Over the time, I&#x27;ll be migrating whatever mess I made on
my happynetbox landing page to this hole and let that landing page be what 
it&#x27;s supposed to be, just a landing page. By the way, I really thank HNB 
admin(s) for allowing me to experiment with everything I could until I set 
up this one, and not banning me for ANSI code styling or concealing their 
tailing ad. Now, it&#x27;s time to go standalone. And more interesting content is 
coming here soon.

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2024-01-29-elite-in-awk-the-beginning.txt</link>
<title>(2024-01-29) Elite in AWK: the beginning</title>
<description><![CDATA[<pre>
(2024-01-29) Elite in AWK: the beginning
----------------------------------------
As I said in the previous post, I wasn&#x27;t joking. Elite, even in its text
version, is an interesting case of how a seemlingly infinite universe can be 
created with pretty limited resources involved. And when Ian Bell, one of 
the original Elite creators, released the universe generation and trading 
algorithms in a form of Text Elite in 1999 (with some fixes going on until 
2015), I knew it was just a matter of time for me to try and port it myself 
when I discovered it.

Of course, what I had to deal with was monstrous. While Bell was a genius who
did wonders in 6502 assembly in the original Elite series, it looks like he 
didn&#x27;t master C at all, even calling it &quot;a brain-dead language&quot; in the 
original source code file. Maybe, that&#x27;s for the best, otherwise I&#x27;d have to 
deal with pointer arithmetic when porting it, to the addition of everything 
else. By the way, I have nothing against Bell himself. But I had to read the 
code of _three_ other ports of Text Elite to understand better what whas 
going on there: in JS, in Python and in Erlang (which I can barely read but 
hey, it&#x27;s much more readable than e.g. Rust). And as for these ports, I do 
have questions to their authors.

Let&#x27;s take a look at this tiny code fragment from the original TXTELITE.C:

int myrand(void)
{	int r;
	if(nativerand) r=rand();
	else
	{	// As supplied by D McDonnell	from SAS Insititute C
		r = (((((((((((lastrand &lt;&lt; 3) - lastrand) &lt;&lt; 3)
        + lastrand) &lt;&lt; 1) + lastrand) &lt;&lt; 4)
        - lastrand) &lt;&lt; 1) - lastrand) + 0xe60)
        &amp; 0x7fffffff;
    lastrand = r - 1;	
	}
	return(r);
}

Screwed-up indentation and typos (&quot;Insititute&quot;, lol) are all over the code,
so I got used to it when porting, but the middle expression that calculates 
r... I really had to spend more time than I should in order to understand 
that this abomination could just be replaced with:

r = (lastrand * 3677 + 3680) % 2147483648

Again, Bell was porting his algos from 6502 &quot;as is&quot; without even trying to
optimize anything (as he didn&#x27;t know C well enough), no questions to him. 
But... all three ports I looked at, written by third-party people, had the 
same original formula with a ton of shifts and parens intact. This means the 
porters didn&#x27;t even try to understand what it was actually doing. And this 
was the point I understood that my own port had to be completed, it had to 
be done strictly in POSIX AWK with no external dependencies and it had to 
become better than all of those.

When done with the basic porting (which was quite non-trivial as POSIX AWK
doesn&#x27;t support hex literals in strings or in code, as well as bitwise 
operations), I quickly understood how boring the original TE 1.5 was. Come 
on, it had &quot;cash&quot;, &quot;hold&quot; and &quot;sneak&quot; commands, as well as free 
intergalactic jumps! It was a fair trade engine recreation, but far from... 
well... a game. It was too easy to perform unfair moves there, and it didn&#x27;t 
even have any reward system besides bare cash. So I decided to move on and 
create my own game-like experience on top of the original codebase, and 
first things I did was to make hyperjumps non-free (5000 credits like in the 
original Elites), to limit the &quot;hold&quot; command to one-time 35t upgrade for 
400 credits (like in the original Elites) and to move &quot;cash&quot; and &quot;sneak&quot; 
commands to a special cheat mode. A bit later, I introduced savestates so 
the cheat mode (along with those commands) became fully unnecessary and was 
removed. I also ported galaxy names from the Archimedes version of Elite. 
But then, I had two more things to do to consider my part done.

The first and, I guess, most important thing is the statistics and ranking
system. The original Elite versions had ranks based on the number of kills, 
which doesn&#x27;t make any sense in this version that has no combat whatsoever. 
One of the modern Elite incarnations, Elite Dangerous, has its own &quot;Trade&quot; 
and &quot;Explorer&quot; ranking scales, but the point system is very different from 
the classics, so I just took the trade rank names from there. But the method 
to calculate the rank still was on me. And I still am not sure whether the 
formula I settled on is viable, but as of today, it&#x27;s like this: the game 
keeps track of *all* your expenses, then calculates the overall profit 
margin as (cash - 100CR) / expenses, then takes floor(log2) of it and 
assigns the rank based on this number. So, e.g. to get to the Elite rank, 
you must have made 128 times more money than you spent. I repeat, I don&#x27;t 
know whether this is possible at all yet, so the formula (or at least the 
logarithm base) might change in the future.

The second thing is the ship upgrade system, and this is the most recent
change to the game to this day. Instead of the &quot;hold&quot; command (one-time 
upgrade to 35t for 400 CR in my port or arbitrarily changing the cargo in 
the original TE), the &quot;upgrade&quot; (or &quot;up&quot;) command was introduced, that would 
allow you to buy different ships (that, in this game, still only vary by 
their cargo hold space) for different prices. Here, the ship names, specs 
and prices were taken from Oolite:

- Cobra Mk3 Extended (35t, 400 CR) - like in the originals
- Python (100t, 200000 CR)
- Boa (125t, 450000 CR)
- Boa 2 (175t, 495000 CR)
- Anaconda (750t, 650000 CR)

The implementation of this feature was quite straightforward (while the
&quot;hold&quot; command seemed like a hack anyway) but again, I&#x27;m still not sure if 
all of them are even necessary. Still, attempting to reach the highest 
upgrade tier can be considered one of the in-game goals.

Speaking of goals, what about finding the legendary Raxxla? Of course, the
planet with such a name doesn&#x27;t exist in the classic Elite universe, but if 
we consider &quot;x&quot; as a wildcard character, there exists precisely one planet 
that fits this template. If you find it, my port will give you a hint about 
that. It might seem totally unnecessary but brings an additional element of 
fun lacking in the original Bell&#x27;s version.

That being said, I invite you to play my port, awlite, and test it out:

git clone git://git.luxferre.top/awlite.git

At the time of this post, its major version is v1.8 (the numbering started
from the direct port being v1.5) and I doubt the savefile format will 
further change much, but the code is readable and the README is a must-read 
if you seriously want to play it. I hope you enjoy.

By the way, please write to me on Matrix if you have any suggestions or
feedback about the game or any other project of mine: 
@luxferre:hackliberty.org

--- Luxferre ---
</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/posts/2024-02-05-busting-stereotypes-ed.txt</link>
<title>(2024-02-05) Busting Stereotypes ep.666: featuring ed and rlwrap</title>
<description><![CDATA[<pre>
(2024-02-05) Busting Stereotypes ep.666: featuring ed and rlwrap
----------------------------------------------------------------
I have a real dilemma now. I&#x27;m no longer sure whether or not I should
continue doing anything regarding nne development or porting it to Nim. The 
reason? I finally learned ed.

No kidding. It actually is even more no-nonsense than the text editor of my
own creation. While jokes about ed are about as old as me or even older, I 
guess there have been some huge misconceptions about it floating around for 
all this time. Hadn&#x27;t I eradicated these misconceptions from my mind first, 
I wouldn&#x27;t be writing this post in ed right now.

From the &quot;normal users&#x27;&quot; perspective, they might be intimidated by not
knowing what to do when they run ed for the first time, that&#x27;s totally true. 
Because they got used to a certain concept of a text editor, even if the UI 
is fully text based and modal like vi/vim. I recently discovered a whole 
team of very professional specialists, working for the same customer as I 
do, that also fell into this trap of knowing only one concept of an editor. 
Meanwhile, I have developed a different approach that gets things much more 
streamlined.

The secret is: forget about &quot;editors&quot; as an app UI class. At all. Instead,
recall the concept of REPL. Nowadays, REPL (&quot;read-eval-print loop&quot; if you 
didn&#x27;t know) interfaces are pretty much everywhere: shells, programming 
language interpreters, console-based DB clients, text adventures, even some 
cryptowallets, and so on and so Forth (pun intended). Professional people 
already have been using REPL interfaces for pretty much everything they 
need, well, why not apply the same concept to the task of text editing? 

As it stands, ed is just a REPL for text editing. And a pretty good one.

Of course, even though ed doesn&#x27;t have a lot of commands to learn, grasping
the simple thought expressed above still might not help the transition right 
away. There still can be two major obstacles, and I&#x27;m going to tell you 
right now how I overcame both of them.

The first obstacle might be the obvious fact that, as with every REPL, ed is
purely line-oriented. You can address every line, move around lines, change 
entire lines, but operating with individual characters inside those lines, 
deleting something particular from and appending to existing lines is not so 
straightforward as you might want it at first. Yes, you really need to 
master the substitution command to do this. Once you do this though, you can 
become as fast as you are with screen-oriented editors.

The second obstacle is that ed, operating on bare stdio streams, doesn&#x27;t
offer you much control even over the line you&#x27;re currently entering, that 
is, no arrow keys per-character cursor movement, no per-word navigation, no 
input/command history etc. And this is where the rlwrap utility comes into 
play. It allows to add the entire libreadline functionality to any program 
run through it. And if you learn to customize it through your ~/.inputrc, 
you can even add custom shortcuts for ed specifically! This is why, instead 
of the vanilla ed, I&#x27;m actually using these aliases:

alias ee=&#x27;rlwrap ed -p:&#x27;
alias eer=&#x27;doas rlwrap ed -p:&#x27;

In addition to using ed with rlwrap, they also set a prompt character by
default. I chose colon because it reminds me of vi command mode, so that 
it&#x27;s easier for me to understand that I&#x27;m in it right now. And the 
~/.inputrc section that adds some custom shortcuts looks like this:

$if ed
set bind-tty-special-chars off
set convert-meta on
Meta-SPC: &quot;\n.\n&quot;
Control-w: &quot;\n.\nw\n&quot;
&quot;\e[Z&quot;: &quot;\C-v\t&quot;
TAB: &quot;  &quot;
$endif

Now I can exit from the ed&#x27;s input mode by just pressing Alt+Space and save
the document right from the input mode with Ctrl+W. I also can replicate my 
vimrc setup where Tab key inserts two spaces and Shift+Tab inserts the 
actual tabulation character. All thanks to rlwrap, which, by the way, now 
also highlights matching opening parentheses when I type the closing ones. 
Ain&#x27;t it beautiful?

As far as I have found out, libreadline integration is just a tip of the
iceberg. I&#x27;m not going to talk about people piping ed to line-oriented 
syntax highlighters like pygmentize: that&#x27;s totally possible (which is 
awesome by its own) but just not my cup of tea. But the overall 
scriptability of ed on one hand (I think the -e flag in diff wasn&#x27;t created 
just for fun) and the ! command on the other hand (to run external shell 
commands without exiting ed) should give you a vague picture of what this 
editor is capable of, especially in combination with the g (global action) 
command and/or supplying regexps directly instead of line ranges. And don&#x27;t 
even get me started on the possibilities that can be achieved with the r 
command that just reads another file contents or even the results of ! 
command into the current buffer position.

So, what are my plans on ed? Where am I going to use it? The answer is
simple: everywhere I can. Writing posts and letters, writing rich text 
documents (Markdown, Gemtext, HTML, roff... wait a minute, we haven&#x27;t talked 
about it yet, probably sometime later), programming, automation, configs... 
You name it. Once you learn it properly (that is, not only learning the 
command set but, more importantly, learning to decompose your editing flow 
into atomic operations), there&#x27;s no reason not to use it instead of vim or 
anything even more bloated. Especially when you have the convenience of 
using rlwrap and its full inputrc-based customization power at your disposal 
for interactive ed usage, while being able to totally omit it for 
scripted/automation usage. By the way, I have hosted my .inputrc at 
git://git.luxferre.top/dotfiles.git, among other things, and it&#x27;s the most 
frequently updated dotfile at this repo as of now. So you can take a look 
how I have set up the flow for myself.

The biggest conclusion out of this though is that stereotypes can and should
be fought. Everything should be doubted these days, even the public opinion 
on such an old and well-known piece of software. And besides, it still is a 
part of POSIX standards, so knowing how to use it won&#x27;t hurt anyway.

Ed, man! !man ed

--- Luxferre ---

</pre>]]></description>
</item>
<item>
<link>gopher://hoi.st/1/</link>
<title>Go to root</title>
<description><![CDATA[<pre>
HOI.ST: Luxferre&#x27;s personal gopherhole
--------------------------------------
Hi, this is Luxferre. I&#x27;m into low-power computing (LPC), non-touchscreen
cellphones, non-Swiss wristwatches, non-proprietary software and 
non-bullshit discussions.

My resources on Gopher:

&lt;a href=gopher://happynetbox.com/1luxferre&gt;My fingerpage on HNB&lt;/a&gt;
&lt;a href=gopher://hoi.st/1/posts&gt;LuxPhlog (my posts and articles)&lt;/a&gt;
&lt;a href=gopher://hoi.st/1/docs&gt;LuxDocs (various useful tech info pages, both mine and third-party)&lt;/a&gt;
&lt;a href=gopher://hoi.st/1/dl&gt;Downloads&lt;/a&gt;

&lt;a href=gopher://hoi.st/0/data-formats-hall-of-fame.txt&gt;Data formats Hall of Fame&lt;/a&gt;
&lt;a href=gopher://hoi.st/0/programming-languages-hall-of-fame.txt&gt;Programming languages Hall of Fame&lt;/a&gt;

My resources on Gemini:

My poems (mostly in Ukrainian)

My resources on &quot;big Web&quot;:

Homepage
Blog (highly subjective)
SourceHut

My Gopher-related projects:

Kopher (client for KaiOS 2.5.x)
Bopher-NG and Bopher Tools
OSmol (static Gopher server in OCaml)

Useful links on Gopherspace:

&lt;a href=gopher://magical.fish/1/&gt;Magical Fish&lt;/a&gt;
&lt;a href=gopher://i-logout.cz/1/bongusta&gt;Bongusta Phlogs&lt;/a&gt;
&lt;a href=gopher://bitreich.org/1/lawn&gt;Gopher Lawn&lt;/a&gt;
&lt;a href=gopher://1436.ninja/1/Port70News&gt;Port 70 News&lt;/a&gt;
&lt;a href=gopher://graph.no/0help&gt;Graph.no weather forecasts&lt;/a&gt;
&lt;a href=gopher://gopher.icu/1/&gt;gopher.icu&lt;/a&gt;
&lt;a href=gopher://bay.parazy.de/1/&gt;Torrent search&lt;/a&gt;
&lt;a href=gopher://tilde.pink/1/~bencollver/dir/&gt;Google Directions&lt;/a&gt;
&lt;a href=gopher://worldofsolitaire.com/1/games&gt;World of Solitaire&lt;/a&gt;
&lt;a href=gopher://happynetbox.com/1/&gt;Happy Net Box&lt;/a&gt;
&lt;a href=gopher://tilde.club/1/~freet/gophhub/&gt;GophHub (GitHub via Gopher)&lt;/a&gt;

Search engines for Gopher (direct 7-type links):

Veronica-2 (by Floodgap)
Contrition (by ForthWorks)
Quarry (by gopher.icu)

For the brave explorers:

&lt;a href=gopher://gopherspace.de/1/alive/new.report.php&gt;Daily Gopher servers still alive&lt;/a&gt;
&lt;a href=gopher://p3x981.com/1/cgi-bin/known-hosts&gt;Known servers&lt;/a&gt;
&lt;a href=gopher://gopher.viste.fr/1/ogup/list&gt;The Observable Gopherspace&lt;/a&gt;

--- Luxferre ---

</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-02-12-cosa-nostra.txt</link>
<title>(2024-02-12) Cosa Nostr-a</title>
<description><![CDATA[<pre>
(2024-02-12) Cosa Nostr-a
-------------------------
This week, once again I came across an interesting case of people offering a
technical solution to a non-technical problem. For some unthinkable reason, 
they still believe in the success of this solution though. Meanwhile, I got 
convinced in the opposite as soon as I found out the details: it&#x27;s built on 
top of websockets, its core developers are clueless noobs that don&#x27;t believe 
there are other version control systems than Git, it tries hard to integrate 
with the most inconvenient cryptocurrency ever, and it already attracts more 
propaganda morons than it should given the stage of its development. By now, 
you might already have guessed I&#x27;m talking about Nostr.

The idea of this network looks quite noble: to create a decentralized,
extensible and censorship-resistant (at least that&#x27;s what they say) free 
speech medium where people could share their ideas in various forms and 
establish social contacts. All Nostr messages are digitally signed &quot;event&quot; 
objects in the JSON-based format described in so-called &quot;NIPs&quot; — Nostr 
Implementation Possibilities. By the way, the name &quot;Nostr&quot; itself means 
&quot;Notes and Other Stuff Transmitted by Relays&quot;. Relays are just servers that 
serve the events and accept them from others. However, this is where the 
first problem is: relays don&#x27;t talk to each other, only to end users.

Why is this a problem? Because everyone is required to agree upon some set of
relays where they can find each other, and even then it&#x27;s not fully 
censorship-resistant. You can only be not silenced if you run your own 
relay, but then, good luck getting anyone else finding it if you&#x27;re already 
censored, and this effectively doesn&#x27;t make any difference from running your 
own website or a gopherhole (like this one).

Another huge problem is implementation bloat. I haven&#x27;t been able to find a
single Nostr client in plain C or Nim. The closest to that was Algia written 
in Go. Requiring EC cryptography, JSON *and* websockets to write a minimum 
viable client is just too much. Not to mention that even web-based clients 
are naturally heavy and don&#x27;t work in non-JS browsers like Links or NetSurf. 
A lot of these clients also integrate &quot;zap&quot; functionality, which is a word 
for giving tips via... Bitcoin Lightning network. And on top of it all, to 
do zaps, they promote custodial (!) wallets which are implemented as browser 
extensions, as well as some extensions to store Nostr private keys... I lost 
count how many security antipatterns were involved in the implementation of 
all this.

Yet, despite its immaturity, Nostr already is infested with propaganda bot
farms just like any other social media. And public relay owners don&#x27;t seem 
to give a shit. That makes me wonder: who really runs those relays and for 
what purpose? I won&#x27;t be surprised if they are used to coordinate botnet 
attacks sometime in the future yet do nothing with it as well.

Still, for some small talk, I&#x27;m gonna hang around Nostr for some time at
least, because this network does have some good things too (in comparison to 
the mainstream ones). Just remember: the most dangerous form of slavery is 
the one that gives you an illusion of freedom.

--- Luxferre ---

</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-02-19-got-into-tron.txt</link>
<title>(2024-02-19) Got into Tron by necessity, staying due to curiosity</title>
<description><![CDATA[<pre>
(2024-02-19) Got into Tron by necessity, staying due to curiosity
-----------------------------------------------------------------
The week that has just passed might be full of the biggest revelations for me
so far in the year 2024.

As I&#x27;m partially moving my funds into crypto, I faced a question: which
currency to store them in? Bitcoin is out of question (yet): too volatile, 
the fees are enormous and the transaction time too (not counting the crutch 
in the form of Lightning network). Monero is awesome but still not stable 
enough. USDT on Ethereum — again, the fees just destroy all the fun. USDT on 
Polygon — too immature and not widely adopted. This is why I decided to base 
it all upon USDT TRC20 token, i.e. on the Tron network.

But then, when this decision was made, another problem awaited me. While
there are several good non-custodial AND FOSS Tron wallets for Android, 
there are no such ones for Linux desktop, and barely any for CLI, not 
mentioning the monstrous official Java-based wallet-cli and some strange 
alternative implementation in Crystal that won&#x27;t work e.g. on OpenBSD 
anyway. Yes, I had to DIY my own Tron wallet specifically for my own usage 
needs. And yes, I went for it: git://git.luxferre.top/kisstron.git

When you clone the repo, you can easily see why I chose Python to implement
kisstron. This wallet is based upon a wonderful tronpy library, which itself 
is opensource and glues together many other pure-Python and C-FFI 
cryptographic pieces to interact with the Tron blockchain the way the 
interaction should be done to be convenient for third-party developers. 
Being quite a noob in Tron, I only needed about 4 days to create a fully 
functional wallet with several advanced features. One of these features 
though, being so minor in itself, became the base for those revelations I&#x27;m 
gonna talk about. But first, I need to clarify several things about the Tron 
network that are very important for understanding what happens next in my 
story.

So, despite Tron was mainly advertised as an Ethereum competitor, it has
quite a different approach to how transaction fees work. Unlike Ethereum and 
similar networks, where you always have to have some main &quot;network-native&quot; 
currency (ETH, MATIC, AVAX etc) on your balance in order to cover the 
so-called &quot;gas fee&quot; in addition to the tokens you want to transfer, Tron, 
while being dependent on its main TRX coin too, offers you a way to pay for 
transactions with so-called &quot;energy&quot; and &quot;bandwidth&quot; points. I won&#x27;t go into 
details on how these points are spent, earned or automatically replenished, 
I can only say two things that are essential: 1) for TRC20 token transfers, 
only energy points are spent (and if there are none, TRX are burned 
directly), 2) any Tron transaction API allows you to set the fee limit, i.e. 
the maximum amount of TRX that can be burned by a transaction, and if the 
transaction requires more, it is considered failed and OUT_OF_ENERGY 
contract execution result is recorded on the blockchain.

Of course, when writing the wallet, I tested it on a Tron testnet (Nile, to
be exact, because Shasta is harder to find *both* TRX and USDT faucets for), 
and testnets generally tend to have higher fees than the mainnet, so I had 
set up a hard fee limit of about 10000 TRX at first, or something like that. 
I naturally thought (because that&#x27;s what I read in the docs) that the 
mainnet fee limit is normal to set to, like, 10 TRX, so this is what I 
hardcoded as well.

Now, the story itself begins when I finally tested out everything I could on
Nile, switched to my primary wallet on mainnet and tried to do the first 
&quot;real&quot; USDT token transaction to an real non-KYC merchant (of course I won&#x27;t 
disclose which one). I got an OUT_OF_ENERGY error on kisstron but the 
payment went through on the merchant&#x27;s side. Of course, at first I thought 
that&#x27;s a bug in kisstron, but then I looked at the transaction ID on 
Tronscan (Tron&#x27;s blockchain explorer) website and found out that nothing was 
wrong on my side, the contract invocation really failed with OUT_OF_ENERGY 
and, on top of that, only a small TRX amount (under 2, maybe even under 1) 
was deducted from my balance along with the energy points, all TRC20 USDT 
were intact. Naturally, I still increased the default fee limit in kisstron 
to 100 TRX so that my mainnet USDT transactions would proceed normally under 
any circumstances, but also introduced a new command-line flag to optionally 
manipulate this limit when one needs it.

Nevertheless, I was hit with a stone-hard fact: there are vulnerable
merchants that cannot validate Tron token transactions properly. And, as I 
quickly confirmed, the vulnerability wasn&#x27;t limited to a particular merchant 
but was the same for the entire crypto payment provider they used, as the 
official demo I stumbled upon had the same exact issue. And this payment 
provider is now a bit outshadowed by some newer competitors, but still isn&#x27;t 
completely unknown. I checked whether or not the vulnerability also was 
present on some of its competitors, turns out they do verify the 
transactions properly, but I thought there was another area to try my luck.

Non-KYC automated crypto exchanges. I tried many, and one among them still
turned out to be vulnerable (at least in some conditions), and it&#x27;s not 
completely no-name either (although I can&#x27;t find out who exactly owns it). 
Now we&#x27;re talking serious business. Because it&#x27;s no longer about physical 
goods or services whose sellers, in case of the latter, can just terminate 
your account if they find any billing mismatch, or, in case of the former, 
can just get your ass kicked at your delivery address. It&#x27;s about the 
situation where someday a lot of funds can disappear from the exchange and 
the owners would have no clue what just happened and what to do with it. 
Especially if they are some shadowy actors themselves and try to hide their 
own identity at all cost, but hey, is it immoral to scam the scammers?

And yes, it turned out that you don&#x27;t even have to have the required amount
of USDT to perform this attack, as the energy validation is done outside the 
contract code and the source token amount check is done later inside it. You 
just have to have the amount of TRX necessary to cover the (failed) contract 
invocations. It&#x27;s interesting, however, that the vulnerability doesn&#x27;t have 
anything to do with the blockchain itself, only with how the transactions 
are checked vs. how they should be checked.

As you can see, this is very simple. I really didn&#x27;t think such huge mistakes
could be possible in 2024, when a horde of developers says they are &quot;into 
crypto&quot; and &quot;driving blockchain innovations&quot;. Yet they don&#x27;t seem to grasp 
basic infosec principles, one of the cornerstones of them being &quot;always 
fully validate the input&quot;. Yes, even if the input already comes from the 
blockchain and not directly from a user. And if we think of how many of 
those morons work in other critical areas like military, healthcare, 
aerospace... I&#x27;m afraid that sooner or later we might have another Therac-25 
case, but on a global scale, unless something is done about it quickly and 
swiftly.

I won&#x27;t disclose any further details about which services are vulnerable, nor
am I going to abuse what I already found. But I still cannot decide whether 
such a &quot;scholar&#x27;s mate&quot; in this realm was left there deliberately or they 
really are THAT stupid. And I don&#x27;t even know the full scale of the problem 
yet, that is, how many other crypto payment processors and exchanges are 
affected. If such a total noob with those blockchains like myself could find 
this vulnerability by accidentally sending a token transaction with a lower 
fee limit, I can only imagine what those who have a full-time job of 
searching and finding such blunders in all similar systems could do.

Moral of the story: always code responsibly, and triple that aspect if your
and others&#x27; money depends on it.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-02-26-luxferres-ten-commandments.txt</link>
<title>(2024-02-26) Luxferre's ten commandments</title>
<description><![CDATA[<pre>
(2024-02-26) Luxferre&#x27;s ten commandments
----------------------------------------
1. Abolish unnecessary complexity.
2. Always validate every input, both in software and IRL. 
3. Everything is a trojan until proven otherwise.
4. Making no purchase is always better than any purchase you don&#x27;t need.
5. The best privacy-friendly device is a pocket notebook with a good pen.
6. Question the intentions of anyone who asks for your personal data.
7. Spare every byte you can. But sparing watts is more important.
8. Those who know, don&#x27;t talk. Those who talk, don&#x27;t know.
9. Not your keys, not your coins.
10. The most dangerous form of slavery is the one that gives you an illusion
of freedom.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-03-04-android-and-other-things.txt</link>
<title>(2024-03-04) Android and several other things this week</title>
<description><![CDATA[<pre>
(2024-03-04) Android and several other things this week
-------------------------------------------------------
This one won&#x27;t be long either. Not gonna lie, I didn&#x27;t get much inspiration
throughout the week to write about something exciting. I was emotionally 
exhausted and whatever I&#x27;d been feeling was totally counterproductive. I 
did, however, manage to focus on one particular thing, and that is, 
&quot;reviving&quot; my Gemini PDA (by Planet Computers), i.e. preparing it for some 
active use.

Alas, there is no ready fully degoogled firmware for this device, although
I&#x27;m thinking of preparing one. But the software I have installed there is 
pretty much all FOSS. Starting with app sources themselves: F-Droid, 
Obtainium and FFUpdater. And I found myself using Obtainium more and more, 
as it allows to directly tap into the repos that have release publishing 
enabled. Of course, my minimum list includes KISS Launcher, Amaze, Aegis, 
Termux, Mull, Tor Browser, Orbot, K-9 Mail, LibreTorrent, OnionShare, VLC, 
Xmp Mod Player, Lagrange, Unstoppable Wallet, Monerujo, Aves Libre and many 
other interesting things. Yes, there already is a version of Lagrange for 
Android, so I can browse Gemini while on Gemini. But why did I dig it up in 
the first place?

Well, the answer is simple. I&#x27;d even say, eSIMple. This device was the first
or the second one in the world including a eSIM module (whose IMEI, I 
reckon, also is editable via the root shell AT commands), and here, this 
module is configured via a separate &quot;eSIM Wallet&quot; application with quite a 
lot of config, including (I think) IMEI/TAC spoofing on the module level, 
which is even better. And I also found a bunch of providers out there who 
offer global eSIMs anonymously for cryptocurrency, including Monero. Of 
course, usually they are data-only, but Gemini isn&#x27;t much suited for voice 
calls anyway, I guess, so that&#x27;s fine for me.

Another reason is that Gemini is the best one to use Termux on. I mean,
nothing beats physical keyboard, even such a relatively small one. All the 
netbook/thin client scenarios I&#x27;ve been thinking about are much more usable 
there than on my main smartphone, Pixel 6. And speaking of Pixel 6, I&#x27;m 
still thinking on how to back everything up so that I could put GrapheneOS 
there and then restore all this properly.

In a nutshell, doing what I can with what I got. Well, as always.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-03-11-the-graphene-saga-part-1.txt</link>
<title>(2024-03-11) The Graphene Saga: part 1</title>
<description><![CDATA[<pre>
(2024-03-11) The Graphene Saga: part 1
--------------------------------------
&quot;Two things are infinite, the universe and human stupidity, and I am not yet
completely sure about the universe.&quot; — Albert Einstein

When it comes to modern touchscreen smartphones, you don&#x27;t choose the good
one by filtering out bad ones, you choose the least worst of them among the 
pile of total garbage. Sad but true. The Google Pixel line is one of such 
&quot;the least worst&quot; examples, because at least they don&#x27;t have any trojans 
preinstalled by the vendor besides trojans from Google itself, which, in 
turn, can be mitigated by installing some custom firmware these top-notch 
bricks usually have excellent support for.

This is, in essence, how I came to know about GrapheneOS. Its website brags
about security hardening, privacy features and curbing the Google-specific 
services to the same privilege level as the rest of the applications, and, 
to be honest, it&#x27;s easier to build a Google-less Android ecosystem if one 
keeps it this way. Needless to say, I was interested in such a system ever 
since I got myself a Pixel 6, but managed to find time and resources to 
install it only recently. And I was... disappointed.

No, I don&#x27;t give a shit about Google Wallet support at the moment (although
we&#x27;ll get to that later). But I do care about two things every 
privacy-oriented smartphone OS must offer IMO: rootability and IMEI editing. 
And, after lurking on Reddit and Graphene&#x27;s own forum, I was a bit surprised 
that its authors (or at least its spokespeople) have been strongly opposing 
both of these things. And they used very weak argumentation, to put it 
mildly, but we&#x27;ll get to that later too. Having thrown several rants on 
Nostr about this, I decided to act alone and fix what they won&#x27;t fix at 
least for my own installation.

Rooting process and subsequent OTA update patching are wonderfully covered by
the avbroot ([1]) guide, and I recommend everyone wanting to root their 
GrapheneOS installation to read it very carefully. The only caveat I need to 
warn about is that you need to reboot from fastbootd back to normal fastboot 
mode before running fastboot erase avb_custom_key and subsequent fastboot 
flash avb_custom_key commands. I wasn&#x27;t (and still ain&#x27;t) sure how 
GrapheneOS works with custom kernels, so I chose the Magisk route instead of 
KernelSU. But then, I came across a different problem: non-working Zygisk.

Yes, of course I want to use this Pixel for some corporate apps that are
behind Intune Company Portal, which is another M$&#x27;s privacy invading piece 
of crap, but they don&#x27;t work without it installed. And of course, it detects 
root. And, of course, Zygisk framework (which all root-hiding modules 
effectively use) doesn&#x27;t work specifically on Android 14 based GrapheneOS. 
And Magisk developers refused to accept the patches adding this support with 
even more stupid argumentation:

&gt; GrapheneOS hates root and any modification, No reason to be compatible with
operating systems that hate us.

&gt; ...if its built upon AOSP, they can avoid breaking magisk (because magisk
supports AOSP). if they really break magisk, that means they dont care about 
magisk. you should ask them to avoid breaking magisk (or submit their 
so-called security changes to AOSP), rather than begging here.

It&#x27;s hard for me to comment on anything of this. Seems FOSS developers really
have forgotten why they do FOSS. In their quarrels, the only one who really 
suffers is the end user. Or are they probably afraid that someone is going 
to have a degoogled AND rooted AND hostile work environment ready Android 
flavour to just enjoy their devices while being in full control of them? 

I tried manually applying those patches myself and spent 3 hours (no kidding)
building Magisk. It built and ran (after I repatched my OTA, of course) but 
Zygisk still didn&#x27;t work (and the modules I run don&#x27;t to jack shit with 
ZygiskNext). Maybe I&#x27;ll spend some time later to figure this out but, for 
the time being, I decided to focus on the next topic: IMEIs. And I don&#x27;t 
even know where to start telling you about them on this Pixel 6.

Well, how about I start by providing several quotations on what GrapheneOS
spokespeople tell us about IMEI changing, the field I&#x27;ve been in for good 5 
years already?

&gt; Spoofing the IMEI shown in the OS is not changing the IMEI used for
cellular, and is not useful beyond fooling yourself. Changing the IMEI used 
for cellular is not possible with modern cellular radios, which are required 
by regulations to prevent changing it. Changing it requires a cellular radio 
exploit, which would imply not patching the radio or using a device with an 
insecure cellular radio.

&gt; Despite articles and guides online it is not possible to truly change your
IMEI. It is baked into the baseband processor and any changing of it in an 
Android perspective is just spoofing the value in the operating system so 
system-level applications would see a different number. When using a 
cellular network the original IMEI would still broadcast out.

&gt; This thread is completely filled with misinformation and will have to be
removed. It&#x27;s not clear why people don&#x27;t listen to our official responses 
explaining that changing IMEI does not mean identifiers are not provided to 
the cellular network and other things we&#x27;ve explained here.

And then they blocked the threads and accused everyone else of &quot;spreading
misinformation&quot;, uh-huh. When they were the only ones actually spreading it. 
No, I don&#x27;t rule out the possibility that they genuinely believe in what 
they say (i.e. that the IMEI sent to the network is still burned into some 
OTP areas like it&#x27;s still 2007 out there) but they provide absolutely no 
proof to their claims. Oh, well. Regulations. I know how they &quot;work&quot;. I know 
that certification process doesn&#x27;t check whether the IMEI actually is 
writeable or not. What really happens afterwards (if the certification 
process even gets to this point, which I suspect it often doesn&#x27;t, they are 
usually only interested in proper radio frequency ranges and output power) 
is that the vendor demonstrates that there&#x27;s no way to edit those 
identifiers in the stock production OS config. That&#x27;s it.

I don&#x27;t work in the telecom industry at all but I know all of this. They
supposedly work on a large enough telecom project and believe in whatever 
Big Brother feeds into their brains. And yes, I do happen to have a cellular 
carrier who displays the model of the phone on the personal account page. So 
I can easily check which IMEI the network really sees. This is, in fact, my 
only source of truth, not what&#x27;s displayed in *#06# etc. So, this was the 
moment I decided to prove that the IMEIs seen by the network are indeed 
editable in Pixel 6 and to publicly demonstrate how wrong these Graphene 
forum warriors are.

Now, here&#x27;s the real kicker: as soon as Pixels switched from Qualcomm to
Tensor chipsets, they also had to switch the baseband module. Guess what 
they chose? Exynos! Ta-da! To be more precise, Pixel 6 has Exynos 5123, but 
it doesn&#x27;t quite matter to me, I had never explored any of them before. So, 
from the bits and pieces, I collected three important facts: 1) IMEIs are 
stored in /mnt/vendor/efs/nv_protected.bin (and the .md5 file is salted so 
no way of restoring it manually), 2) AT commands are sent by forwarding them 
to /dev/umts_router, 3) there are AT+GOOGSETNV and AT+GOOGGETNV commands 
with some already known syntax and examples on GitHub. That&#x27;s friggin&#x27; it. 
Everything else was to be found out without any external help.

Of course, first I decided to dump everything found under /mnt/vendor, and I
noticed it also contained the bare modem firmware binary image. Luckily, it 
didn&#x27;t contain any compression or obfuscation, so I could search for some AT 
commands and NV item names. And indeed, three of them (yes, three) turned 
out to be related to IMEIs: CAL.Common.Imei, CAL.Common.Imei_2nd and 
DS_CAL.Common.Imei. The third one, I guess, was supposed to be used if the 
device has the second physical SIM slot in addition to eSIM, but of course, 
this is never the case for Pixel 6. Interestingly, the IMEI value for this 
non-existing slot matches the old Nokia 6310. A very unexpected tribute.

Anyway, I found out how to write IMEIs to the &quot;live&quot; EFS (mapped to that
nv_protected.bin file, and the .md5 is updated automatically, so no need to 
worry about it). The IMEI format is byte by byte BCD, normal order, 
zero-padded at the last byte (after the check digit). You must write 9 bytes 
(from 0 to 8), the last one being always 00. E.g. the 
AT+GOOGSETNV=&quot;CAL.Common.Imei&quot;,0,&quot;35&quot; command writes the first two digits of 
IMEI1 (35) to the first byte, and AT+GOOGSETNV=&quot;CAL.Common.Imei_2nd&quot;,5,&quot;82&quot; 
writes the 6th digit pair 82 to IMEI2. You get the idea. I even employed a 
newly found AT+GOOGFLUSHNV command to make sure the items are written. 
However, it wouldn&#x27;t be a flagship smartphone if everything were that 
simple. These values get overwritten upon reboot (or AT+GOOGNVRESET, for 
that matter).

Where are they overwritten from? I really don&#x27;t know yet. Not
/mnt/vendor/efs_backup for sure. Not other (nv_normal.bin) EFS values. Not 
the metadata partition. Probably the devinfo partition, but it can&#x27;t control 
everything, can it? This is something I really need to research further. 
However, for the time being, I decided to check whether the new IMEIs could 
be seen by the network without rebooting. For that, I remembered the old 
good AT+CFUN command. To my surprise, only AT+CFUN=4,1 actually did seem to 
work: 4 means cellular flight mode, 1, as I found out elsewhere, means 
&quot;modem state save&quot; or something like this. Well, state save was exactly what 
I needed. And then I turned the radio back on with AT+CFUN=1. And guess 
what... It worked! My carrier did show my active phone as Nokia 6310 in the 
account page. Well, who&#x27;s a fool now?

Of course, when trying to automate all this, I went through a lot of trial
and error and it still doesn&#x27;t seem very stable (I really need to switch to 
a more reliable way of interacting with /dev/umts_router than just echo and 
cat), but a general approach to getting temporary IMEIs (that is, the ones 
working until reboot) is quite clear for the time being. I&#x27;m going to post 
some work-in-progress documentation about what I have found so far. In the 
LuxDocs section, of course. But the quest for persistent IMEI editing on 
these devices has just begun.

--- Luxferre ---

[1]: https://github.com/chenxiaolong/avbroot
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-03-18-the-graphene-saga-part-2.txt</link>
<title>(2024-03-18) The Graphene Saga: part 2</title>
<description><![CDATA[<pre>
(2024-03-18) The Graphene Saga: part 2
--------------------------------------
...and probably the last.

I got tired of this circus pretty quickly. I have published whatever I&#x27;ve
found so far in the LuxDocs section, but... I really need a second Pixel 
device to keep going with this research. Because I still depend upon several 
applications that are incompatible with root, and GrapheneOS does nothing to 
help me with masking the root, on the contrary, making things much harder. 
In fact, one of Magisk modules broke the boot partition, so I had to 
reinstall the vanilla Graphene from scratch.

However, as I hinted before, the main problems of the project are not
technical, they are human. The devs don&#x27;t understand not everyone can afford 
losing ~1GB of traffic for OTA updates twice a week, and then additional 10 
minutes of time for the &quot;app optimization&quot; process. The devs don&#x27;t 
understand not all users are that stupid to not allow them root access, at 
least with ADB. The devs don&#x27;t understand how modern OEM manufacturing 
really works and &quot;regulations&quot; don&#x27;t. They seem to be living in a fantasy 
world where people only need a &quot;more secure stock&quot; without actually giving 
back control over their own devices to them, only making the permission 
model more granular and protecting against the exploits no one will ever be 
able to execute in the real world.

The sad part is, there doesn&#x27;t seem to be anything better at the moment.
DivestOS lags behind by a major version but essentially suffers from the 
same issues. CalyxOS is too opinionated and endorsing some dubious things 
like WhatsApp, Signal and Cloudflare, and also repeats the same silly mantra 
as Graphene and Divest (&quot;Running any Android device with root permissions 
severely undermines the security of the device&quot;). LineageOS is probably the 
freest of them all (when speaking of Pixel 6) but lacks all the security 
advantages of all the above three. I plan on trying it out on the Mi 8 Pro 
though (because anything is much better than the stock MIUI spyware), but it 
can only happen in two weeks. And _if_ I manage to get an ADB root not 
visible by the rest of the OS there, then I&#x27;ll consider moving from 
GrapheneOS to Lineage on the Pixel as well. And then I&#x27;ll be able to 
continue the research on my main subject.

The main subject, as you might have seen in LuxDocs, is now stalled at the
stage of finding where the IMEI SHA checksums are stored. Because the IMEIs 
themselves are stored in the devinfo partition in the plain ASCII form 
(although the partition itself is binary), and this partition, contrary to 
my expectations, really controls everything over the EFS. Of course, if 
either IMEI doesn&#x27;t match its checksum, the device reports both of them as 
000000000000000 to both the OS userspace and the network. And I could 
partially do this search in the offline mode as I dumped the modem firmware 
image along with everything EFS-related while I still had the root access. 
But, of course, I should have dumped everything I could.

Moral of the story: technical superiority isn&#x27;t everything. Human
understanding of what really matters is much more important.

This week is going to be quite tough but I really hope I get rewarded on the
end of it. So I&#x27;ll definitely have something interesting to write about next 
time.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-03-25-i-need-to-take-a-break.txt</link>
<title>(2024-03-25) I need to take a break (for two weeks)</title>
<description><![CDATA[<pre>
(2024-03-25) I need to take a break (for two weeks)
---------------------------------------------------
Recently, the concentration of stressful situations in my life drastically
increased. I feel too emotionally exhausted to be able to write anything 
constructive on Gopher, Nostr or elsewhere. But I know this is temporary. So 
expect a new post here on April 8.

It&#x27;s hard for me to plan any upcoming activities for this time, but I at
least hope to deal with two Xiaomis: Mi 8 Pro and Qin 1s. The former is 
gonna be equipped with a suitable LineageOS version, the latter will be 
researched to be able to replace the firmware or, in case it&#x27;s not possible 
without Faildows, at least it will be equipped with a suitable SIP client 
with ZRTP support. I&#x27;m also going to try and make use of some phones with 
remote Bluetooth dialer functionality and find (or even create) a service to 
pair them with my SIP accounts on my Arch Linux PC or my Orange Pi Zero 
(with a BT adapter). If it works, as a result, I&#x27;ll have a nice home-office 
phone and definitely will make a post about it.

For the time being though, silence is also meant to say something. Don&#x27;t
worry. I&#x27;m here. Just need to rest.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-04-08-a-few-more-words-about-dos.txt</link>
<title>(2024-04-08) A few more words about DOS</title>
<description><![CDATA[<pre>
(2024-04-08) A few more words about DOS
---------------------------------------
I&#x27;m back. And I still dislike DOS, its backslashes and interrupts. However, I
still consider it quite underrated nowadays, especially the flavors like 
FreeDOS and SvarDOS (which is based upon FreeDOS, in case you didn&#x27;t know).

Why? Because, well... Yes, DOS looks, feels and pretty much *is* ugly. The
thing is, it&#x27;s not its own fault, but rather of the underlying hardware 
architecture. Any DOS is ugly solely because x86 itself is. Period. DOS, as 
a kernel, just offers some absolutely minimal abstraction over this complete 
trainwreck. It showcases all the imperfections of x86 and clearly 
demonstrates how much more complexity is required to fully abstract away 
from them. Through this, DOS lets us fully understand where the root of all 
evil in modern computing resides, and how &quot;more modern&quot; began to 
automatically mean &quot;more complex&quot;. If you still don&#x27;t understand what I&#x27;m 
talking about, just look at the distribution sizes for some programs, 
interpreters and compilers backported to DOS from other OSes. If these sizes 
are bigger than for e.g. Linux, that&#x27;s how much complexity Linux kernel and 
system libraries hide from you.

On a flip side though, for those who still have to stick to x86, DOS is one
of the last safe havens if you&#x27;re serious about retaining control over 
things at least to some extent. Because, of course, we lost the battle long 
ago when it comes to hardware (no one can solder a modern PC the way we 
could solder a Spectrum clone anymore), but, with DOS, we still can be as 
close to that hardware as possible and not let various corporations with 
dubious intentions decide for us. As a bonus of reduced complexity, we 
automatically get reduced energy consumption (i.e. less consumed power and 
better battery life) and ability to better understand WTF is going on if 
anything doesn&#x27;t work as expected. To be honest, I really have been missing 
this feeling even with Linux and OpenBSD, so I started looking at 
possibilities to daily-drive some DOS whenever I could.

This is how I found SvarDOS ([1]), a FreeDOS derivative that has an Arch-like
package system. Its main maintainer, Mateusz Viste, has developed a lot of 
other interesting DOS software. He also has a Gopherhole at gopher.viste.fr, 
and I highly recommend visiting it. Despite being a derivative, SvarDOS 
doesn&#x27;t have 100% of the processes identical to FreeDOS, so reading its own 
documentation is mandatory. And the documentation format, AMB (Ancient 
Machine Book), is just wonderful, I plan on writing my own set of tools to 
work with it in a cross-platform way. The SVP package format is also simple 
enough, and the repo contains much more useful software than FreeDOS itself 
can offer. It even sparked my interest in learning the Rexx scripting 
language, as the Regina interpreter has a DOS port too. I also was really 
surprised to see the things like Lua and SQLite3 there, and, of course, the 
Links browser that has fully working graphics (with -g option) and even 
modern SSL support. Among the software not present in the SvarDOS repo, I 
can highlight a huge official Vim 7.3 port and this ([2]) SSH2DOS version 
that is pretty sufficient to turn any DOS box into a full-featured thin 
client. Of course, if you have the necessary packet drivers.

And this is pretty much the only obstacle I have: drivers. So far, I&#x27;ve been
testing SvarDOS in VirtualBox only, and, as you might have guessed, all 
networking is in place there. Whenever I find a suitable real PC to run it 
on (otherwise there won&#x27;t be any fun), I may face the problem of not having 
even Ethernet drivers available for DOS. And WLAN, AFAIK, is pretty much a 
no-go with the current-gen chips, but I do have a spare Ethernet port on my 
home mesh node in the room. Anyway, it&#x27;s worth giving it a try, maybe I&#x27;ll 
find out even more about the hardware itself. And if it works, well, I&#x27;ll 
make sure to put a good use to it.

The black abyss of DOS is calling to escape from the dystopia of complexity.
Are you ready for it?

--- Luxferre ---

[1]: http://svardos.org/
[2]: https://github.com/AnttiTakala/SSH2DOS
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-04-15-no-news-sorry.txt</link>
<title>(2024-08-15) No news, sorry</title>
<description><![CDATA[<pre>
(2024-08-15) No news, sorry
---------------------------
I&#x27;ve lost the file with the text I was about to post, so scrap it. Will
prepare something better for the next week.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-04-22-i-refreshed-my-watch-requirements.txt</link>
<title>(2024-04-22) I updated my baseline watch requirements, and they sadden me</title>
<description><![CDATA[<pre>
(2024-04-22) I updated my baseline watch requirements, and they sadden me
-------------------------------------------------------------------------
My collections keep refreshing slowly but surely: all of them are going to
consist of less items but of better quality ones. It started with the 
purchase of a Lamy 2000 EF fountain pen, full-steel version of it, to be 
exact. But FPs are the topic I don&#x27;t have any strict requirements about at 
the moment. Moreover, I have determined my personal top-3 (Lamy 2000 SS, 
Hongdian Black Forest Max, Pilot MR) and don&#x27;t feel like anything could 
shake it. Cellphones and wristwatches, on the other hand, are such topics. 
And, as I concluded that all hope has been lost about cellphones, my 
collection of them keeps diminishing. For watches though... Let&#x27;s see.

No, I do love pretty much every watch I still have in the collection. But if
I want to continue with the hobby, it&#x27;s time for my criteria for new entries 
to be updated. I&#x27;ve had experimented enough with different tech and 
materials to shape a solid baseline of what I really want as of now, to be 
practically usable on my own wrist yet still collection-worthy. So, here&#x27;s 
the list of my current baseline criteria.

1. Case diameter, mm: 36..40 (38 or 39 preferred).
2. Case thickness, mm: &lt;=13.
3. Lug width, mm: 18 or 20 (16 or 22 are viewed individually).
4. Lug to lug distance, mm: 48 max, 46 and below recommended.
5. Case material: titanium (scratch-resistant coating recommended).
6. Water resistance rating: &gt;=10 bar (&gt;=100m).
7. Glass crystal material: sapphire (AR coating recommended).
8. Power: solar rechargeable OR 5+ years battery (10+ years recommended) OR
automatic winding.
9. If quartz, de-facto accuracy must be within ±5 s/mo OR the watch must have
a longwave/GPS/Bluetooth time sync system.
10. If automatic, the stated accuracy must be within -15/+25 spd AND the
watch must have a straightforward way to regulate it at home.
11. If analogue, the watch must have distinct minute markers AND, if these
markers are flat on the dial surface, the minute hand must reach them.
12. Integrated bracelets are discouraged, integrated straps (that can&#x27;t be
replaced with any other third-party strap or bracelet) are prohibited.

Now, here&#x27;s the sad part: can you find at least ten different models (that
are currently in production) matching all these requirements?

Because I tried. I really tried. Found three so far. One of them is, of
course, a crazy expensive Casio MRG-B5000D, another one is a JDM Citizen 
Promaster (PMD56, the requirement #11 doesn&#x27;t apply as the markers are on 
the inner ring), and the third one is a titanium version of Certina DS 
Caimano (C035.410.44.087.00). That&#x27;s pretty much it. I can&#x27;t find more as of 
yet. I remember we had much more choice. What&#x27;s happening? Is there some 
titanium shortage on the global market or did the marketologists decide for 
us that we don&#x27;t need long-lasting AND lightweight watches anymore? Did the 
Swiss and the Japanese have a conspiracy to not deliver an ideal wristwatch 
ever, and instead feed us with compromises on one aspect or another? I just 
got the aforementioned Certina (maybe will compose a post about it someday) 
and probably gonna get the Promaster too, but the choice became too narrow 
compared to the amount of mediocrity the watch industry has been pumping out 
to this very day. Even the companies I highly respect, like Casio and Epson 
(Orient), mostly gave up on titanium in favor of stainless steel or even 
brass (for the lowest-end models). Seiko abandoned almost all of its 
titanium watch production as well. Citizen still holds to some extent but 
most of its titanium models fail my other criteria pretty hard. And the 
Swiss... again, Certina is one of their few brands even worth starting to 
look at if you want a real watch that can keep time, not a fashion item.

To be honest, I really don&#x27;t know what to do. Maybe I should just wait until
the trend for bulky wristpucks wears off (pun intended). Maybe I should just 
abandon the hobby altogether, at least until something _really_ interesting 
comes up. After all, I probably already have enough time measurement tools 
for the rest of my life. And the Citizen PMD56 looks like a perfect point to 
make this stop. But the hope that it won&#x27;t be a full stop still remains. As 
I already said, I&#x27;m a dreamer, sort of.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-04-29-when-an-interesting-watch-is-boring.txt</link>
<title>(2024-04-29) When an interesting watch is boring... as it should be</title>
<description><![CDATA[<pre>
(2024-04-29) When an interesting watch is boring... as it should be
-------------------------------------------------------------------
It might look like I&#x27;m breaking my oath by writing about a Swiss made
wristwatch in a place where non-Swiss wristwatches are usually talked about. 
Well, considering I&#x27;m about to exit the hobby soon, I think this might be an 
acceptable exception. So far, I&#x27;ve been using this watch for a week but this 
is exactly the case when you know almost everything about it from the week 
of usage, and the only things you can learn over time is how well it holds 
to its claims about accuracy and battery life. Of course, I&#x27;m talking about 
my current choice, the Certina C035.410.44.087.00 aka DS Caimano 39 Titanium.

Well... What&#x27;s so interesting about it then? First, it fulfills all of my
requirements stated in my previous phlog post with its 39mm diameter, &lt;8mm 
thickness, 20mm lug width, &lt;44 mm lug to lug distance (although this isn&#x27;t 
documented anywhere), all-titanium casing and bracelet, sapphire crystal 
with AR coating, 10 bar WR rating, minute hand reaching the minute markers 
and, finally, a thermocompensated quartz movement I&#x27;m gonna talk about a bit 
later. Second, this is the first watch in a long time where I don&#x27;t want to 
replace the stock bracelet with something else, particularly because it&#x27;s 
also thin, lightweight and made of titanium. It&#x27;s a jubilee-type bracelet 
with solid links (including end links), dual finishing (the inner links are 
polished and the outer are matte titanium) and an internal two-sided 
butterfly-type clasp that totally disappears under the bracelet itself in 
the closed state, leaving only the pushers visible. And the total weight of 
the watch is quite featherlight for what it is: 67.4 grams after shortening 
the bracelet to my wrist size. Which, for the record, is only about 12 to 14 
grams heavier than my all-resin 5600-series G-Shocks and still virtually 
disappears on my wrist.

The only downside to this bracelet is that it, due to its construction, has
no micro-adjustments: I have gotten away with it by removing one link from 
the upper side and two from the lower, but I understand how this might be a 
problem for some people. And speaking of downsides of the watch itself, I 
have found only one so far: lume. It is present on both hands and hour 
markers, but in miniscule quantities. I also predict the press-on caseback 
might be a problem for me when the time comes to replace the battery. I&#x27;ve 
been a strong proponent of screw-on casebacks for that very reason, and have 
some examples of rather thinner watches that successfully implement them. 
But we&#x27;ll see. Of course, this watch ideally should have solar charging so 
that the battery replacement would never be required (I mean, CTL920/1616 
replacement might be required every 20 to 30 years but that&#x27;s a lot of time) 
but that&#x27;s the Swiss we&#x27;re talking about, they will never implement such a 
killer feature in addition to everything else for this price. For the same 
reason, we won&#x27;t find an autocalendar feature here and we&#x27;ll have to move 
the date manually if the month doesn&#x27;t have 31 days.

Now, let&#x27;s talk about the real killer feature of this watch: the ETA
PreciDrive series movement. Particularly, F05.411, which is only known to be 
used in the DS Caimano models, otherwise it&#x27;s all but extinct by now as it 
has been replaced with a &quot;superior&quot; F05.412. Little is known about the 
movement itself, other than the fact that it 1) is thermocompensated and 
promises to keep time within 10 seconds per year deviation if the ambient 
temperature is within 20..30 degrees Celsius, 2) uses a Renata 371 battery 
which corresponds to the SR920SW standard marking, and the movement is 
supposed to work for about 5 years on this battery and indicate when the 
charge is low with the second hand skipping 4-second intervals. By the way, 
when setting this watch, the hands are moving tighter than usual, but the 
same precautions apply: first you move the minute hand a bit (~10 min) ahead 
and then slowly adjust it back to the exact minute position. Otherwise 
you&#x27;ll find the minute hand lagging behind the second hand, which is 
extremely annoying. On the positive side, this movement has very distinct 
date and time setting crown positions, so you can pull the crown advance the 
date without being afraid of stopping the hands and ruining the timekeeping 
accuracy.

Neither of these things would seem out of the ordinary, save for one simple
detail: I got this watch for about US $490. This is a lot for a usual quartz 
three-hander with a date, this is more than average for a titanium quartz 
three-hander with a date, but this is dirt cheap for a titanium HAQ 
three-hander with a date. Yes, 10 seconds per year deviation without any 
synchronization is considered HAQ (high accuracy quartz), and if you step 
into this territory, you won&#x27;t find any other offerings for such a price or 
even twice that, if considering the same case material. It&#x27;s a totally 
different money and a totally different level of availability to general 
public. And I don&#x27;t even know whether or not the discontinued Longines VHP 
line had anything made of titanium, or any current Tissots or whatever using 
F05.412 have anything made of it, or if there were any titanium Citizen 
Chronomasters or 9F-based Grand Seikos, but as of now, I have a strong 
suspicion I&#x27;m wearing the least expensive titanium HAQ watch in the world. 
This alone IMO makes it worth mentioning in this phlog, putting into my 
collection and into my personal &quot;top 10 watches of all time&quot; list, despite 
it being non-Japaneseand the dial having a minor but obvious design blunder 
around the minutes 28 to 32 where the markers are replaced with a &quot;SWISS 
MADE&quot; writing.

Other than that, this Certina is pretty boring and looks like a typical Swiss
watch from the dressier side of the spectrum. But I think this is how it 
should be: interesting inside, boring outside. There&#x27;s something about the 
feeling when only you, as the owner, know what your watch really can do. 
Like, it impresses me that it still maintains 10 bar WR with such a look and 
a press-on caseback. Unfortunately, this, as I already said, is an exception 
to the overall &quot;quantity over quality&quot; trend, but not without its own 
compromises either: looks like the world manufacturers really aren&#x27;t 
interested in producing a perfect wristwatch, otherwise they wouldn&#x27;t have 
anything new to sell afterwards because no one would need anything new if 
what they already have was perfect. 

Sure, one cannot exclude such a conspiracy in these crazy days, but there
might be another reason. Probably good watches also became a rarely seen 
phenomenon because less and less people still use any wristwatches for their 
only intended purpose and view them solely as fashion items rather than 
tools they should be, and the market responds accordingly. The mere fact 
that such a term as &quot;GADA watch&quot; (GADA means &quot;go anywhere, do anything&quot;) 
appeared in the English-speaking community already means that people started 
to admit that most watches are naturally non-GADA, i.e. created to blindly 
increase consumption by fitting one watches for one situations and others 
for others. It&#x27;s hard to overestimate the level of conceptual absurdity of 
making an accessory out of a tool. Imagine buying various phones for various 
occasions nowadays. How does a wristwatch fundamentally differ? Those who 
view the time on their phones, by the way, shouldn&#x27;t have a right to vote on 
such questions as they represent a pre-wristwatch era of pocket watches, 
only now they are much bulkier, less reliable and allow their manufacturers, 
governments and various third parties to track their owners.

This is why I&#x27;m going to end my collection with a watch that is undoubtedly
GADA, totally autonomous and will surely enter my top 3, not just top 10 
like this DS Caimano. The only problem is that I&#x27;m still not sure when I&#x27;m 
gonna get it (hopefully within this week), but I&#x27;ll definitely let you know 
about this. For the time being though, I really like this Certina too and 
hope it will live up to the manufacturer&#x27;s promises about its accuracy and 
battery life.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-05-06-on-tablet-pcs.txt</link>
<title>(2024-05-06) On tablet PCs</title>
<description><![CDATA[<pre>
(2024-05-06) On tablet PCs
--------------------------
Throughout my life, I&#x27;ve had five tablet PCs in total. The most recent one is
a Google Pixel Tablet (8/128GB, JP version) bought last month, but I&#x27;ll get 
to that in a bit. While the smartphone topic is still a bit controversial 
(not for me but for the general public), tablet PCs are nowadays viewed by 
most people as peak consumerism, and that opinion is pretty much unanimous. 
In other words, the public says that&#x27;s a kind of device you never *really* 
need. Which is hilarious, because I&#x27;ve lived long enough to remember current 
smartphone screen diagonals (6+ inches) to be considered &quot;tablet PC sizes&quot; 
as well. Heck, I even remember how 5.5 inch devices were called tablets. And 
I did understand back then that it already was all about marketing, and the 
only things separating a tablet from a touchscreen-based smartphone were the 
physical size and, optionally, lack of a cellular module. Although I do 
remember some blonde chicks proudly putting their 7-inch Galaxy Tabs to 
their ears and talking like that in public. That&#x27;s something you never 
forget: once you see it, you can&#x27;t unsee it.

So, back to Pixel Tablet. Technichally, it is a Pixel 7a without a cellular
module and in the tablet form factor. The lack of a cellular module is IMO a 
good thing in this case, but still, precautions needed to be taken and I did 
install GrapheneOS on it on day one. It comes budled with a magnetic 
charging dock stand doubling as a speaker: because of this contraption, all 
official and almost all third-party cases have a huge rounded hole in their 
backs. And the dock itself is powered by a &quot;universal&quot; AC adapter (although 
only suitable for US-type sockets, so I needed to find an adapter for this 
adapter) through a rather non-standard plug (whatever happened to the 
worldwide USB-C adoption). Luckily, this isn&#x27;t the only way to charge the 
tablet and the device itself is running off USB-C... and that&#x27;s the only 
socket in it. No 3.5mm minijack. And, just like with Pixel smartphones, you 
can&#x27;t use a normal headphone adapter either: it needs to have an active DAC 
inside. I happen to have none of those, so, for the time being, I have to 
use Bluetooth headsets with this new acquisition.

Why did I switch to it from the Nokia T20 I&#x27;ve been using almost all the time
ever since moving to the village? Well, the first and simplest reason was 
that I want to return to retro and indie gaming, at least to the smallest 
extent. There are some games where 3GB RAM is simply not enough, especially 
if you want to directly livestream the gameplay and still be mobile (in a 
sense of not being attached to the desk where the PC is standing). The 
second reason was the lack of physical storage space on the T20 (at least 
for my purposes) and of the overall control over the OS (although it still 
is much cleaner than everywhere else that doesn&#x27;t come straight from 
Google). Yes, I ultimately wanted to switch to Graphene or something similar 
on every Android device actively used by me. And the T20 went to the dad 
whose Lenovo TB-X606X had been already struggling with some firmware issues 
and, of course, no official updates or custom firmware to fix those issues.

Of course, gaming/media station is my primary but not the only use case for
this tablet. Sometimes I also use it as a makeshift laptop replacement when 
paired with my wireless trackball and, even more often, a wireless keyboard 
(ironically made by Apple) and running Termux there. With Segfault&#x27;s remote 
shells, it&#x27;s a bliss as you can continue on one device from where you left 
on another, although, of course, your own VPS is better for this kind of 
stuff. When not using a physical keyboard, by the way, I found Thumb-Key to 
be most comfortable and useful there, and it&#x27;s also fully FOSS and 
privacy-friendly. For the record, on touch-only smartphones, I fully 
switched to the Unexpected Keyboard, which also is FOSS, privacy-friendly 
and extremely customizable out of the box. Moreover, with the lifestyle I&#x27;m 
currently forced to live, I started viewing my Pixel 6 as a smaller version 
of the same tablet rather than considering the tablet a larger companion to 
the smartphone. And both devices running the same up-to-date GrapheneOS only 
helped this vision.

I might be bad at storytelling but what I&#x27;m trying to say is there really are
little boundaries between different device types if you really know what 
you&#x27;re doing. Contrary to the popular belief that only some of them are 
suitable for content creation and others are only meant for content 
consumption, whether or not you put the PC part into the notion of &quot;tablet 
PC&quot; is entirely up to you. Especially if the manufacturer doesn&#x27;t get in the 
way of you owning your own device. Of course, no Android is ideal and pure 
Linux distros, like those in Librems or PinePhones, would be much better, 
but, for running *some* proprietary shit along with FOSS if you have to and 
for the &quot;doing what I can with what I&#x27;ve got&quot; approach, a Pixel + 
GrapheneOS/LineageOS (depending on whether or not you want trouble-free 
rooting as a bonus) looks like a perfect combination as of now. And I hope 
it lasts even longer than the previous one.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-05-13-the-grand-finale-watch.txt</link>
<title>(2024-05-13) The grand finale watch</title>
<description><![CDATA[<pre>
(2024-05-13) The grand finale watch
-----------------------------------
If I, purely hypothetically, had to lose my entire watch collection and only
remain with a single watch on my wrist, which one would it better be? This 
question had been haunting me for some time and led to a point in my life 
where I decided to get a timepiece that would finally end it all, at least 
for as long as I live where I&#x27;m living right now.

Exiting a hobby is always hard. In my case, it was even harder as the
particular watch that I wanted to buy as the last one had been eluding me 
for several weeks. The quest of obtaining this watch started on April 19 and 
finally came to an end on May 7 when I got my package 5 minutes before a 
thunderstorm started. The seller was quite shady as he had postponed the 
delivery date for three times and insisted on an off-the-record deal to 
avoid the platform fee for himself. This was enough for me, in turn, to 
insist on pay-on-delivery even though I had to pay some extra for it from my 
end. I decided to suffer through this deal for two reasons: 1) no one else 
offered this watch in my country of residence (and the abroad logistics is 
quite complicated here with our crazy import tax), 2) this is literally my 
last watch purchase in the foreseeable future. So, if you&#x27;ve been reading my 
most recent posts, you probably already know which watch I&#x27;m referring to 
right now: Citizen PMD56-2951 from the JDM Promaster Land series.

Technically, it might not be as advanced as some of my other watches in the
collection. While having full auto calendar, hour offset and hand 
calibration functions, it lacks any Bluetooth, GPS or optical sync features, 
and, as it is indeed a Japanese domestic model, its longwave receiver only 
supports JJY stations whose signal most of the world can&#x27;t receive, but more 
on that later. This watch, as well as pretty much any other one in the same 
lineup, was designed to do one thing and do it well, and do it under any 
circumstances. In fact, everything there is streamlined for maximum 
efficiency: huge hands, digits and markers (with a huge amount of lume 
that&#x27;s visible for at least 8 hours of total darkness), a crown that&#x27;s 
extremely easy to grip, anti-reflective coating on the inside of the 
sapphire crystal and an on-the-fly 3-step micro-adjustment system on the 
bracelet. After the Certina DS Caimano Titanium with its almost non-existent 
lume, thin hands, no weekday display and no micro-adjustments on the 
bracelet, the contrast it makes cannot be overstated. You might argue that 
the DS Caimano is a &quot;dress watch&quot; and the Promaster is a &quot;field watch&quot; but 
again, real use cases are not defined by market positioning and it&#x27;s hard 
for me to justify anything that&#x27;s not GADA by default. Besides, I have 
enough &quot;dress watches&quot; left even after shrinking my collection in half, so I 
don&#x27;t feel any need to fill that particular niche, especially when having 
zero occasions throughout the year where I&#x27;d want to specifically wear them.

Now, functionality-wise, there&#x27;s surprisingly a lot to cover for this
particular Citizen, even though I already have one that performs almost 
exactly like it — AS2050-10E — with the obvious differences of it being 
DCF77-only and having no weekday display. For instance, the weekday display 
in PMD56 is bilingual, the second language obviously being Japanese (kanji). 
The process of manually setting the watch is not that complicated: even 
though one might have to resort to poorly autotranslated manuals for the 
H100 movement, it basically is the same as for the AS2050 whose movement has 
manuals written in most European languages. When really necessary, one can 
quickly figure it out. At least, for almost a week of usage, I haven&#x27;t had 
any problems with operating this watch whatsoever, and with hands alignment 
either — even the second hand hits every marker, although it doesn&#x27;t reach 
them, but I can forgive this here because they are placed onto a non-flat 
inset around the dial rather than the dial itself. As for the accuracy 
between synchronizations, the H100 movement is said to keep 15 seconds per 
month deviation at most but, from what I&#x27;ve already seen throughout 6 days, 
it&#x27;s gonna be closer to 3 seconds per month _at most_ for my particular 
watch. I&#x27;ll keep you updated in June about this.

For me though, the most important and not so obvious feature in both AS2050
and PMD56 is the hour offset setting, which is called &quot;time difference&quot; in 
European Citizen manuals: neither of these two watches officially support 
timezones (those that do also implement fractional zones like the ones for 
India and Nepal) but they allow to specify the offset from the reference 
radio signal they support in order to keep the correct time upon reception, 
as this reception can span across multiple regions. E.g. on AS2050-10E the 
reference point is the Central European time (UTC+1 or UTC+2 depending on 
whether it&#x27;s summer or not) observed in Germany, and on PMD56, the reference 
point is JST (UTC+9). So, as I&#x27;m located in a region where DCF77 still can 
be received but my time is an hour ahead, I set the time difference to +1 to 
not get in trouble with my AS2050, and I set the time difference to -6 on 
the PMD56 to match my summer time relative to JST, and now can use my own 
JJY emulator (or whatever I find) to sync proper JST time and still display 
my local one. And yes, this is the only purpose of this function, it has no 
use in the places where the longwave signal can&#x27;t be received or emulated, 
so I don&#x27;t know why people still think this is somehow related to timezones 
in any way and complain it doesn&#x27;t support the fractional offsets it was 
never meant for. If you use PMD56 in a place with a fractional timezone and 
want to emulate the signal, just set this offset to zero and make your 
emulator send whatever time you need. Speaking of which, I&#x27;m already in the 
middle of porting my own JJY.js (aka &quot;Fukushima&quot;) JJY40 sync library/webapp 
to Python 3. It transmits the signal on the third harmonic via the audio 
output. I&#x27;m going to dedicate a separate phlog post to it when it&#x27;s ready 
because longwave emission with non-radio circuits is an interesting topic on 
its own. 

Functionality, however, isn&#x27;t everything when it comes to a watch. This
Citizen just looks and feels premium, which might seem strange because it&#x27;s 
not even that expensive, basically cheaper than Casio GM-B2100BD and even 
the aforementioned GMW-B5000D (where it&#x27;s still being sold). Yet it&#x27;s built 
out of much better materials and with better finishing than those two. Not 
surprising, by the way, considering that the Promaster Eco-Drive lineup is a 
direct high-end G-Shock competitor within Japan. I&#x27;m saying &quot;Promaster 
Eco-Drive&quot; because there also are mechanical Promasters which are honestly 
beyond my interest as they tend to be much bulkier. Indeed, just like a 
G-Shock, PMD56 offers 20 bar WR rating (the screw-down crown facilitates 
that, as well as zero need for battery replacement for every 20 or even 40 
years) and a shock protection system for the movement. The difference is, it 
doesn&#x27;t compromise on the looks and uses the materials that only much more 
expensive G-Shocks are using: all-titanium casing, titanium bracelet and a 
_domed_ sapphire crystal. The dial is not purely green, but a gradient from 
field green under a direct bright light down to charcoal dark grey in the 
darker conditions, and I love that transition. The overall look is much more 
impressive on the wrist than on any photo you could possibly see around 
there. It looks so good that I even ditched the idea of getting a PMD56-2952 
as well, which has a pure-black dial but a yellow second hand and a black 
PVD-coated bezel as opposed to matte titanium here just like the rest of the 
watch case. The case itself, by the way, is quite compact, and the general 
diameter (without the crown) doesn&#x27;t exceed 40 mm for sure, and the 
lug-to-lug distance doesn&#x27;t exceed 45. Just what I needed.

So, where do I go from here? Is my interest in watches fading away after
getting my hands on this ultimate artifact? Heck no. I&#x27;m still interested in 
watch-related news, reviews and discussions, as well as supplementary topics 
like watch straps/bracelets or longwave/Bluetooth/optical synchronization. 
It&#x27;s just that my personal search for a perfect watch is over for now. There 
*might* be some &quot;evolutionary&quot; upgrades to my collection, e.g. if I also 
find a Casio MRG-B5000 for a reasonable price to replace my GMW-B5000 with a 
titanium counterpart, or a titanium Seiko chronograph to replace my 
SSB401P1, or a hypothetical titanium mech diver from Orient to replace my 
Kamasu, but all that is highly unlikely. And the main reason for this is 
because Citizen PMD56-2951 really answers the first question I&#x27;ve asked in 
this post: if I ever have to leave a single watch only, however I love the 
rest of my collection, this one will be the watch that&#x27;s going to stay.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-05-20-a-look-from-another-side-again.txt</link>
<title>(2024-05-20) A look from another side of the spectrum... again</title>
<description><![CDATA[<pre>
(2024-05-20) A look from another side of the spectrum... again
--------------------------------------------------------------
If you remember my post about sub-$100 sub-50g watches, I mentioned a handful
of them based on Miyota 2035 movements, like Bertucci A-1R and Casio 
MQ-24/MQ-71. However, I also mentioned some digital ones that could serve 
the same purpose while obviously offering much more functionality for the 
price *and* having rather longer battery life as well. And today, I&#x27;d like 
to talk about the model that usually goes under the radar but is 
nevertheless significant in several ways and, as such, isn&#x27;t going to leave 
my collection anytime soon. In terms of price, materials, design and 
functionality, it&#x27;s pretty much an antipode of my ultimate recent 
acquisition (Citizen PMD56-2951), but this is the case when the popular 
stereotype &quot;digital watches are bought for functionality, analogue watches 
are bought to please your soul&quot; gets busted again: this watch isn&#x27;t 
something to boast about, it&#x27;s something to please my soul, partially 
exactly because how feature-packed and retro-looking it is. Enter... Casio 
DB-36.

Yes, DB-36. Not 360. I didn&#x27;t need the chrome-y bling that would peel off
over time anyway because the case on the DB-360 still is plastic, I didn&#x27;t 
need the shiny buttons that would decrease the water resistance rating, and 
I didn&#x27;t need a crappy bracelet I&#x27;d have an urge to change ASAP. DB-36, on 
the other hand, looks as humble as it can, almost too cheap for what it&#x27;s 
capable of (nowhere as cheap as the Skmei knockoffs though). By the way, 
I&#x27;ve got rid of all the other databank-enabled Casios from my collection 
(AW-80, AMW-870, G-2900F and the one I only wore once, DBC-611) for various 
reasons, but this one, with its resin case, PUR strap and acrylic glass 
crystal, is a survivor. I know the strap is a weak point though so I&#x27;m 
probably gonna find a black 18mm mesh if I&#x27;m going to wear it more actively, 
but it&#x27;s no big deal if it breaks. And that&#x27;s a huge upside of this watch: 
even if something breaks, it&#x27;s no big deal. I remember a vlogger whining 
about how he broke his PMD56 (not really breaking it but ruining the WR 
protective seal), telling that, like, he sold his collection to get it and 
then it was broken (again, not really). Well, a $40 watch like DB-36 is much 
easier to mess around with than a $600 watch like PMD56. That is, if you 
ever need to.

And it turns out you normally don&#x27;t: like G-2900F and other Casio watches
developed in the first half of 2000s, DB-36 (as well as 360, it&#x27;s the same 
module 2515 inside) is powered by a CR2025 battery that allows it to run for 
over 10 years. As far as I know, the only other Databank model with the same 
battery is the calculator-featuring DBC-32. All other watches in the series, 
AFAIK, run on CR1616 at best. Curiously enough, old ana-digi Twincepts (ABX 
series) did run on CR2016 but, of course, that 10-year lifespan wasn&#x27;t 
guaranteed and was close to 5 years IRL. Besides occasional battery and 
strap changes, the only other piece of maintenance I can think of regarding 
DB-36 would be polishing the crystal if it picks too many scratches. Which 
is, again, much easier to do because it&#x27;s acrylic. For what it&#x27;s worth, this 
watch is meant for daily-driving and not showing off, but there&#x27;s even more 
to that. And I&#x27;ll return to this &quot;more&quot; after briefly going over its 
functionality.

And when it comes to functionality, I&#x27;m still impressed. I mean, I hadn&#x27;t
seen anything like this in any analogue, ana-digi or digital watch for &lt;=$40 
except this one. Yes, I&#x27;ve had an AW-80 which is close enough but it still 
is more expensive. First, you&#x27;ve got a selection of 13 languages to display 
the weekday and to determine the data entry characters. Since the module had 
been developed back in 2002, I won&#x27;t bash the language set too much. Then, 
you&#x27;ve got a databank itself, which, with its 30 records, functionally 
resembles the AW-80 Telememo but has a but different charset for both name 
and number parts, and while the name part still is 8 characters long, the 
number part is only 15 characters long, one less than in the AW-80. Then, 
you&#x27;ve got five independent alarms (as opposed to 3 in AW-80) with the 
ability to set them daily, monthly, for a specific date or for every day on 
a specific month. Hourly chime and a snooze mode for the AL1 included in the 
box. Then, you&#x27;ve got a rather typical stopwatch and a no less typical 
countdown timer, both supporting full 24-hour measurement. Finally, you&#x27;ve 
got a dual-time mode, which might *seem* like a downgrade from the AW-80&#x27;s 
world time mode but it&#x27;s much more practical as the Reverse button in the 
timekeeping mode serves as a quick-access button to the second time if you 
keep it pressed for 2 seconds. Conversely, by the way, the Forward button 
serves as a quick-access button to view the recently accessed databank entry.

I haven&#x27;t discussed any peculiarities yet but you might have guessed where
I&#x27;m heading now that&#x27;s the features are on the table. Stemming from the 
Casio&#x27;s lineage (no pun intended) of 1980s/1990s Databanks and having 
undergone some cost-driven simplification, DB-36 is a minimum viable product 
of a watch specifically designed for people WHO DON&#x27;T RELY UPON CELLPHONES 
AT ALL. There, I said it. And the fact that it even can be bought new in 
2024 speaks better than anything else: there still is some market for such 
watches. Indeed, some people can organize their lives around desktop 
computers, desktop phones (including various IP telephony), pen and paper 
and wristwatches. I even think most people can do that but they just don&#x27;t 
want to. I can resonate with that mindset myself in some security models 
when the cellphone is used in an extremely hostile environment where you 
must be prepared to lose it or erase all the data from it at any moment. In 
2024, a phone (as well as a paper notebook, for that matter) is much more 
likely to be searched or confiscated than a cheap Casio digital watch 
(unless it says F-91W, because, you know... stereotypes). If you store your 
information securely enough, such a watch could contain some breadcrumbs 
you, and only you, could recover it with.

Speaking of which, I&#x27;d also like to remind you about the Telememer ([1])
project. Yes, I plan to port it to a standalone CLI program too, probably in 
Python as it has somewhat nice big integer support, but for now it&#x27;s 
implemented as a library in BigInt-only JS. The idea of Telememer is simple: 
given the alphabet sets for both name and number parts, as well as their 
lengths, convert arbitrary binary data into a set of records for 
databank-enabled Casio watches, and, of course, be able to perform the 
reverse conversion and reconstruct the source data from these records. As 
the phonebooks in these watches are forcefully auto-sorted, the first 
character in the name determines the index position of the record, thus 
detracting from the amount of bits that can be stored this way. Yes, it 
doesn&#x27;t save you from the hassle of entering all the records manually and 
then retyping them into the program when you need to recover the data, but 
at least it allows you to store much more types of info than the plain 
Telememo records do. With DB-36, however, it gets even more interesting as 
different interface languages have different amount of input characters, 
ranging from 46 for English to whopping 60 for French. This partially 
compensates for the disadvantage of only having 15 characters available for 
the numeric part and only 14 &quot;digits&quot; available, making one record in French 
hold about |log2(14 ** 15)| + |log2(60 ** 7)| = 98 bits (considering the 
first letter is reserved for the index) with the entire set holding up to 
2940 bits or 367 bytes, which is 11 bytes lower than the theoretical limit 
for AW-80 but still impressive.

&quot;But wait, there&#x27;s more!&quot; If you really want to maximize your storage
capacity by fully sacrificing the convenience of data entry and having to 
switch the interface language every now and then, you can still do this! 
Omitting two sets of very similar letters, you have 119 distinct characters 
to operate on, and the above formula will give you... well... 105 bits per 
record and 3150 bits or 393 bytes per the entire set, which is more than for 
AW-80. I guess that&#x27;s the theoretical limit of how much you can squeeze out 
of the DB-36&#x27;s databank, but whether or not it&#x27;s worth all the entry trouble 
is up to you. For comparison, English-only entry will yield 95 bits per 
record and 2850 bits/356 bytes per the entire databank. Still over 2048 bits 
so think for yourselves. When porting Telememer onto Python, I&#x27;ll also 
prepare some character sets to make it easier for you to select which model 
and language you&#x27;re going to use. Not that I expect a lot of people to 
actually practically use it for encoding valuable information into their 
watches, but I hope it might help someone in a &quot;life or death&quot; situation, 
and also potentially am going to use it myself. I mean, if you sacrifice the 
first letter for indexing, you could even store some BIP-39 mnemonic phrases 
for crypto wallets: not a great idea from the first glance, but if 
absolutely necessary, why not?

Last but not least, DB-36 also functions quite well as... a watch. The screen
is pretty legible and clear, the accuracy is a bit of a lottery from item to 
item but tolerable (although I haven&#x27;t worn it for a prolonged period of 
time yet to measure it properly), the seconds reset with the Forward button 
is extremely easy without having to remove the watch from the wrist, and the 
green backlight is just great. The only two visible drawbacks I could notice 
are: 1) there doesn&#x27;t seem to be a way to switch the backlight duration from 
1.5 to 3 seconds, 2) there doesn&#x27;t seem to be a way to mute the buttons, 
i.e. turn off the button press confirmation beep. Curiously enough, both of 
these issues had been mitigated in DBC-32, but some other trade-offs had 
been made there that I&#x27;m not willing to accept. Another very important 
aspect of DB-36 is that it&#x27;s extremely lightweight and weighs just 27 grams 
on the stock strap. I can bear 150-gram watches, I got used to 100-gram 
watches, I&#x27;m pretty comfortable with 50-gram watches, but sub-30-gram ones 
just disappear on the wrist until I actually need to look at them. This is, 
like, the densest concentration of functionality per gram I have ever seen. 
And per dollar too, by the way. I&#x27;m also going to change the 18mm strap for 
a textile one from my W-800H mod (that came from a variant of AE1200WH) as 
it is more durable over time and definitely is gonna make the watch look 
more stylish while not increasing its weight significantly. As an 
alternative, a 18mm steel mesh is being considered.

Either way, DB-36 is a quintessential digital Casio in many aspects and the
biggest &quot;bang for a buck&quot; when it comes to purely LCD-based models still in 
production, followed only by DW-5600E and G-2900F. Despite some of its 
flaws, it can be a remedy for those looking for a more disconnected 
lifestyle, it&#x27;s not heavy on the wrist both literally and figuratively, and 
its affordability reduces the amount of headache if anything happens. Highly 
recommend it, and if you can change the strap, doubly so.

--- Luxferre ---

[1]: https://gist.github.com/plugnburn/63cc2825e02311a617af55653aecef1a
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-05-27-a-small-crypto-rant.txt</link>
<title>(2024-05-27) A small crypto rant</title>
<description><![CDATA[<pre>
(2024-05-27) A small crypto rant
--------------------------------
The state of things gets... well... interesting but I&#x27;m getting tired pretty
quickly. When I&#x27;m not as mentally strained as now I&#x27;ll definitely have a 
follow-up on the JJY signal format. For now, I&#x27;m gonna focus on something 
that I can fit into four paragraphs, not including this one. As hard as it 
gets, I&#x27;m not going to name names, but I think it will be easy for everyone 
to guess what exactly I&#x27;m talking about. The reason I&#x27;m not going to name 
names is because a certain cryptocurrency has developed a cult-like 
following, and I don&#x27;t want to become a target of hundreds of brainless 
fanatics thinking that their chosen coin, which in fact fully deviated from 
its original vision and essentially became fiat in disguise, is going to be 
the future of online transactions and a path to real freedom. People these 
days get easily triggered by the smallest amount of criticism, especially 
when they cannot into a single bit of critical thinking themselves. 
Therefore, my rule of thumb is: if *anything* develops a cult-like 
following, run away from it as far as you can.

For now, I&#x27;m not even as deep into crypto as most of its (non-fanatic) users,
but from what I have observed, there are about five types of them (besides 
those fanatics and self-proclaimed influencers): stackers, traders, 
cypherpunks, miners and normies. Let&#x27;s start with the last category. 
&quot;Normies&quot; isn&#x27;t necessarily a bad thing in this context: they just use 
crypto like any other currency, to hold some of it and pay for goods and 
services. They don&#x27;t like volatility so they usually use USDT or other 
stablecoins, not caring about privacy implications much. For them, it&#x27;s just 
another way to pay. Cypherpunks, on the other hand, also view crypto as a 
mere payment tool, but privacy is of the primary concern. I, for instance, 
place myself on the overlap of the normie and cypherpunk category. I don&#x27;t 
like volatility and like doing good things, but only if they can be done 
anonymously, because that&#x27;s the way to go.

Miners, as the name suggests, are usually only working with the first-gen
cryptocurrencies where the &quot;proof of work&quot; is required, because they produce 
it themselves. They also may or may not be stackers, believing in the market 
stability of the coin they own and mine, but the stacking concept is beyond 
the PoW currencies, and most of the time people mine one currency and then 
exchange it to stack another. The weakest-minded stackers, however, may 
become the aforementioned fanatics, and their amount is a clear indicator of 
what cryptos you should avoid. Finally, there are traders, and they don&#x27;t 
believe in the crypto at all but try to make everybody else think they do. 
For them, it&#x27;s just another speculative asset, and they essentially don&#x27;t 
differ much (if at all) from stock exchange traders because the nature of 
their business is pretty much the same, only with a little bigger grade of 
anonymity, although KYC-only centralized exchanges take away that advantage 
as well, and most coin traffic is still going through them. Which, IMO, 
nullifies the value of cryptocurrency in the first place, but hey, I&#x27;m more 
of a cypherpunk type.

Now, where am I going with all this? As with any other topic involving money
(especially big money), cryptocurrencies can and do generate a lot of hype, 
controversy and hate. The more money is involved, the more people start 
forgetting that it&#x27;s just a tool. When a tool becomes an object of 
speculation, it&#x27;s bad enough already, but when it becomes an investment, 
it&#x27;s outright pointless and sometimes even dangerous. This equally applies 
to, say, luxury watches as well as cryptocurrencies, and the existence of 
some real marketplaces where one can buy the former for the latter just 
indicates how spoiled the society has become on so many levels. And the 
apparent ease of entry, by the way, attracts more gullible general public 
that doesn&#x27;t have a clue how to behave in this space and falls prey to 
multiple scammers, which, by the way, are among the loudest crypto endorsers 
out there. All this paints an even more negative image on the technology 
itself and certainly doesn&#x27;t help the cause it was initially designed to 
serve.

Let&#x27;s be real: most crypto users don&#x27;t even realise why it is better than
fiat. They just want fiat minus some qualities they don&#x27;t like about it. For 
miners, this quality would be governmental monopoly on issuing, for 
cypherpunks — governmental control and lack of privacy, for stackers — 
inflation, for traders — tighter regulations. Surprisingly, very few people 
actually get the entire picture and fully understand why they want a totally 
new form of peer-to-peer digital currency, not just &quot;a better cash&quot;. And 
when they do understand this, there remains no place for hate, arrogance or 
fanaticism. Only a desire to make the world a better place by doing what 
they can with what they&#x27;ve got.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-06-03-greed-is-bad.txt</link>
<title>(2024-06-03) Greed is bad</title>
<description><![CDATA[<pre>
(2024-06-03) Greed is bad
-------------------------
This one is gonna be rather short. First of all, I didn&#x27;t even notice how I
had got past the 100-post milestone on this phlog. If there are any readers 
out there, then I hope it all wasn&#x27;t in vain. My present life is rather 
dynamic in both good and bad senses, so I don&#x27;t have a lot of time to do 
thorough writeups, although I do remember all the topics I wanted to cover, 
so stay tuned. This time, however, I want to talk about a particular case of 
greed I encountered on the past week, and it truly is something that doesn&#x27;t 
go unpunished in the long run.

Imagine that you&#x27;re running a VPS hosting reselling business, and your
selling point is that you&#x27;re doing it for cryptocurrencies and require no 
user data except an email address. In fact, you&#x27;re not doing pretty much 
anything else. How much of a markup can you ask from your customers for such 
a service (relative to the original VPS price)? +5%? +10%? +15%? Well, how 
about a whole fucking +50%? Yes, the &quot;hosting&quot; I stumbled upon last week (of 
course I won&#x27;t tell you the name) is doing exactly that: registering 
DigitalOcean droplets via the API requests from a dashboard and charging 
1.5x the original price of the droplet, whatever it may be. That&#x27;s it.

Here&#x27;s how it works: you register and account and agree to the ToS (and the
ToS page is the only place on the entire website where they mention they 
just resell DigitalOcean and thus their ToS apply firsthand), you top up an 
internal spend-only &quot;wallet&quot; on the dashboard and you order a droplet from 
inside that dashboard as well, where you have to supply your own SSH key to 
access it. Of course, no custom images, no monitoring opt-out on the setup 
stage and no API keys (they do seem to have the ability to create them but 
they don&#x27;t work at all as of now). As with DO itself, the amount in your 
wallet gets diminished on an hourly basis regardless of whether you&#x27;re using 
the instance or not. And no, powering off/rebooting servers from the 
dashboard doesn&#x27;t work either. You can only create or delete them. And they 
ask 1.5x for that kind of service. Pathetic.

I had done a small background check on who the developers of this abomination
might be, and after finding out, I wouldn&#x27;t have any moral concerns about 
attempting to bring those scammers down. Now, what&#x27;s the most creative way 
of doing this? Because if I just do something that results in a complaint 
from DigitalOcean (and I could do this there without spending a dollar), 
this will only cause my instances being terminated within a day by them and 
then the account banned by this &quot;hosting provider&quot;, who still will be doing 
just fine even after the complaints. Luckily, they are not that high in the 
search rankings and I hope less people will find them by accident, but the 
amount of greed just amazes me. For such a markup, they could have at least 
fixed their dashboard functionality and billing vulnerabilities. But why 
bother, they probably think, if people still eat this crap?

Well, not for long, folks. Not for long.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-06-10-wind-of-change.txt</link>
<title>(2024-06-10) Wind of change (recent news)</title>
<description><![CDATA[<pre>
(2024-06-10) Wind of change (recent news)
-----------------------------------------
OK, I&#x27;ve got nothing in particular to talk about today (JJY writeup still
pending, I know) but the past week has been tough and this one is going to 
be even more difficult for me. Besides work, there are essentially two 
things I&#x27;m focused on right now: energy independence and migration to Kaspa. 
The latter shouldn&#x27;t be a surprise as Kaspa is now in a very good state: 
it&#x27;s PoW but with tokens coming (KRC20), it&#x27;s very fast and fully 
decentralized. Even at a relatively small loss, I chose to abandon Mina and 
everything else (except, of course, Monero) in favor of it. And I surely 
don&#x27;t regret it as of now. When it comes to Kaspa, I&#x27;m more of a stacker 
than anyone else in my previous classification.

As for the former, the very first stage of my plan is complete and a power
inverter with a 6 kWh backup battery has been installed. Think &quot;Powerwall 
for the poor&quot;. The next stages are going to be a 800W wind turbine 
(hopefully done within this week) and a solar panel array (hopefully done 
within this month). I&#x27;m also going to run a small Kaspa miner ASIC (IceRiver 
KS0 Pro, only has ~100W consumption) purely on the energy collected from the 
wind turbine. By the way, it&#x27;s not about ROI, it&#x27;s about getting some 
experience with ASIC mining and slow but sure accumulation of the currency 
that I&#x27;m counting upon in the future. Besides, we shouldn&#x27;t forget about 
e.g. Sedra and Bugna that can be mined using the same KHeavyHash algorithm 
but are located at much earlier stages of the curve.

All this, however, is a part of a bigger plan that&#x27;s going to be revealed
when the time comes. Right now, it&#x27;s more about maintaining peace of mind 
than anything else.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-06-17-wind-of-change-pt2.txt</link>
<title>(2024-06-17) Wind of change, part 2</title>
<description><![CDATA[<pre>
(2024-06-17) Wind of change, part 2
-----------------------------------
Today, the turbine is finally operational. Now just I have to wait for a
consistent stream of wind with the speed above 3.5 m/s to appear at my 
location. 11 m/s, they say, is optimal, but it&#x27;s not the season now. This is 
why a 5.6 kW solar panel array also is in my plans. I&#x27;m being told, however, 
that this will take the entire June and probably half of July, so that&#x27;s a 
longer term than I thought. Well, I&#x27;m catching up to what should have been 
done a year or two ago, and doing that as quickly as humanly possible.

Given that, the KS0 Pro is being tested from the mains only but I&#x27;ll probably
turn it off this evening. Being this small, it still is a tremendous heat 
source, I think I can use this in the autumn/winter but now it doesn&#x27;t seem 
to be as helpful. Nevertheless, even with the current KAS price, it would 
fully pay for the electricity it consumes, it&#x27;s just not that simple for me 
to go for it where I have to live right now, so I&#x27;m gonna wait until a fully 
self-sustainable solution is implemented.

Other than that, I don&#x27;t really have a lot to tell about. Oh well, looks like
my Casio DB-36 is about 18 s/mo slow. I don&#x27;t know if it&#x27;s going to gain or 
lose anything within the two remaining days but I&#x27;m mildly disappointed. I 
mean, it&#x27;s not as bad as the W-800H I already got rid of, but I do have some 
mech Orients that are more accurate than that across the month. I still 
stand firmly upon my position that the main function of a wristwatch is to 
tell the time, and the accuracy of doing so is the main measurement of its 
quality. Yes, materials and design come after: even the most perfect looking 
watch is useless for me if it constantly lies to me about what time it is 
now or isn&#x27;t capable of being useful on the wrist 24/7 for various reasons 
(need of charging, low WR etc). And, to be honest, it saddens me that even 
some mechs can be regulated at home to improve their accuracy but most 
non-sync digitals can&#x27;t. Either way, I&#x27;m returning to my PMD56 within two 
days as the ultimate relief for my watch anxiety.

I think the name of this post is perfect for some of non-topical musings
about my everyday thoughts, so, most likely, there will be more parts. Stay 
tuned.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-06-24-cry-of-a-turtle.txt</link>
<title>(2024-06-24) Cry of a turtle</title>
<description><![CDATA[<pre>
(2024-06-24) Cry of a turtle
----------------------------
Recently, I&#x27;ve told that my watch collection is not going to expand but some
upgrade replacements of existing items might be possible, although that&#x27;s 
pretty unlikely. Well, now I&#x27;m willing to talk about one of those unlikely 
replacements I&#x27;ve had to perform recently, and I&#x27;m going to explain why it 
had to be done.

If you&#x27;ve been following this phlog since the beginning, you know that one of
my early posts was about Invicta 8926OB. This had been my most interesting 
mech until I got the Kamasu (Orient RA-AA0001B). While the Invicta&#x27;s 
performance was pretty OK (and I never had it unscrewed for regulation), 
there are some unavoidable issues that can only be spotted if you actually 
wear Kamasu or some other higher-spec automatic watch of the same &quot;diver&quot; 
class. I&#x27;m talking about (relatively) weak lume, stiff bezel and that stupid 
date magnifying glass slapped onto the (mineral) crystal in an overall 
attempt to look like a Rolex. And then there&#x27;s that engraving (which doesn&#x27;t 
bother me at all btw) made in an attempt to show that no, it&#x27;s not a Rolex. 
For its actual price, 8920OB is pretty good but the Kamasu has been superior 
in every possible way. And I actually took the time to make it even better 
by regulating its accuracy myself. But still, with this weight and diameter, 
it still is on the usability borderline for me when it comes to wearing it 
24/7. I felt a need to upgrade my mechs the same way I upgraded my quartz 
timepieces to Citizen PMD56: replace them with something rugged while 
stylish, compact and lightweight. Alas, Orient doesn&#x27;t offer anything &lt;=40mm 
made of titanium these days, and all Citizen&#x27;s mech titanium Promasters are 
41mm or larger, so I had to take a look at the Swiss offerings one more time.

And again, Certina to the rescue. This time, the model number is
C032.807.44.081.00, and the market name is Certina DS Action Diver 38mm 
Powermatic 80 Titanium. I don&#x27;t even know which one of these two model names 
is better. The Swiss (and Swatch Group in particular) definitely need to 
work on their model numbering. Just like in case of the DS Caimano 39mm, 
only a single color variant of this model actually has a titanium casing, 
and it is the one with turquoise blue markers and hands. That, I must add, 
definitely is a marketing stunt, because otherwise this watch definitely 
competes with the 39mm Tudor Pelagos while being about $4700 cheaper. After 
some initial research, I ordered it from the same shady dude I got the PMD56 
from, but I only had to wait for one day this time, not for three weeks. In 
fact, I had to wait for exactly 24 hours since that dude responded to my 
order before the watch was delivered to the house. Wonders do happen 
sometimes.

The first thing I noticed out of the box is that the watch is quite compact
indeed. IRL, it looks even smaller than the PMD56 but the actual dimensions 
are pretty similar. Definitely within my comfort zone. Still don&#x27;t get why 
they chose this exact colorway but this model definitely looks more 
expensive than it is: no wonder they compare it to those titanium Tudors. 
However, I also don&#x27;t understand why they chose to go with 19mm lug width 
for their bracelets: the Citizen&#x27;s 20mm would look just as fine and make the 
transition here even smoother. Still, this is probably *the* smallest 
diver&#x27;s watch that I have ever seen. And yes, it&#x27;s not just a &quot;diver-style&quot; 
watch, it is ISO 6425:2018 certified for 300m submersion. And the bracelet, 
besides having a wonderful system that allows to remove &quot;half a link&quot; from 
either side, has an extension to be able to put the watch over a diver&#x27;s 
suit, by the way. Hence, I don&#x27;t want to open it up myself (it would be 
pointless btw, see below) and will try to live with the level of accuracy 
that it provides out of the factory, but more on that later.

As it stands, this Certina is pretty comfortable to wear. With my personal
bracelet configuration (two full links removed), it weighs just 93 grams, 
less than the Invicta 8926OB on a NATO or the Kamasu on a mesh. And that&#x27;s 
the main point of going full titanium on something worn 24/7: your hands 
will thank you. The crown is convenient to operate and the rotating bezel 
action is reassuring: although I kinda miss the smoothness of the Kamasu 
bezel here, it&#x27;s not nearly as stiff as on the Invicta and has zero backplay 
whatsoever. And lest we forget about the decent amount of lume on the 
markers, hands and the bezel 12h lume pip: all that allows to easily see the 
current time in the darkness, including the second hand with that circular 
bubble. The turtle on the caseback reminds us of its true purpose. So, 
overall, it looks and feels as GADA as it can, at least to the extent 
available for its purely mechanical innards.

Speaking of which... This ETA Powermatic 80.611 (aka C07.611) is something
that is at least looking good on paper: 80-hour power reserve, antimagnetic 
balance spring (made of an alloy branded as Nivachron) and additional 
shock-resistant construction. Also, unlike the base and controversial 
C07.111, this one doesn&#x27;t use any plastic parts and is projected to last 
much longer. On the other hand, I might want to see C07.811 where those 
parts are made of silicon but I understand that they might detract from the 
overall shock resistance. There is, however, a potentially big problem with 
this movement: it is regulated at the ETA factory by laser-trimming the 
balance wheel to adjust its inertia once and for all, and it cannot be 
further regulated at home with any currently known method. In other words, 
we have to live in the setting imposed on us by the manufacturer of the 
movement, and this is something that I&#x27;m not 100% comfortable with. Because 
no matter what, the accuracy is bound to change over time, and the main 
question is whether or not it still is going to remain within my range of 
tolerance. For now though, I&#x27;ve started collecting the realtime data and the 
first two days look promising with well under 0.5 spd deviation in my 
all-round wearing mode. Let&#x27;s see what it settles upon in the future.

Now, just like with the DS Caimano Titanium, the paradox keeps repeating
itself here. That one delivered a ±10 s/year movement in a WR100 titanium 
case for far cheaper than any Japanese brand could do. This one delivers a 
laser-regulated 80h power reserve movement in a **compact** ISO-certified 
WR300 titanium case for far cheaper than any Japanese brand offers. And 
believe me, I searched really hard, there&#x27;s nothing else under 41mm with 
these specifications. Surprise me, Citizen, Orient and Seiko, try beating 
the $950 price of this one, I know you can do this as soon as you stop 
catering to the bear-handed and revert to smaller sizes and lighter 
materials. I, for instance, would *love* to see a 38mm titanium version of 
the Orient Kamasu, especially equipped with an F8 series movement with 
better OOTB accuracy and longer power reserve. And Certina, on the other 
hand, looks like one of the very few remaining Swiss brands (and probably 
the only one left in the Swatch Group) that aim for producing real watches 
and not those decorative marketing gimmicks for showing your &quot;status&quot; in the 
shallow world of corporate vanity. For the time being, this DS Action Diver 
38mm Titanium occupies the firm second place among the top 10 watches that 
I&#x27;ve ever owned. After Citizen PMD56-2951, of course.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-07-01-time-needs-to-be-reformed.txt</link>
<title>(2024-07-01) Time needs to be reformed</title>
<description><![CDATA[<pre>
(2024-07-01) Time needs to be reformed
--------------------------------------
As much as I love watches, I can&#x27;t help thinking about how imperfect our
overall system to display time and dates is. Some efforts to fix both appear 
in the world from time to time. I was reminded of this topic by seeing the 
article about the &quot;Swatch Internet Time&quot; when the normal (24-hour) day is 
just divided into its 1/1000th parts called .beats, and those .beats can 
optionally be divided into 1/100 units for greater precision. All this 
relative to Swiss standard (non-DST) time, not UTC. Some digital Swatches in 
late 1990s and early 2000s were displaying both traditional time and this 
&quot;Internet time&quot;, which by the way, was deemed convenient by some 
international gamers of that period.

The approach is rather radical, and I&#x27;ll return to something similar at the
end of the post. But let&#x27;s start from the bottom up with what we can do to 
the traditional timekeeping. The first and most obvious thing we can get rid 
of is DST, daylight saving time. Nowadays, it&#x27;s no longer justified to make 
people switch their life rhythms every half a year. Just let them live in 
the time natural for the area. Those who need to fully utilize the daylight 
will do so anyway, without any artificial limitations, and those who don&#x27;t, 
will live healthier when those limitations are lifted. For now, I feel that 
DST is more of a political decision than a really useful one. And with more 
and more work being done remotely, different areas can just coordinate via 
UTC if they need to.

Which brings me to the next point: abolish timezones. Completely. Timezones
are an absolute mess, and you can look at the map to see why. The opponents 
of this idea might say that it would totally twist the way of how we 
perceive time. So what? We&#x27;ve evolved for a reason. If you live somewhere 
where the morning starts at 21:00 UTC, so be it. It&#x27;s not as big of a 
problem as you might think. Those who live and work close to the poles 
(Northern or Southern) already have the nights and days longer than anyone 
else is used to, yet their clocks and watches count the same 24 hours as for 
the rest of us. So the connection between the sunlight part of the day and 
the time displayed on your clocks can be broken far more easily, especially 
if you travel a lot or work near the poles like I mentioned.

And yes, please also get rid of that stupid AM/PM notion and stop calling
24-hour time designations &quot;military time&quot;. It&#x27;s not about the military, it&#x27;s 
the only actual time we have right now. Newsflash: we don&#x27;t have 12 hours in 
a day, we have 24. The reasons to divide it in half are purely historical 
and don&#x27;t make much sense in the modern world.

Finally, let&#x27;s talk about the calendar. It&#x27;s a mess almost the size of the
timezone mess. Just like the length of the day is essentially determined by 
one Earth rotation about its axis, a year is determined by one revolution of 
Earth around the Sun. Throughout the history, people tried to simplify this 
number as far as they could, but it fought back. Essentially, what I would 
propose is to set the day and the week as the baseline and, given that the 
notion of week already is culturally universal, abolish the concept of 
months altogether and settle on something like the &quot;ISO week calendar&quot; with 
the year always starting on Monday and having an integer amount of weeks 
every single time. Which means, most years would have 364 days (52 weeks) 
and once per 8 years there would be one with 371 (53 weeks). When enough 
days from the would-be leap years have accumulated, another 53-week year is 
inserted into the next cycle. That would, however, happen once per 28 years 
on average. Anyway, I would also change the year numbering as well, but 
that&#x27;s definitely a discussion for another time.

But back to the beginning of the post with the Swatch&#x27;s radical proposal.
Like, is there any better way to divide the day than into the traditional 
hours, minutes and seconds? I think there is but it would take a lot of 
getting used to in comparison to any calendar reform. While I also like the 
initial .beats idea (except I&#x27;d use UTC as the baseline as opposed to CET), 
I have heard another proposal recently. In essence, the day is divided into 
3 parts (8 hours each) and each of those parts is divided into 256 units. 
The size of one unit turns out to be 1.875 of a standard minute. And, when 
displayed in hex, not only does it take only 2 indicator positions (within a 
part), but also the 0x10 value maps to 30 standard minutes. So, the first 
hex digit essentially measures 30-minute intervals, and the second one 
measures 1/16 parts of those intervals. If we go even further, we can reduce 
each &quot;day&quot; to 8 (standard) hours and thus completely get rid of the sunlight 
dependency when it comes to time measurement, which would justify UTC 
unification even better. And one week would have 21 of these neodays instead 
of 7 standard days. And, e.g. the date and time I&#x27;m writing this paragraph 
would be described not as 2024-07-01 08:25 UTC, but 2024-27-02-0D (27th 
week, 2nd neoday, time unit 13). If we need greater precision, we can 
further divide those units into 256 parts again, but I think you get the 
idea. Kudos to wd for sharing this interesting proposal with me.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-07-08-the-future-of-keypad-apps.txt</link>
<title>(2024-07-08) The future of keypad mobile applications (or lack thereof)</title>
<description><![CDATA[<pre>
(2024-07-08) The future of keypad mobile applications (or lack thereof)
-----------------------------------------------------------------------
If you&#x27;ve been keeping your eyes on what&#x27;s going on with current
NON-smartphone hardware lineups, then you might already have realized the 
situation is more than grim. I mean, it looks like the only LTE-enabled 
featurephone chipset is Unisoc UMS9117 or UMS9117L. And all the phones that 
have it run proprietary Mocor OS by Unisoc as well, modded a bit for HMD in 
case of Nokias. After Nokia 3310 3G, they also fully dropped J2ME support, 
and the MiniJ/MRP support had been dropped even earlier, so right now all 
the apps are fully compiled-in by the firmware builder. No customization 
whatsoever, despite the hardware itself being more than capable of doing it.

And now, trying to fill the vacuum, they started offering &quot;cloud apps&quot;. Of
course not for the phones sold where I live, and yes, the lack of this 
feature was proven to be a pure marketing gimmick. But those who do have 
these, find an adaptation of the Puffin browser interacting with the device 
in a fashion of Opera Mini (with the browser itself being a very dumb 
client) but providing a more seamless interaction with modern web 
applications. There is an entire platform provided by CloudMosa, there is 
some developer documentation and community of real people developing for it 
but... WTF happened to using your native offline resources? You have a 1 GHz 
CPU there, come on! Instead, you opt to constantly depend on the internet 
connectivity even for the tasks that are traditionally offline, blindly 
trust Puffin/CloudMosa and share all your in-app actions with who-knows-who 
every time you launch anything on that &quot;platform&quot; proxying all your 
requests. An ideal scenario for total control over &quot;the next billion&quot;, as 
they put it, in the NWO era where users own nothing anymore.

My take on it is simple: if you can&#x27;t invent anything better, just return
J2ME support. We&#x27;ll figure it out ourselves.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-07-15-need-to-take-a-break-again.txt</link>
<title>(2024-07-15) I need to take a break again</title>
<description><![CDATA[<pre>
(2024-07-15) I need to take a break again
-----------------------------------------
Battling the heatwave and stupidity of both the employers and family members
at the same time can be hard. These two weeks are going to be extremely 
difficult and I&#x27;m afraid I&#x27;m not gonna feel motivated to write anything 
extra here or elsewhere. Survival is the first priority now.

See ya on Aug 5, I guess.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-08-05-random-thoughts.txt</link>
<title>(2024-08-05) Random (vacational) thoughts</title>
<description><![CDATA[<pre>
(2024-08-05) Random (vacational) thoughts
-----------------------------------------
Well, I&#x27;m back. Kinda. Recently, I&#x27;ve been extremely tired, and continue to
be even when my vacation started in the middle of the previous week. The 
good news are that, firstly, my solar panel setup is making a good progress 
recently, secondly, I got all my retro gaming stuff back, including, but not 
limited to, a modded GBA with an EZFlash Omega flashcart. So, I&#x27;m also 
arranging my romsets to fit it.

If you&#x27;re wondering how my titanium Certina DS Action Diver has been doing,
it&#x27;s still my daily driver for now, having gained about +12 seconds per 
previous month but now settling on a faster side of things, closer to +1 
spd. Still not as critical but I liked the previous indication much better. 
Besides, I&#x27;m planning on getting one more watch just for the design sake of 
it: alas, it won&#x27;t be a titanium one but everything else will be just as 
nice, and its movement will be regulatable unlike this one. The PMD56, 
however, is still my favorite no matter what and I still rotate it sometimes 
with this Certina.

As for the recent Kaspa price drop (along with the rest of the crypto
market), I remain fully calm. For me, it&#x27;s just another opportunity to buy 
more for less. I haven&#x27;t given up my plans to accumulate at least 100-120k 
KAS and see what happens next. I&#x27;m also looking for some higher-level Python 
API to create an easy to use CLI wallet for Kaspa similar to the one I 
created for Tron. In the long term period, I see Monero and Kaspa as the 
only two networks worth doing anything on or with right now.

Overall, I hope this 2-month vacation will allow me to regain some strength
and return to my creative path. Conserving mental health is very important 
in this crazy world. That&#x27;s why we all need to rest sometimes.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-08-12-sokoban-occam-razor.txt</link>
<title>(2024-08-12) Sokoban: a case of Occam's Razor in video gaming</title>
<description><![CDATA[<pre>
(2024-08-12) Sokoban: a case of Occam&#x27;s Razor in video gaming
-------------------------------------------------------------
In case I didn&#x27;t already mention it, my gaming preferences have significantly
changed throughout the year. Now, I prefer the games that are more quiet, 
calm and don&#x27;t require a lot of timing. For instance, I have beaten Solomon 
no Kagi 2 (along with its 50 bonus levels), started playing Saboteur (on BGA 
website) and mastering FreeCell (usually on Aisleriot but I also have my own 
AWK-based implementation), but I think the most important switch in my 
&quot;gaming career&quot; was the return to Sokoban.

Being introduced back in 1982 by Hiroyuki Imabayashi and his company called
Thinking Rabbit, Sokoban was a major hit. Few people know, however, that the 
original NEC PC-8801 version had some mechanics in the second half of the 
game that later disappeared from all subsequent releases, as well as 
unofficial clones: false walls. Quoting Sokoban Fandom: &quot;These false walls 
appear just like regular walls, with no indication that they are different, 
but if the player moves toward them from a specific direction they are 
demolished, thus forming a path through which the player can move through. 
This mechanic would be ditched after this game.&quot;

Well, why was it ditched? I think it was because such a mechanic would be
excessive, and much more variety can be added just by designing new levels 
instead of complicating the engine itself. And this is what I&#x27;d like to 
emphasize on: simplicity at the core. Sokoban rules are so simple and 
well-defined that the entire engine can even fit into a 512-byte x86 boot 
sector ([1]). There are just seven different tile sets: empty space, a goal 
spot, the player, a box, the player on a goal, a box on a goal and a wall. 
Together with a &quot;line break&quot;, this only means 8-state (3-bit) pieces needed 
to design a level. Moves and collisions are easily implementable by simple 
comparisons and bitwise operations. Controls require 6 buttons at most, that 
is, if we include undo functionality, which is a good practice for any 
Sokoban developer. There were some famous official Sokoban implementations 
though (like Boxxle on Game Boy) that only included last move undo and 
didn&#x27;t save entire history, but nowadays that&#x27;s more of an exception.

Considering this simplicity, the amount of alternative engines and even two
different &quot;standard&quot; level formats (one plaintext and one XML-based), 
there&#x27;s no wonder Sokoban gathered a huge community around it. And this is 
where the variety I was talking about comes from: as of now, there are over 
43000 ([2]) community-submitted levels! Just try to wrap your head around 
the number and imagine the total playtime of all this. Even if some of them 
could repeat others, it still would easily be over 40k levels of unique 
content. No game publisher could ever do such work. Maybe we are going to 
see some AI-generated Sokoban levels in the near future, but I&#x27;m referring 
to a database of the levels designed, played and proven to be solvable by 
real people. This is what I call success: from 20 levels on an obscure 
Japanese PC to over 43k levels that can be played on any platform existing 
in the world that allows to sideload applications.

Needless to say, I&#x27;m in.

--- Luxferre ---

[1]: https://ish.works/bootsector/bootsector.html
[2]: https://www.sourcecode.se/sokoban/levels.php
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-08-19-all-about-ownership.txt</link>
<title>(2024-08-19) It's all about ownership</title>
<description><![CDATA[<pre>
(2024-08-19) It&#x27;s all about ownership
-------------------------------------
There is a huge problem no one can be silent about anymore. A huge question
that you need to ask yourself whenever you consider buying anything, or even 
receiving it as a gift or something: &quot;Will I own it once I get it?&quot;

The scale of this problem is much larger than you think. And I&#x27;m not even
talking about legal restrictions (let&#x27;s be honest, no one fully cares about 
them), I&#x27;m talking about real dystopian stuff. I&#x27;m talking about every time 
some game or software switches to a subscription model (including the 
firmware of frigging cars that people already have paid a ton of money for), 
every time a hardware crypto key breaks, every time you rely upon streaming 
services instead of downloading everything locally and self-hosting it 
whenever required, and then suddenly find out that the content you got used 
to is no longer there and you can do absolutely nothing about it.

For instance, I keep seeing cryptobros actively advertising &quot;hardware
wallets&quot; like Tangem or Ledger. Who is to guarantee that the master keys 
never leave the device or weren&#x27;t backed up by the manufacturer even before 
putting them inside? How do you restore your wallet keys if the device 
breaks? If you choose the recovery passphrase option, how does it differ 
from you just memorizing the BIP passphrase and using it to restore the keys 
with any other wallet, without introducing this extra point of failure? 
Lastly, whatever happened to &quot;paper wallets&quot; where you just print out your 
keypairs as QR codes and hex values and store them in a secure place?

This kind of plague didn&#x27;t appear overnight. Since mid-2000s, we&#x27;ve been
slowly but surely fed the worst Orwellian practices disguised as 
technological progress. All &quot;for our own convenience&quot;, of course. It&#x27;s just 
that now the amount of things we have to pay for but still don&#x27;t own becomes 
so apparent that it can&#x27;t be ignored anymore. The most important part of it 
is that such things were considered a novelty and out of place back then, 
but are so commonplace now that the general public already can&#x27;t imagine 
otherwise, and readily and mindlessly considers anyone who prefers to 
actually download stuff &quot;a pirate&quot;, &quot;a dork&quot; or &quot;a geek&quot; at best. This is 
how scarily efficiently Overton window sliding works nowadays.

So, what can be done about this? Plenty, actually. Increase your local
long-term storage capacity. Archive your software and data. Go Linux and BSD 
way. Go FOSS-first regardless of the platform. Collect game ROMs and support 
emulation. Download books, music and movies. Learn how to liberate the 
firmware of whatever proprietary electronics you have to use. Combat planned 
obsolescence. DIY wherever you can. Learn analogue skills (mech watch 
regulation, slide rule usage, Sun/star navigation etc). Resist corporate 
normie propaganda and keep your mind clear of online noise.

They may have won the battle, but not the war.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-08-26-helping-retro-meet-modern.txt</link>
<title>(2024-08-26) Helping retro meet modern</title>
<description><![CDATA[<pre>
(2024-08-26) Helping retro meet modern
--------------------------------------
I have reinstalled everything on my main PC, the Asus PN41 nettop. Now,
instead of Arch + OpenBSD, it&#x27;s only running Alpine Linux. That&#x27;s what I 
spent the main part of the week on, and the results satisfied me. By the 
way, I&#x27;ve migrated from WindowMaker to Fluxbox and it still is perfect for 
my needs. My weekend though has also been rather interesting, and that&#x27;s 
because of two things. First, I got a VoIP ATA gateway, namely Grandstream 
HT802. Second, my 12-watch waxed canvas roll by Bertucci finally arrived, 
alongside the watch I&#x27;d been hinting about before: Seiko SBDC091 (aka Seiko 
Prospex Alpinist Green). And yes, that&#x27;s SBDC091, not SPB121, although one 
couldn&#x27;t probably tell any difference without the original packaging. What 
do these things have in common? They bridge modern and retro technologies.

As for the HT802, I&#x27;d been planning to purchase something like this for some
time. It&#x27;s only powered by micro-USB, drawing 1A of current at most, hence 
its maximum power consumption is 5W. But it&#x27;s capable of serving two phone 
lines via different SIP accounts simultaneously, and more importantly, it 
supports pulse dialing. Yes, one can hook an old rotary phone to this small 
box and have it talking to the Internet in no time. This is how I got 
interested in the purchase. I was initially planning on buying an even older 
version (HT502), but it turned out to be out of stock when I ordered it, so 
I got this one. And didn&#x27;t regret it.

The firmware upgrade process (the recent version is 1.0.55) wasn&#x27;t as
straightforward: Grandstream had changed the main update URL to 
firmware.grandstream.com, so it had to be adjusted in the admin dashboard, 
making sure plain HTTP is being used. Afterwards though, even an option to 
configure routing all traffic through OpenVPN appeared, and I might put it 
into some use in the future. But what am I using this device for? Well, I do 
have a SIP number reachable from normal PSTNs, and I do have some old wired 
phones lying around. With this device, I can create a fixed line phone for 
my room that would be using that SIP number, and actually use it for calling 
instead of any of my cellphones, since I&#x27;ve been spending most of my time in 
this very room anyway. And yes, I&#x27;m going to get an old Tesla rotary phone 
from a friend of mine within this week, so the process is going to be much 
more fun than I can imagine.

Now, to the Seiko SBDC091. I&#x27;ve immediately put a different strap on it
(dark-green canvas with quick-release bars) but, other than that, it looks 
much cooler than it costs. Time will tell how accurate it is out of the box, 
but, unlike the Certina DS Action Diver (which still had been running well 
under +1 spd, by the way), the 6R35A in this one is regulatable, so it 
shouldn&#x27;t be a problem in the long run. I probably will write a separate 
post with my impressions about this watch some time into the future, but for 
now I can say one thing: those who keep comparing it to the SARB017 are 
doing so in vain. In fact, SARB017 had no _real_ advantages over this one 
besides probably a white-on-black date window. SARB017 didn&#x27;t even have 
anti-reflective coating whereas this one has. Here though, where they 
returned the date magnifier (and I say returned, not added out of nothing), 
I understand why they implemented a black-on-white date: this new model is 
all about improved legibility under any conditions. And believe me, all of 
us start appreciating this when we grow older. Again, the only thing I&#x27;d 
really like to see is a titanium cased version of the very same watch.

Finally, the Bertucci watch roll. I must admit, that&#x27;s one of the things one
can never understand until trying it out. The concept is so simple yet so 
efficient that I wish I had bought one before even buying any of the 
&quot;traditional&quot; watch boxes. It&#x27;s extremely convenient for securely packing 
the entire watch collection into a bag or backpack without taking a lot of 
additional space or weight. As I only have 13 watches left in my entire 
collection (including SBDC091), this roll perfectly fits my purpose: twelve 
watches packed and one on the wrist. And the collection is probably gonna 
shrink even more in the years to come, so it&#x27;s future-proof enough for me.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-09-02-rotary-phones-are-underrated.txt</link>
<title>(2024-09-02) Rotary phones are underrated (feat. VEF TA-68)</title>
<description><![CDATA[<pre>
(2024-09-02) Rotary phones are underrated (feat. VEF TA-68)
-----------------------------------------------------------
The Tesla (Typ 66?) telephone that I got as promised last week turned out to
be a bit faulty: its dial (made py Polish RWT) sometimes emitted one more 
pulse per digit than necessary. And I deduced that nothing could be done 
about it, so the same friend of mine got me a VEF TA-68 made in Latvia in Q3 
1978 that his family had been using up until the mid-2010s. Yes, that 
red-case/black-base neato had been manufactured 46 years ago, and still is 
in a fully working condition, while the Tesla (seemingly) made in Jan 1990 
already has some dial issues (well, it also fell from the table so the case 
got several cracks as well, while the VEF&#x27;s case is almost like new). It&#x27;s 
remarkable that both of them adhere to the ancient German-type wire color 
coding, so the brownish wire is Line+ and the white one is Line-. The rest 
of the wires were unnecessary in either case in order to connect the device 
to the RJ-11 cable I ripped off from some faulty Genius steering wheel 
pedals. Both phones passed all GR-909 tests offered by my Grandstream HT-802 
ATA, so, after a bit of cleaning, the TA-68 was ready to use. It&#x27;s amazing 
how a 46-year old landline phone is much easier to bring back to life 
without running into any legal troubles, as opposed to, say, some NMT or 
AMPS mobile phone from early 1990s.

This is how my SIP DID number from Intertelecom came alive again. I had been
using it for testing my FrugalVox IVR program, but that never got any actual 
use as of yet. Now, it&#x27;s serving as my landline, although it&#x27;s quite amusing 
how this &quot;landline&quot; actually gets routed through a wireless mesh and then a 
Starlink satellite dish. All thanks to that ATA that is being powered by a 
measly microUSB cable, requiring 1A of current at most. So, with 5W max 
power, you can serve _two_ such phone lines (each over a 5+ meter long VEF&#x27;s 
cord, mind you) and route them to various VoIP providers independently. And 
here comes the most interesting part: dialing a number with a rotary dial 
doesn&#x27;t consume any extra line power, it&#x27;s just a series of rapid 
disconnections that make up these pulses. On the contrary, any push-button 
keypad does consume extra power for the circuits to generate the DTMF 
signals to work, and the signals also have to be loud enough, or, in case of 
still using pulse dialing, to programmatically emulate those pulses on the 
line. Another paradox is that pulse dialing is purely digital (it&#x27;s a 
variation of so-called &quot;unary code&quot;) while DTMF (I hate the &quot;touch-tone&quot; 
term) is analogue, being a mixture of two sine waves, while both generating 
and decoding it require some digital signal processing.

So, why were rotary phones phased out so quickly (how quickly, depends on the
part of the world) despite being so simple, robust, reliable and energy 
efficient? Well, the #1 answer is convenience. Not only did keypads allow to 
dial the numbers faster and the electronic circuits allowed adding the 
features like caller ID display and answering machines, but the introduction 
of DTMF support (which, by the way, wasn&#x27;t a thing until late 1990s where I 
lived) by automatic phone exchanges opened up a whole new world of 
possibilities to customers. Even this Grandstream HT802, despite pulse 
dialing support _and_ converting pulses to tones down the line, can be 
controlled internally by dialing *** (three stars) which is impossible to do 
on a rotary phone. And to quickly check the balance without signing into the 
Intertelecom&#x27;s web portal, I need to dial *7501, again, no way I can do this 
on my TA-68. I even have an Actionline DTMF beeper to be used on the phones 
that don&#x27;t support tone dialing — need to replace those 3xLR44 batteries 
though. So, now I think you can see the problem: the phones kept working but 
the world around them kept changing. Ten digits are no longer enough, it 
seems. There are some workarounds like the &quot;hook flash&quot; technique for call 
control, but this only works when you already made a connection, and there 
doesn&#x27;t seem to be any way to remap the flash event into dialing e.g. the 
star character.

Furthermore, when cordless landline telephones were introduced, people became
hooked on them even quicker. Now you could talk virtually anywhere in the 
house (or flat) and return the handset to the base station only when it 
needed charging. With the first generations of cordless phones (or 
radiotelephones, as we called them), no one even thought about the fact that 
they could be eavesdropped to with a simple VHF/UHF receiver. This is, like, 
the first time when convenience won over privacy in the history of 
telecommunication for &quot;mere mortals&quot;. Long-range radiotelephones (Senao etc) 
added even more salt into the wound, interfering with other public radio 
systems (and leading to innocent people being prosecuted for this) and being 
equally insecure. In fact, until the introduction of the DECT standard, you 
could not be sure whether the handset-base link was properly encrypted or 
not. The problem with DECT standard is... when it really became widespread, 
it was too late: its original encryption had been broken by 2008, and 
cellphones already took over the world. The question &quot;did mobiles kill 
landlines?&quot; is highly debatable though, I might dedicate a separate post 
about it but my short answer is &quot;I wouldn&#x27;t be so sure&quot;.

What I am sure about is that, despite all those points, rotary phones are
still underrated. They still can work on remaining landlines with no extra 
power required, they can be used on VoIP lines with as little as 5W of power 
required in total, they don&#x27;t have any electronics to possibly spy on you 
inside (with a traditional landline, that&#x27;s what your phone company can do, 
but with VoIP, you are in control of at least some security options on the 
ATA), they are not prone to radio eavesdropping (at least not without some 
extremely specialized equipment), they have little to no parts that can 
break under normal conditions, but even then they are extremely easy to 
repair and modify, most of them still have some controls like a ringer 
volume regulator, some of them have microphone mute and/or hook flash 
buttons, and most importantly, they don&#x27;t have _any_ distraction factors 
whatsoever. You can just place and receive calls, optionally control them 
with a hook flash and adjust the ringer bell volume. That&#x27;s it.

That&#x27;s it. And I think that&#x27;s beautiful.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-09-09-making-c-great-again-mongoose.txt</link>
<title>(2024-09-09) Making pure C great again (feat. Mongoose)</title>
<description><![CDATA[<pre>
(2024-09-09) Making pure C great again (feat. Mongoose)
-------------------------------------------------------
Here, I&#x27;m not going to talk about that Mongoose that has something to do with
MongoDB, I don&#x27;t even know what it does. I&#x27;m going to talk about that 
Mongoose that resides on https://mongoose.ws and is an &quot;industrial-grade&quot; 
networking library with more or less complete Web server capabilities. It&#x27;s 
written in pure C and compilable directly with your own sources: you just 
add mongoose.c and mongoose.h to the project, include the .h file in the 
source and the .c file to the list of files passed to the compiler, and 
that&#x27;s it. I, for instance, could not find the process as straighforward for 
libhttp, which they say had been derived from Mongoose some time ago.

I&#x27;ve stumbled upon this library when looking to simplify my upcoming
livestream chat aggregation server to make it not use raw socket and become 
more cross-platform in the future. The backend-server-frontend interaction 
concept had changed several times but I still liked the idea the core should 
be written in pure C with little to no third-party additions, and its 
structure should be kept as simple and robust as possible. The amount of 
manual work, however, began to overwhelm me and made it look not so simple 
anymore, so I started to look for alternatives. And this is the one I 
decided to use after all.

Of course, Mongoose does have some issues on its own: it doesn&#x27;t compile with
-std=c89 (or pretty much any -std for that matter), it can throw out 
&quot;Illegal instruction&quot; errors when compiled with -O flags (and I&#x27;m not even 
sure what&#x27;s to blame here — Alpine Linux, musl or Mongoose codebase itself), 
and it leads to pretty heavy binaries even when no additional compiler flags 
are switched on. However, despite all this, the package provided here is so 
complete that I can live without -O flags and C89 compatibility. You get 
your own multiprotocol networking, API server, static file server, 
WebSockets, JSON decoding, string processing and much, much more in a single 
place. It&#x27;s adding to C almost everything that I have missed here from what 
I have in Python. And it&#x27;s also said to be extremely portable across various 
OSes and architectures, which is quite important for what I&#x27;m trying to 
achieve here.

By the way, I&#x27;m also still looking for a pure C, lightweight, cross-platform
desktop GUI library. The closest to what I want is libui/libui-ng, but its 
almost non-existent scrolling capabilities make it a no-go for most real use 
cases. Dear ImGUI+cimgui, raylib+raygui and Nuklear only seem tiny, but the 
first two of them are built on top of OpenGL, which is quite far from my 
definition of lightweight, and the third one requires providing your own 
platform-dependent rendering code. In fact, the search for a sane desktop 
GUI is still ongoing, but for now Web-based seems like the only choice.

Lastly, I think pure C programming is still underrated. Of course it requires
you to adhere to strict security practices in terms of memory management and 
type checking, but when done properly, it rewards you with full knowledge 
about the internal flow of the program and unparalleled execution speed and 
resource efficiency, something that&#x27;s sadly often forgot about these days.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-09-16-going-in-the-right-direction.txt</link>
<title>(2024-09-16) GOing in the right direction</title>
<description><![CDATA[<pre>
(2024-09-16) GOing in the right direction
-----------------------------------------
No existing programming language is ideal. Everything has its own quirks,
inconsistencies and/or bloat. And, generally, the more mainstream the 
language is, the more reasons I find NOT to use it. One of them, however, 
had been under my radar for quite a long time, and I really don&#x27;t understand 
why. Yes, my StreamGoose project is officially live ([1]), and yes, I 
rewrote its message broker server from C to... Go.

Before Nim was introduced, a lot of people referred to Go as a &quot;compiled
Python&quot;, but that&#x27;s not entirely true. I found Go to include much more 
&quot;batteries&quot; and be much more versatile in certain ways. My only complaint is 
that the default behaviour is to link as much as possible statically, so 6+ 
MiB binaries are normal here. I don&#x27;t necessarily have any problem with 
that, considering the runtime is pretty complete AND ready to cross-compile 
too. You don&#x27;t even need to bring in any external cross-compiler unless 
you&#x27;re linking with external C libraries, as this process is using Cgo and 
you need to specify the C/C++ compilers to use with it. And it turns out 
that Go supports plenty of platforms and architectures out of the box, and 
for those who need something totaly exotic, there&#x27;s TinyGo ([2]) as well, 
although its capabilities are much more limited as of now.

The main thing that astonished me, however, is how easy Go is to pick up and
start writing real-life stuff in it. Learning basic syntax and keywords is a 
matter of several hours, and getting really familiar with the language is a 
matter of several days. As for someone with C/JS/Python/AWK background, some 
Go&#x27;s syntactic features still look quite odd to me, but once you learn more 
idiomatic ways of doing things over time (and a very short period of time, I 
must add), these features start looking less and less strange. Anyway, as I 
remember that Go was developed as an alternative to C++, I must add that 
it&#x27;s lightyears ahead in terms of readability even for complete noobs like 
myself. As I already mentioned in one of my posts, I ditched syntax 
highlighting back in the past. With Go, you can fully understand what&#x27;s 
going on (no pun intended) in a piece of plain, non-colored code. With C++, 
it turns into a mess. That&#x27;s a sign of good language design: to not require 
the user to rely on highlighting crutches.

Nowadays, by the way, the hype around Go has settled down, as the hipstest
among the techbros already have jumped to Rust. That&#x27;s a good sign for Go as 
it is entering the real maturity stage. And, while I understand some merits 
of Rust for some system software that requires tight memory management and 
really high network throughput, I don&#x27;t understand why everyone else (who 
doesn&#x27;t have those requirements) starts jumping onto the same inhumane 
syntax bandwagon. Because Rust really looks even less readable than C++ or 
Java for the general purpose usage.

And, strategically speaking, Go has the potential to shift them off in the
desktop([3]), server and even mobile ([4]) areas. As I said, it is quite 
versatile itself. Even WebAssembly is one of its target platforms. Go has a 
wonderful native multithreading (Goroutines) and inter-thread communication 
(Go channels) support, something that would require using a third-party 
library in many other runtimes. And they are integrated into the language in 
the most straightforward way I have ever seen: you declare channels with the 
chan type (providing a data type that will be used inside), communicate with 
them using the &lt;- operator, and make any function a Goroutine by launching 
it with the &quot;go&quot; keyword before it. Yep, that simple. Overall, it feels like 
a perfect candidate for that &quot;one language collection&quot; if you&#x27;re too lazy 
(or afraid) to learn anything else.

So, I&#x27;ll definitely try keeping Go as my main programming language of
operation for the time being. And hopefully, sometimes in the future, I&#x27;ll 
tell you how it goes.

--- Luxferre ---

[1]: https://codeberg.org/luxferre/StreamGoose
[2]: https://tinygo.org
[3]: https://awesome-go.com/gui/
[4]: https://pkg.go.dev/golang.org/x/mobile/app
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-09-23-on-desktop-gui-dev.txt</link>
<title>(2024-09-23) In 2024, desktop GUI development is still pretty wild</title>
<description><![CDATA[<pre>
(2024-09-23) In 2024, desktop GUI development is still pretty wild
------------------------------------------------------------------
Throughout the recent, like, 10 years already, there is a popular question:
why are the current-gen desktop GUI application developers so lazy that they 
keep choosing humongous browser-based frameworks like Electron to implement 
the GUIs, as opposed to using traditional native toolkits consuming much 
less resources? Well, I think I know several sad but true answers to this 
question now. And the first and the most obvious one is: Web browsers work 
and everything else doesn&#x27;t.

Let&#x27;s consider a simple use case inspired by my StreamGoose chat UI which is
in no way any complex: a scrollable vertical list of messages that are 
dynamically appended and can have various height, which is solely determined 
by the width of the list block itself and the length of their contents. 
That&#x27;s it. Any Web browser does this naturally by design. Now, try and find 
a desktop-native GUI framework, preferably a cross-platform one, that would 
allow you to do just that.

I tried doing this for Go, which is a rather popular programming language and
has a lot of bindings to a lot of graphical stuff. Even the well-renowned 
Fyne framework doesn&#x27;t allow you to have dynamic list item height, you&#x27;ll 
have to calculate it for each item individually, which is nuts, considering 
you can resize the window and also have some images inside the message. The 
only library that **kinda** worked in this scenario was Gio UI ([1]), but 
it&#x27;s far from any desktop-native experience because all of those &quot;immediate 
mode UI&quot; toolkits are built on top of OpenGL or similar renderers, and when 
you have to use an engine designed for 3D gaming just to be able to draw 
simple GUI widgets, I think there&#x27;s something inherently wrong with that. 
Some GUI libraries (e.g. giu and Cogent Core) didn&#x27;t compile at all on my 
Alpine because of some dependencies not working with musl, great... And, of 
course, Go&#x27;s libui binding generally lacks scrolling support just like its C 
counterpart.

Unfortunately, the problems don&#x27;t stop there. Returning to Fyne, there also
is no straightforward way to set a different color to a label or even a part 
of the rich text element. The developers say it&#x27;s a part of &quot;enforcing UI 
consistency&quot;, but what if e.g. coloring individual Twitch usernames 
according to their preferences is a necessary part of application design? As 
such, you&#x27;ll have to use cumbersome workarounds where you really shouldn&#x27;t. 
If I color a label, I fully know what I&#x27;m doing and why. My take on it is: 
stop treating developers like idiots and don&#x27;t restrict their freedom to do 
stuff. Otherwise, the use cases of your toolkit won&#x27;t go far beyond what you 
can see on the Fyne&#x27;s app gallery page ([2]). Because yes, for anything 
slightly more advanced, you&#x27;ll have to become MUCH more verbose.

And no, it&#x27;s not a new thing, desktop toolkits have been like this forever.
MFC, VCL, GTK, Qt... All of them invented a bunch of problems that you 
absolutely must solve instead of thinking about the UI itself. In terms of 
development complexity and boilerplate code amount, none of them progressed 
a lot compared to what bare WinAPI/Xaw/etc could offer. This is because all 
of them were created from the perspective of those who code, not those who 
compose. Web browsers, on the other hand, became too atrractive despite all 
their flaws because they could offer something radically different: you are 
no longer constrained to a predefined set of layouts, themes, fonts etc, you 
get fluid text and widget flow basically for free, and, once written 
correctly, your GUI will work virtually anywhere, from a 2.5-inch KaiOS 
phone to a giant iMac. In fact, Web UIs became notoriously bad and slow just 
because most devs stopped knowing how to write them correctly and started 
stacking a lot of useless framework bloatware on top of them, not because 
the underlying technology is evil or something.

Speaking of which, my second answer to the initial question is: yes,
developers are lazy, they usually don&#x27;t have time and/or budget to rewrite 
the same UI for different target platforms, or, in case of cross-platform 
frameworks like Qt and Fyne, to fight any quirks they might find in a 
particular deployment. With HTML5, as long as you adhere to the &quot;lowest 
common denominator&quot; of Web standards, you&#x27;re generally fine on any platform. 
With true desktop GUI, it&#x27;s not so obvious, and someone might eventually 
catch some bug that you can&#x27;t reproduce on your own system. Go-based 
applications are generally less susceptible to platform-dependent bugs, 
because they usually bundle as much stuff as possible inside the binary, but 
that brings another problem to the table: a Fyne hello-world, compiled in 
release mode, weighs about 22 MB (18 if you exclude the built-in emoji 
font), and even a barebones Gio UI application is about 7MB large. Of 
course, any of this is nowhere near the 90 to 170 megs of full-bundled 
Web-based applications, but in that case you always have an option to offer 
the user a headless version and to connect with an existing Web browser to 
it (I even have such a build option for StreamGoose). Anyway, I know that 
storage space is not a huge issue these days and that most of the stuff 
occupying that space is not binaries themselves, but OTOH I am old enough to 
remember self-contained complex UI programs taking up much less than a 
single 3.5&quot; floppy, so... what happened?

Alright, enough of this rant. What I&#x27;m trying to get to is: don&#x27;t yell at
those who chose Web for desktop GUI, at least without providing any 
**viable** alternatives. They chose it simply because everything else they 
could hear about just sucks at being a good tool for their purposes. This is 
why we also keep hearing that desktop is dying on the mass consumer market: 
no one actually wants to write for it. In order to prevent this, we need a 
really well thought out, close to native, easy to use, lightweight, 
accessible and cross-platform GUI toolkit. As soon as we find one, things 
are going to change for the better.

--- Luxferre ---

[1]: https://gioui.org/
[2]: https://apps.fyne.io/all
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-09-30-lets-talk-input-devices.txt</link>
<title>(2024-09-30) It's time to talk about PC input devices</title>
<description><![CDATA[<pre>
(2024-09-30) It&#x27;s time to talk about PC input devices
-----------------------------------------------------
I&#x27;m writing this post using a new mechanical keyboard. The catch is, it costs
much less than my older optomech (A4Tech Bloody B800) and generally is as 
Chinese as it can get, but in fact I like it even more. I&#x27;m talking about 
Ziyoulang K68, but that definitely isn&#x27;t the only thing I&#x27;m gonna talk about 
today.

You see, I&#x27;d used to be more or less indiscriminate about choosing my
peripherals. Having used membrane keyboards and regular mice for most of my 
life, I can say that they are objectively not as bad as some might think. 
There is, however, a difference between &quot;not bad&quot; and &quot;really good&quot;, and the 
difference is huge. So, once I, for example, moved away from mice, touchpads 
and sometimes trackpoints to trackballs, I&#x27;m not even thinking anynmore to 
try and go back. Trackballs were the first thing to change my interaction 
with the PC forever. The same holds true when I bought the B800. I just 
don&#x27;t want to return to other keyboard types after having tried it out and 
extensively using it for several years. I can definitely recommend it for 
whoever wants a full-sizer and doesn&#x27;t care about RGB, as there only is a 
single backlight color for the entire keyboard.

Why did I switch to the cheapo K68 though (besides just buying it as an extra
to a Data Frog SF2000 and a Miyoo Mini+ that deserves its own dedicated 
post)? Well, two reasons. The first one is that it&#x27;s fully wireless and can 
support 3 devices at the same time (1 via the dongle and 2 via Bluetooth), 
which is perfect for my &quot;personal nettop + corporate laptop + tablet&quot; 
scenario. The second one is that it&#x27;s really compact (in the 65% class 
although some folks even put it into the 60%) and takes up much less space 
than the B800 on the table. Moreover, I just wondered how long will a 
Chinese wireless mech keyboard last for such a price, so consider this 
another kind of experimnent. And, despite it lacking the general key 
backlight, I&#x27;m pretty liking it with its red Jixian switches. Even though 
that&#x27;s the first keyboard I&#x27;ve seen in my life that has a dedicated combo 
for locking just the &quot;Win&quot; (&quot;Super&quot;) modifier key. What&#x27;s that for? Who 
knows, who cares... I only have a single thought about it: maybe it&#x27;s made 
for Faildows-based gamers who are afraid to accidentally press it and have 
the game minimized as the &quot;Start&quot; menu will show up. No other ideas, sorry.

As I said, however, this isn&#x27;t the only change I&#x27;ve made in my setup, having
also upgraded the trackball itself in a similar fashion: from a brand to a 
noname with more connectivity options. For the recent 2.5 years, my main 
trackball has been a Logitech Ergo M575, a wonderful piece of engineering 
that, nevertheless, had its scrollwheel broken beyond repair within the 
2-year period. For its retail price, I find this unacceptable. So, I have 
upgraded to a Jomaa (aka Jelly Comb) MT50 trackball. This one, despite a 
lesser price, has several advantages over the M575, the most important of 
them being three devices support (again, 1 via the dongle and 2 via 
Bluetooth) instead of just two in the Logitech, a DPI adjustment button and 
a USB-C charging port. Although, I must admit, an option to use an AA 
battery instead also wouldn&#x27;t hurt. On top of that, the ball itself in the 
MT50 isn&#x27;t as good as in M575, but the good news is, they are absolutely the 
same size and easily swappable, so that&#x27;s not a problem at all for me now. 
In fact, I&#x27;m running it with the M575&#x27;s as of the time of writing this post.

Overall, having tested both of these goodies with three aforementioned
devices, I can conclude that they pretty much fit for the job and haven&#x27;t 
let me down once yet. How long they will last is another question, but I&#x27;ll 
defintely keep you posted. 

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-10-07-omnia-mea-mecum-porto-tcltk.txt</link>
<title>(2024-10-07) Omnia mea mecum porto (feat. Tcl/Tk)</title>
<description><![CDATA[<pre>
(2024-10-07) Omnia mea mecum porto (feat. Tcl/Tk)
-------------------------------------------------
Two posts ago, I ranted about the lack of sane and decent desktop GUI
solutions. Now, I must emphasize that whatever I said there only applies to 
compiled programming languages. The domain of desktop GUI scripting hasn&#x27;t 
been vacant for all these years and hasn&#x27;t been only limited to JS, VBscript 
(remember that shit?) or some obscure languages from the past like Rexx. 
Popular interpreted programming languages like Python and Perl have been 
having their own bindings to all possible C/C++ GUI frameworks in existence, 
and some languages like Rebol and Red even have self-contained GUI 
facilities. There is, however, one particular language that&#x27;s simple enough 
to learn it and start rolling functional GUIs in a matter of minutes, 
lightweight and cross-platform enough to be sure your GUIs will run 
anywhere, mature enough to deter any hype riders trying to parasite on it, 
and dynamic enough to ward off &quot;static typing is everything&quot;-type snobs. 
That&#x27;s why, in the light of a recent new major version release of this 
language and its accompanying graphical toolkit, I&#x27;d like to talk about 
Tcl/Tk today.

What&#x27;s interesting is that despite both of the names being acronyms (Tool
Command Language and Tool Kit respectively), they are just written with only 
the first letter capitalized. Even without Tk, the Tcl language itself is 
interesting on its own: it&#x27;s built upon the same principles TRAC and REBOL 
were built (even naming TRAC as one of its predecessors on its wiki), but 
it&#x27;s simpler to grasp than both of them. The language grammar itself is 
defined with the famous Dodekalogue ([1]), sometimes combined into Octalogue 
(the example is given on the same wiki page), but I have built an even 
simpler understanding of what&#x27;s going on there, and this can be formulated 
in just several sentences.

In Tcl, everything is a string. A string containing a whitespace-separated
sequence of other strings is a list. Unless specified as a literal (within 
double quotes or curly braces), the following three sentences apply to any 
string. Any list is interpreted as a command. A script is a newline- or 
semicolon-separated sequence of commands. Any string can be enclosed in the 
square brackets which causes it to be interpreted as a Tcl script and the 
result substituted in place of the string. If the string starts with $, then 
it gets replaced with the value of the variable whose name is in the string 
(not counting the $). {*} makes each item in a list an individual argument 
of the current command.

That&#x27;s it, that&#x27;s the entirety of Tcl language rules. Of course, the
Dodekalogue also contains definitions of valid characters and backslash 
notations and whatnot, but the basic understanding can be as concise as the 
paragraph above. Everything else in the language builds upon the notions of 
strings, lists, list expansion, variable and script substitutions. All 
built-in commands like proc, if, for, foreach, list, dict, array etc have 
nothing special about them and are just commands operating on lists and 
strings. Even the comment operator # is just a command, this is why a space 
is mandatory after it (unless it&#x27;s a shebang) or a semicolon is mandatory 
before it if you append it to an existing line of code. All this makes Tcl 
even closer to Lisp than one would realize just because of how different 
they seemingly look, although the LISt Processing nature makes Tcl use the 
same prefix notation for all things in the world, even the mathematical 
expressions need the &quot;expr&quot; command if you need to use anything infix. By 
the way, conditional commands call &quot;expr&quot; implicitly, this is why you can 
use infix expressions in the conditions as well.

From the functionality perspective, Tcl is quite a &quot;batteries included&quot;
language even not counting Tk, although the choice of builtin commands and 
packages may seem quite strange. All this because it has a rather unique 
distribution system and plenty of various implementations, sometimes not 
very compatible with each other. Once you have Tcllib ([2]) up and running 
though, I think you&#x27;re pretty much set ([3]) for 95% of real-life non-GUI 
development scenarios. The GUI part, as you might have guessed, is covered 
by Tk and the Wish (WIndowed SHell) components of the Tcl/Tk distribution. 
And there also is a Starpack system which, although quite outdated (the most 
recent sdx kit file is from 2011), still works pretty well for packing any 
Tcl application into a single binary file. Just make sure to include Metakit 
(mk4tcl), Tcllib and TLS packages into your tclkit binary when building it 
with whatever way you prefer. That&#x27;s the absolute viable minimum. And, of 
course, don&#x27;t also forget to include Tk there if you&#x27;re targeting GUI 
development. On top of that, even the stock Tcl interpreter, tclsh, also 
runs great interactivly as a REPL. Well, it becomes much more usable as a 
REPL provided that you run it via rlwrap, the same way we did back in the 
ed-related post.

Now, let&#x27;s briefly talk about what Tk can offer us in terms of GUI. While the
Tk widget command syntax is not fully declarative, it&#x27;s as close as it can 
get to it with zero boilerplate and without sacrificing the flexibility. To 
me, who spends a lot of time with command-line parameters, this syntax is 
much more readable than XML or even HTML, let alone Go/Fyne, C/GTK, C++/Qt 
or other &quot;traditional&quot; GUI toolkits, the only minor inconvenience being 
widget creation and placement consisting of two separate commands. That, 
however, takes place because Tk has three completely unrelated modes of 
widget placement: absolute (&quot;place&quot; command), stacking (&quot;pack&quot; command) and 
grid-like (&quot;grid&quot; command). In real life, you&#x27;ll mostly find yourself 
combining at least two of these three modes, so you definitely need some 
flexibility when using them, so I don&#x27;t really regret the creation and 
placement not stuffed into a single line of code. As for the widgets 
themselves, you can find everything you really need for desktop GUI 
programming out there, and the most recent version, Tk 9, even added support 
for host OS printing, notifications and systray. Now you really have no 
excuse to move elsewhere.

The most underappreciated Tk feature though, in my opinion, is bidirectional
data binding out of the box. I reckon this only is available when you use it 
directly with Tcl and not through other language bindings like Tkinter. The 
thing is, you can assign a global variable to any widget that can change its 
parameters, and whenever the user modifies the widget contents (enters text, 
clicks checkboxes, moves sliders etc), the variable automagically changes, 
and vice versa. You don&#x27;t need to subscribe to any change events or signals, 
or call the value retrieval method or whatever, it just happens by itself. 
This way, the actual widget value always stays in sync with what the user 
sees.

One last thing: contrary to popular beliefs about the bland and obsolete UI
style, Tk is themable. In fact, it is now recommended to create all widgets 
from the ttk (themed Tk) namespace. The non-themed Tk engine is only there 
for backward compatibility with previous versions. Besides several 
preinstalled themes (&quot;default&quot;, &quot;alt&quot;, &quot;clam&quot; and &quot;classic&quot;), you actually 
have plenty of options with ttk. You can even try to match the host OS style 
if you want to. For instance, for normal OSes, there is a dynamic theme 
called &quot;gtkTtk&quot; ([4]) that you can separately build on your system and have 
it pull whatever GTK theme you&#x27;re currently using to apply the same style to 
your Tk applications. For other OSes, there are &quot;aqua&quot;, &quot;winnative&quot;, 
&quot;winxpnative&quot; and &quot;vista&quot; themes to match corresponding native widget 
styles. In your own code, if you&#x27;re trying to target as many people as 
possible but don&#x27;t want to include your own theme, you can even write 
something like this in the catch blocks:

# iterate over available platform-dependent themes, apply &quot;clam&quot; if none found
ttk::style theme use clam
catch {ttk::style theme use aqua}
catch {ttk::style theme use winnative}
catch {ttk::style theme use vista}
catch {ttk::style theme use gtkTtk}

Well, I guess this covers the most basic stuff about this incredible
scripting language. I think it deserves more attention than it gets, 
especially in the modern world of total nonsense and bloatware, 
**especially** when it comes to interpreted programming languages and 
graphical frameworks. Some people might argue it&#x27;s not &quot;enterprise-grade&quot; 
enough despite its history in computing is already quite long. For me 
though, Tcl/Tk still is something really viable for day-to-day desktop 
scripting. I can imagine creating a bunch of small utilities for personal 
use (both online and offline) that could easily fit on a floppy (let alone a 
thumbdrive or a microSD) and travel with me from system to system, from 
environment to environment, not being affected by anything external at all 
as long as the underlying Tcl/Tk version can run there. And with version 9, 
it got even better, although I still will have to stick to 8.6 for some 
time, at least until the 9 gets ported to the AndroWish/undroidwish stack 
and some other places that new versions get ported to much later than the 
upstream ones. Regardless, it still feels exciting to rediscover such a 
language and its capabilities and see a new major version released after a 
long time. Now, I think, desktop GUI development finally got a chance, after 
all.

--- Luxferre ---

[1]: https://wiki.tcl-lang.org/page/Dodekalogue
[2]: https://www.tcl.tk/software/tcllib/
[3]: https://core.tcl-lang.org/tcllib/doc/trunk/embedded/md/toc.md
[4]: https://github.com/Geballin/gtkTtk
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-10-14-goodbye-bopher-hello-bfg-boxcl.txt</link>
<title>(2024-10-14) Goodbye Bopher, hello BFG: a practical case of Tcl/Tk's GUI</title>
<description><![CDATA[<pre>
(2024-10-14) Goodbye Bopher, hello BFG: a practical case of Tcl/Tk&#x27;s GUI
------------------------------------------------------------------------
Tcl 8.6 is a powerful tool indeed, and I hope my previous post have shown
why. As I have already written, I could see having my personal portfolio of 
small but useful Tk GUI apps. And, of course, I had to start somewhere. So, 
as my first &quot;real&quot; Tk application, I decided to write something that I 
myself would use a lot: a new Gopher browser instead of my pretty limited 
and unmaintainable Bopher-NG and the already bloated Lagrange. What I came 
up with is now called BFG ([1]), short for &quot;Back/Forward/Go&quot;. Yes, I put it 
up on Codeberg because I might need some bugfixes and contributions from 
folks from the &quot;outer web&quot;, who knows... Of course, I decided to replace 
Lagrange completely, which meant that my own replacement should at least 
support not only Gopher, but Gemini, Spartan and Nex too. And it does, along 
with Gopher-over-TLS if you ever need such a thing.

Nex, by the way, is wonderful. In a way, it&#x27;s even lighter than Gopher
(because there are no complicated Gophermap-like directories) and borrows 
the only thing from Gemtext that I really like: its link format. The 
specification ([2]) is so short that I could copy and paste it right here, 
but I think I can rephrase it even shorter. Protocol-wise, Nex is fully 
identical to Gopher: you send a CRLF-terminated (although I think it can be 
just LF-terminated, the spec says nothing about line endings) selector path 
to the TCP server (the default port is 1900), and the server returns 
whatever content is under the path. For directories, the content is pure 
plain text with the exception of Gemini-like links, with the only difference 
from the Gemtext spec being that the space after =&gt; is mandatory. So, Nex 
links always start with these three characters &quot;=&gt; &quot;, which makes writing 
Nex-only (Nexclusive?) clients even easier. The spec ends with the 
assumption that an empty path or a path finishing with / is a directory and 
that a document should be displayed based on the path&#x27;s file extension. 
Without an extension or a trailing slash, plain text is assumed.

And yes, that&#x27;s really it. That&#x27;s the kind of evolution I wanted to see
instead of Gemini or Spartan in the first place. Nex is accompanied by a 
posting protocol called NPS, which is even more straightforward: connect on 
the port 1915 (by default), send raw text data and end the transmission by 
sending a line with a single dot. The server must respond with text and 
close the connection. Implementing NPS, however, is outside the scope of BFG 
(maybe a separate program will be created for it), I just mentioned it to 
highlight the simplicity of the overall idea.

But I digress. When writing BFG, I learned a lot about the protocols and how
stuff is done in Tk and Tcl in general. Even though I think my code is in no 
way optimal because I still am a noob in the language, I managed to pack the 
entire functionality I wanted to see into under 1000 SLOC, and I really want 
the codebase size to stay that way. Nevertheless, BFG is now my daily driver 
that really has replaced both Bopher-NG and Lagrange for the &quot;small web&quot; 
browsing purposes, and I really enjoyed writing and testing it on various 
resources. I hope whoever reads this post gives it a try as well.

Now, onto the case number 2: a Sokoban game in Tcl/Tk. I called it BOXcl
([3]), and this is where I don&#x27;t need outside contributions, so I placed it 
onto my own self-hosted Git. It doesn&#x27;t even have a readme because it&#x27;s a 
single Tcl script that contains everything. Yes, I even put the tile data in 
there (base64-encoded PNGs) and it still didn&#x27;t exceed 500 SLOC. Which is 
awesome, I think. The thing is, I already did have my own prototype Sokoban 
engine written in JS, but that was it, an engine. Which means it defined the 
internal field representation format and move logic. Now, besides 
translating this logic into Tcl, I had to &quot;draw the rest of the owl&quot; and 
convert it into a playable game. This allowed me to learn how image import 
and composition works in Tk, as well as comprehend the basics of Tk&#x27;s 
canvas. And, of course, I included an in-game help that can be called by 
pressing h or F1, and also made my game compatible with the huge databank of 
levelsets that can be downloaded from sourcecode.se ([4]) in the plaintext 
format (not the XML-based one though, maybe I&#x27;ll create a format converter). 
By the way, I&#x27;m really thinking about rehosting all the levelsets somewhere 
around here, on Gopher or even Nex, along with a copy of the game script 
itself.

--- Luxferre ---

[1]: https://codeberg.org/luxferre/BFG
[2]: https://nex.nightfall.city/nex/info/specification.txt
[3]: https://git.luxferre.top/boxcl
[4]: https://www.sourcecode.se/sokoban/levels.php
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-10-21-time-to-ditch-x86.txt</link>
<title>(2024-10-21) Time to ditch x86: why I have switched to RPi5 as my main desktop</title>
<description><![CDATA[<pre>
(2024-10-21) Time to ditch x86: why I have switched to RPi5 as my main desktop
------------------------------------------------------------------------------
If there is a single redeeming quality that the modern era of computing has,
it&#x27;s the abundance of single-board computers. We can call it modern if we 
forget that some of the earliest home PCs (like ZX Spectrum) also mostly 
were single-board: obviously the components were larger but replaceable at 
home, something that current SBCs totally are missing. If we return to the 
modern side of the topic, then, besides the highly dubious area of &quot;internet 
of things&quot;, single-board computers nowadays have matured enough to replace 
your average desktop while eating less electricity. And this is what I 
decided to check out myself as the low-powered computing is one of the main 
topics of this phlog. And here&#x27;s what I decided to do: get a Raspberry Pi 5 
starter kit and install Alpine Linux on it. Of course it would be Alpine, 
what else could it be? Who needs a huge &quot;official&quot; blob of Raspberry Pi OS 
(ex-Raspbian) when you should use such amount of computing resources 
sparingly? I won&#x27;t lie, the installation process didn&#x27;t go as well as I 
thought at the first time, but that was totally my fault for not following 
the manuals and not quite understanding what exactly I was doing. Once I 
finally understood it though, everything went smoothly and I have a fresh 
&quot;classic&quot; setup on the RPi5 I&#x27;m already writing this post on. The 
post-installation phase is another thing, and I don&#x27;t think it&#x27;s necessary 
to describe it here, just a small hint which groups your user should be a 
part of at the end of this process: lp wheel audio input video netdev 
docker. That&#x27;s it.

That being said, I&#x27;d like to talk about &quot;why&quot; instead of &quot;how&quot;. Well, for
starters, the x86 history is full of crutches upon crutches, and now it&#x27;s a 
total architectural mess. It&#x27;s easier to start from scratch. ARM is one of 
those attempts that got the most success. Ideally, I dream about writing the 
same post regarding RISC-V, but there&#x27;s no viable RISC-V-based low-power 
desktop option as of the time of this writing that I could get my hands on. 
Alas. I hope they appear soon enough though. The second point is, because 
ARM is less quirky than x86, my RPi5 consumes 15 to 20 watts less than my 
Asus PN41 nettop I replaced with it, while providing virtually the same 
processing power. That&#x27;s 20..23 vs 40..45 W (including the same display 
monitor) we&#x27;re talking about. This is significant. The third point is, with 
all the power economy, it still is the same familiar Linux environment at 
the end of the day. The only hurdle to overcome is running Wine (if you ever 
need that), but other than that, it really is the same. I don&#x27;t feel limited 
in anything here compared to the previous nettop, besides the amount of RAM 
(4 vs 32 GB). The thing is, I don&#x27;t need that much RAM here either: its main 
consumer is the Web browser (another motivation to use 
Gopher/Nex/Spartan/Gemini more than ever, btw: my BFG eats at most 9MB per 
window) and the problem mitigation is as easy as keeping the number of tabs 
low. So, as the last point, I&#x27;d like to state that such environments keep 
you disciplined and mindful about resources whatever you&#x27;re doing.

Of course, such migration is no small thing and I spent a couple of days just
getting everything to the state I used to have. But I think that the end 
result turned out to be more than satisfying. Given that my main &quot;hobbyist&quot; 
focus has shifted to Tcl/Tk scripting which has excellent support of such 
frugal setups, there really seem to be no drawbacks for making such a 
fundamental architecture switch. By the way, it really goes to show how 
seamless Linux and FOSS ecosystem is: you can switch architectures and get 
the same set of tools and software in no time. This, I think, is one of its 
greatest strengths. And more importantly, it demonstrates that the x86 
hegemony has just been historically based upon the Faildows hegemony in the 
desktop segment (the &quot;wintel&quot; amalgamation): once the latter has started 
being not so important as before, so has the former. The destruction of this 
status quo has already begun, and I hope that more accessible RISC-V-based 
SBCs (or even some laptops, who knows) will be the final nail into the 
corporate desktop monolith that cracks it apart. For now though, let me 
continue with the Aarch64. I&#x27;ll keep you posted on how this goes in the 
future. At least the small part of this future that I still can shape myself.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-10-28-meet-ii.txt</link>
<title>(2024-10-28) Meet ii, a new old distributed plaintext communication network</title>
<description><![CDATA[<pre>
(2024-10-28) Meet ii, a new old distributed plaintext communication network
---------------------------------------------------------------------------
Remember FidoNet? Now, imagine something like that but much simpler and over
TCP/IP (as of now, even over HTTP(S)). This is a very vague description of 
what I&#x27;m gonna talk about.

As lightweight protocols are among my hobbyist interests, I was delighted to
find out about the existence of such a protocol as ii. Gopher, Nex, Spartan, 
Gemini etc. are nice but they don&#x27;t solve the problem of distributed 
communication. Email does but it&#x27;s too complex and bulky. Misfin is fine for 
one-to-one communication but totally impractical for mailing lists or so. We 
really need something like Usenet/FidoNet but without all the crutches of 
the past.

This is what ii is about. It was developed in the spring of 2014 and its
initial author had abandoned it long ago, but it lives in the form of the 
&quot;IDEC network&quot; (where IDEC stands for &quot;ii-like Data Exchange Convention&quot;) 
that implements some extensions to the original protocol, which are fully 
optional, and is backwards-compatible with ii. I have tested some of those 
extensions myself but eventually deemed all of them unnecessary for my own 
client purposes, so my client only implements the basic ii standard as 
described in ([1]), and yes, I think my doc is more concrete and concise 
than the original GitHub. Maybe I&#x27;ll also add it to the Gopherspace to make 
sure it doesn&#x27;t get lost somewhere.

As for the client, it&#x27;s called tii, written in Tcl/Tk 8.6 (of course) and
distributed in the same repo ([2]) as a suite of several scripts, including 
the tiifetch.tcl CLI fetcher, tiipost.tcl CLI poster, tiiview.tcl CLI viewer 
and tiix.tcl GUI client. As of now, tiix.tcl depends upon the tiifetch.tcl 
and tiipost.tcl for corresponding functionality, so tii is not a 
single-script solution and wasn&#x27;t designed to be one. By the way, I couldn&#x27;t 
find any existing GUI clients for the ii/IDEC network, so tiix.tcl might be 
the first one. Besides Tcllib, the tii suite also depends upon the sqlite3 
package because this is the actual backend for ii message storage.

As for the server side, there exist several ii/IDEC node (aka station)
implementations in various programming languages (mainly C and Python), but 
they have some inconsistency in terms of following even the basic ii specs, 
so I&#x27;ve decided to start working on tiid, my own ii station software. It is 
only going to serve basic ii requests (GET /list.txt, GET /u/e, GET /u/m and 
POST /u/point) and have a message DB format compatible with the client-side 
one. Ideally, it should reuse the same codebase and algorithms from 
tiifetch. The only thing left to design before I start coding this up is the 
mechanism of secure anti-spambot signup without requiring JS or any Web 
browser whatsoever on one hand, and without any manual approvals on the 
other hand. Essentially, this part will probably be more complicated than 
the rest of the node codebase, but we&#x27;ll see how it goes.

Overall, I think this network has some potential to become a lightweight
censorship-resistant environment island in the ocean of centralized 
locked-down Web bloatware. This is why I&#x27;m totally in.

--- Luxferre ---

[1]: https://git.luxferre.top/tii/file/ii-doc.txt.html
[2]: https://git.luxferre.top/tii/file/README.html
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-11-04-mysterious-orient-2007.txt</link>
<title>(2024-11-04) A mysterious but extremely cool Orient Eroz from 2007</title>
<description><![CDATA[<pre>
(2024-11-04) A mysterious but extremely cool Orient Eroz from 2007
------------------------------------------------------------------
What I&#x27;m wearing right now is a watch that anyone should have heard about but
no one actually had. It&#x27;s an automatic mechanical three-hander with date 
window in a 37mm case. Made in May 2007, if I decoded the serial number 
correctly. Amazingly comfortable and handsome looking. Yet it&#x27;s almost 
impossible to look up any information about it on the Internet. I&#x27;m talking 
about Orient BER0Z002K, although this model number might say nothing to you, 
and I would understand it. It really seems like a mystery, a forgotten gem 
from the end of pre-Epson era of Orient. Yet here it is, and I&#x27;m going to 
tell everything I know about it.

I had bought this watch in a local online shop. Clearly in a used condition
but hardly ever worn by the previous owner who even retained the original 
tags on the bracelet. It&#x27;s one of those tags that I knew the full model 
number from. Because it was only sold as &quot;Orient Titanium&quot;, and this 
moniker, of course, stuck to another blast from the past that&#x27;s too large 
for me, the ER2F series, which was, as you might have guessed, on the same 
48743 movement as this one. And when I read the tag and saw that the model 
number starts from B, this became the first mysterious element about this 
watch. I know that pre-Epson model numbering used S for Japanese factories 
and F for Chinese ones. I don&#x27;t remember what C stands for, but B? Da hell 
is that? I still don&#x27;t have any answer to this day.

Second, the online information about this model is extremely scarce. I
literally found up to 3 websites ever mentioning it. Fortunately there were 
some online shops that I trust that retained the old catalog information, so 
I verified with them and confirmed that was a legitimate model and there 
even had been a whole lineup of similar models. Given how good this one is 
while not being crazy expensive even at the time it was new, I don&#x27;t get why 
it got no reviews whatsoever while the ER2F lineup (which, of course, was a 
bit newer) got so much praise. It&#x27;s as if Orient deliberately didn&#x27;t want 
anyone to know about the entire ER0Z series. Or the information had been 
erased by the natural course of time...

Now, how would I describe the ER0Z models? Well... Imagine a pre-Epson 37mm
Orient Tristar that suddenly got rid of the weekday (and corresponding 
pusher), got a much cleaner and slicker dial and got put into a slimmer case 
made of titanium with the accompanying folded-link (sic) titanium bracelet 
with the same 8 microadjustments that Tristar owners got used to. That&#x27;s it, 
that&#x27;s the description of this watch in a nutshell. In case of my particular 
model (ER0Z002K), it got a &quot;titanium sunburst&quot; dial with gold-accented 
hands, markers and crown. I wish I could find the 001K (silver instead of 
gold) but this one also looks nice enough, both much more retro and much 
newer than it actually is. In my case, however, the looks are not 
everything. Performance is also a very important factor for a watch to stay 
in my collection. And oh man, did the performance of this one not disappoint.

Get this: my initial daily accuracy measurement had shown a mere +15 s/day
deviation. For a 17 years old automatic watch (which I&#x27;m pretty sure hadn&#x27;t 
undergone any regulation before me), that&#x27;s a miracle. And, like all other 
mech Orients I encountered, this one is extremely easy to regulate to the 
point of spot-on timing (seems like my guide for F49/F6 fully applies here 
as well, but I also have ordered a demagnetizer because who knows where this 
watch had been stored). When I opened the caseback, I could easily see why 
even F4902 was superior to 48743, but I really wish Orient had brought back 
such an ultraslim design in 37mm and titanium for their newer models, and a 
hypothetical no-weekday version of F49 could fit perfectly into such a 
design. Back in the late 2000s, I got used to quartz models in the packages 
that slim and lightweight, not friggin&#x27; autos with a working rotor.

Anyway, I am extremely satisfied with this watch. I can&#x27;t call it retro or
vintage yet (17-year period still isn&#x27;t a lot), but a rarity for sure, 
especially given its lack of online presence. Maybe my item underwent some 
service in the past, who knows. But the level of satisfaction it gives me 
now is only second to the Citizen PMD56. And that&#x27;s something, considering 
their price difference. Overall, I think this is one of the most underrated 
Orients of all time. Kamasu is tough, Bambino is hip, Vega is practical, but 
this one is so cool but so undeservedly forgotten that the marketologists 
didn&#x27;t even give it a nickname. I think I can coin one from the model number 
itself: Eroz. And a corresponding slogan: &quot;Orient Eroz: trust your 
mechalust!&quot; LOL. All jokes aside, I think that Epson really needs to bring 
this model back. It needs to be seen, heard about and worn by a bigger 
number of people. And I hope this post also helps with that.

--- Luxferre ---

</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-11-11-cat-s22-first-glance.txt</link>
<title>(2024-11-11) CAT S22 Flip: a keypad Android flip phone done almost right</title>
<description><![CDATA[<pre>
(2024-11-11) CAT S22 Flip: a keypad Android flip phone done almost right
------------------------------------------------------------------------
Since KaiOS flopped (and I&#x27;m sure it&#x27;s already pretty safe to declare that),
the question of finding a decent keypad phone with modern application 
support is relevant again. And things like Xiaomi Qin1s or AGM M7 are not 
exactly the kind of support I&#x27;d have expected in 2024. Luckily, Bullitt 
Mobile to the rescue once again, and the model that still can be found new 
(although sometimes unofficially), CAT S22 Flip, seems to deliver the very 
experience I was looking for. It has a Qualcomm Snapdragon 215 (although its 
Fastboot menu was compiled for 430), 2 gigs of RAM and 16 gigs of built-in 
flash storage with, of course, ability to expand it with a microSD card 
which I surely did. It runs Android 11 Go Edition with some (unwanted but 
removable with root) T-Mobile/Sprint customization, and it looks like the 
firmware comes in two versions: 29.04 and 30.03, the latter being what my 
device has, and TBH I doubt this model will get any more updates in the 
future. And here&#x27;s where things get interesting: it has both a physical 
keypad and a touchscreen, although you&#x27;d never guess it had one from the 
pictures. Hence, both Gboard and KikaInput are preinstalled in the firmware, 
and, thankfully, I managed to replace both of them with Unexpected Keyboard 
and Traditional T9 respectively.

Hardware-wise, this phone is, as Action Retro would say, &quot;not too shabby&quot;,
despite only supporting a single SIM in the nano-SIM form factor. The lack 
of a 3.5mm audio port was compensated with an adapter in the box (which, by 
the way, you will need if you want to listen to the FM radio), and the lack 
of wireless charging was compensated with a two-pin dock connector on the 
back. Good luck finding that dock though, but you still have an option 
nevertheless. And let&#x27;s not forget a really useful flashlight, which has 
recently almost replaced my regular flashlight for the kind of tasks where I 
have to wander around the house at night. My only real complaint about all 
this is the size: CAT S22 Flip is almost twice as thick as CAT B40 while 
being just as wide and only a couple of centimiters shorter than it... in 
the closed state. You won&#x27;t really understand how humongous it looks until 
you see it in person. Granted that it&#x27;s heavier than B40 as well... I think 
you get the idea. I understand that this chungus houses a removable 2000 mAh 
battery but still, they should have consulted whoever manufactures Sigma 
X-Style 31 how it should be done. So let&#x27;s hope that all this extra size and 
weight is for some impact and ingress protection the brand claims for this 
model.

To be honest, rooting and debloating are the first things one should do with
such a phone, although it&#x27;s not without its quirks. The XDA forum has a 
guide ([1]) on how to root the S22 Flip, but my mileage varied a bit. First, 
the guide doesn&#x27;t tell you what to do if the &quot;OEM Unlocking&quot; option is 
greyed out. My device already came here carrier-unlocked (although the 
T-mobile branding is all over in the firmware) but I still had no option to 
toggle the OEM unlocking until I entered my Google account. Afterwards, 
beware that the firmware has encryption setting turned on, so running 
&quot;fastboot flashing unlock&quot; will erase your userdata partition. Then you can 
follow the guide, although I didn&#x27;t have to patch the vbmeta partition 
either. Maybe I&#x27;ll post the patched v30 boot.img somewhere alongside the 
simplified guide, we&#x27;ll see.

The post-rooting steps though are the very thing where one can screw up very
easily. Of course, I installed F-Droid (and FFUpdater and Obtainium via 
F-Droid) and a number of essential things, the De-Bloater application being 
among them. This is a Magisk module to move system applications out of the 
visibility while not physically deleting them. Nevertheless, it still can 
render the system unbootable, so be careful. By the way, you can find TWRP 
for this device, but I tested it and found out that it will do more harm 
than good, so I don&#x27;t recommend it. Here&#x27;s the list of packages (as they&#x27;re 
called in De-Bloater) I debloated this way and the system still boots and 
every basic function works:

* 12-Key Keyboard
* Android Setup
* Calculator
* Calendar
* Camera
* Carrier Device Manager
* Carrier Hub
* Cat phones
* Chrome
* Clock
* com.android.providers.partnerbookmarks
* Data Restore Tool
* Digital Wellbeing
* Drive
* Duo
* Files by Google
* Gallery Go
* Gboard
* Gmail
* Google Assistant Go
* Google Contacts Sync
* Google Go
* Google Location History
* Google Partner Setup
* Google Play services
* Google Play Store
* Google Services Framework
* Google SetupWizard Customization
* Google Speech Services
* Google TV
* Keep Notes
* Maps
* Market Feedback Agent
* MCM Client
* Messages
* Mobile Installer
* Speech Services by Google
* T-Mobile (AdaptClient.apk and TMobile.apk)
* T-Mobile Diagnostics
* TDC
* TMO-RSU-Common
* TMO-RSU-Sys-Service
* Unlock
* Visual Voicemail
* YouTube
* YouTube Music

Quite a lot to remove for such a device, if you ask me. I even considered
moving to a LineageOS setup (yes, there is one for S22) but it&#x27;s not quite 
ready yet, as it has no support for the programmable side button and, which 
is more important to me, for the external display. Also, do not remove 
Quickstep even if you install an alternative launcher: you&#x27;ll lose the 
ability to switch and close windows. Maybe this stock Android distribution 
just was built this way, who knows.

Other than that, it&#x27;s a pretty &quot;normal&quot; Android 11, so you can install
whatever you want, of course, remembering about the amount of RAM and 
storage here. I, for instance, switched to my favorite Android launcher for 
small screens, KISS Launcher, and likewise replaced a lot of the 
aforementioned debloated stuff with its FOSS counterparts: Files by Google 
=&gt; Amaze, Gmail =&gt; Thunderbird (former K-9 Mail), YouTube =&gt; Clipious, 
YouTube Music =&gt; InnerTune, Maps =&gt; OsmAnd~, Gallery Go =&gt; Fossify Gallery 
and so on. I think F-Droid is an excellent starting point to look for such 
replacements.

Of course, all that is fun and games, but how about the *real* stuff some of
you come to this phlog for? Well, for starters, let&#x27;s list some secret codes 
(where applicable, only the mnemonic is provided):

- *#*#INFO#*#*: standard Android info
- *#*#LOG#*#*: Qualcomm LogKit launcher/stopper
- *#*#ENGMODE#*#*: a comprehensive engineering menu with netmonitor and other
stats
- *#*#0202#*#*: some TFT stats or version
- *#*#02#*#*: device info
- *#*#2846#*#*: version info and test toolkit 

Okay, let&#x27;s see where the IMEI is stored in there. And it MIGHT seem like
it&#x27;s stored in the plain ASCII in the partition called &quot;factory&quot; at the 
offset 104 (0x68), but that&#x27;s an illusion, it gets overwritten there from 
somewhere else. Nor is it stored in the fsc, fsg, modemst1 or modemst2 
partitions. So where is it? Well, the answer seems to be not so pleasant: 
it&#x27;s stored in the &quot;persist&quot; partition, in the encrypted form. Namely, the 
mounted path of the directory is /mnt/vendor/persist/data/keymaster64, and 
inside this directory there is a &quot;keymaster64&quot; file that contains the index 
of all encrypted factory information files, including the IMEI number, in 
the same directory. It&#x27;s the Qualcomm KeyMaster, baby! Well, it looks like 
I&#x27;ve got myself another rather long-term research target in addition to the 
Pixels.

So, what&#x27;s my overall first impression about CAT S22 Flip? Well, about 7/10.
It definitely is usable (especially once rooted and debloated), it 
definitely is rugged, it definitely is &quot;smart&quot;, but it also is a little too 
bulky in size and too non-free to my taste, considering not only IMEI 
encryption but also the amount of work required to debloat the stock OS and 
lackluster support for third-party builds like LineageOS and TWRP. As I 
said, it makes a nice research target for the future efforts and a nice 
Android workhorse for the present time, but I&#x27;d also like to find something 
that would even more adequately respond to my needs. Something 
Android-based, maybe with a keypad/touchscreen combo as well, but slicker 
outside and friendlier to folks like me inside.

--- Luxferre ---

[1]:
https://xdaforums.com/t/tut-root-how-to-root-cat-s22-flip-on-version-30.46269
1/
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-11-18-lets-put-nix-into-pubnix.txt</link>
<title>(2024-11-18) Let's put Nix into a Pubnix (feat. nix-portable)</title>
<description><![CDATA[<pre>
(2024-11-18) Let&#x27;s put Nix into a Pubnix (feat. nix-portable)
-------------------------------------------------------------
The concept of Pubnixes is very common among the current generation of Gopher
users. Pubnixes are public access shared servers running Unix-like (usually 
Linux or BSD) systems and offering shell accounts to users, either for free 
or at extremely low prices. They are great for those who can&#x27;t afford or 
just don&#x27;t need a VPS for their cloud shell activity. Sometimes they also 
are called &quot;tildes&quot;, hence the name Tildeverse. In general, you are given a 
limited user account (although some projects like segfault.net will give you 
an isolated root account) and a limited amount of software packages 
available for you to run. One of the Pubnixes called Project Segfault (that 
sounds terribly similar but has no relation to the segfault.net) gives you a 
limited account but offers a Nix package manager to install packages in your 
local environment only. I thought: why not try and achieve the same in all 
other tildes that don&#x27;t have Nix available or exposed to your user? Luckily, 
there already is a project to ease your life in such a case.

Enter nix-portable ([1]). This is a self-contained binary that offers
everything you need to run a fully isolated Nix package environment under 
limited user privileges. It&#x27;s a rather large file because everything is 
built statically inside, but that makes it fully independent from the 
underlying distribution and it will work on any Pubnix as long as it runs 
Linux on an x86_64 or aarch64 architecture and gives you a large enough disk 
quota (do not even attempt to try this out if your quota is under 512 MB: as 
an example, I got banned on one shell provider for instantly overquoting its 
100 MB with this tool). You may try building nix-portable for other 
architectures and platforms as well, I didn&#x27;t try yet. Anyway, after 
downloading the file you just make necessary symlinks (as nix, nix-env and 
nix-shell) and you&#x27;re all set, just specify the packages in shell.nix and 
run ./nix-shell. To ease the process, I&#x27;ve created a simple shell script:

#!/bin/sh
WORKDIR=$PWD
echo &quot;Setting up nix-portable in $WORKDIR...&quot;
NIXBIN=$WORKDIR/nix-portable
RELEASEURL=https://github.com/DavHau/nix-portable/releases/latest
curl -L $RELEASEURL/download/nix-portable-$(uname -m) &gt; $NIXBIN
chmod +x $NIXBIN
ln -s $NIXBIN $WORKDIR/nix
ln -s $NIXBIN $WORKDIR/nix-shell
ln -s $NIXBIN $WORKDIR/nix-env
# setup the shell file in the same working directory
echo &quot;Setting up shell.nix in $WORKDIR...&quot;
cat &lt;&lt;EOF &gt; $WORKDIR/shell.nix
let pkgs = import &lt;nixpkgs&gt; {};
in pkgs.mkShell {
  nativeBuildInputs = with pkgs; [
    # list all the Nix packages you need here
  ];
}
EOF
echo &quot;All set, run ./nix-shell and enjoy!&quot;
echo &quot;Edit the package list in ./shell.nix if you need anything else&quot;

After you&#x27;ve run this script and see the nix-portable binary and
corresponding symlinks, you can run ./nix-shell and wait until you end up 
being in a Bash prompt with the &quot;nix-shell&quot; as a user name (although in fact 
the current user won&#x27;t change, i.e. the whoami output will be the same as 
the user you&#x27;ve run ./nix-shell from). The first run is going to be rather 
long as nix-portable will download and setup all the necessary dependencies 
and directory structures. Afterwards though, you can edit the shell.nix file 
and the next ./nix-shell run will just download and install the new packages 
you&#x27;ve added there.  

Ironically, I have first successfully used this approach on a root-enabled
Pubnix, as the systemwide package changes you make there are not persistent 
across sessions (it&#x27;s running in a container) but the changes made to your 
home directory are. However, it really doesn&#x27;t matter which user you&#x27;re 
running nix-portable as. In fact, the more limited it is, the more benefit 
you get from this kind of setup. One note of warning: it&#x27;s not a full chroot 
(or rather proot in this case) and sometimes the commands can interfere with 
the same command from the outer system, so you need to check your current 
$PATH to make sure the necessary binaries are loaded. I.e. if the outer 
system offers tmux, you&#x27;ll have a better luck running tmux on the host and 
the nix-shell inside it. If it doesn&#x27;t though, nothing prevents you from 
adding tmux package into your Nix environment.

Now, to close it all off, are there any personal free shell account
recommendations of mine? Well, from my own experience, most of them require 
you to get a manual approval, write an email, fill out Google Docs forms 
(can you believe that?), submit a PR, hang out in IRC channels or go through 
all other circles of humiliation (in some cases, it even goes as far as 
having to mail a physical postcard!) in order to get an account. There are, 
however, a few I can think of which offer decent levels of freedom and 
automated signup with no wait or hassle.

* The aforementioned segfault.net ([2]) looks like the best choice so far:
  - no email or password required;
  - no SSH key required (you get a secret env variable that you need to save)
although you can save it if necessary;
  - has fair terms of usage (you can keep your background services running
within 36 hours of your last logout);
  - runs Kali Linux with a lot of pentesting tools pre-installed;
  - routes traffic through three different VPNs;
  - provides you with a (containerized) root account.
* hashbang.sh is a more traditional approach to Pubnixes:
  - no email or password required (only your username and public SSH key);
  - you have to figure out the correct way to register by yourself, but it
just boils down to two REST API calls ([3]);
  - provides you with a limited Linux account, an email account
(username@hashbang.sh) and a lot of pre-configured stuff.
* ...suddenly, deepnote.com:
  - not a Pubnix per se but offers a free plan with 2 AMD EPYC cores and 5GB
RAM;
  - requires a Google or GitHub account to register;
  - the SSH shell can be obtained via tmate.io from a terminal (that can be
added to the project) like this:
    wget
https://github.com/tmate-io/tmate/releases/download/2.4.0/tmate-2.4.0-static-
inux-amd64.tar.xz \
      &amp;&amp; tar xJvf tmate-2.4.0-static-linux-amd64.tar.xz \
      &amp;&amp; tmate-2.4.0-static-linux-amd64/tmate -F
    or from a Jupyter notebook by prepending ! to the above command.

I have personally tested my nix-portable setup script on all the shells
listed above, so that you know that it works with no issues whatsoever 
there. Well, as of now, you know hot wo have fun with Nix packages on 
limited systems, and where to have fun with them. So, have fun!

--- Luxferre ---

[1]: https://github.com/DavHau/nix-portable
[2]: the instructions are available at https://www.thc.org/segfault
[3]: really, just read the website&#x27;s source code, it&#x27;s... unusual
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-11-25-boosting-8-bit-skill-avr.txt</link>
<title>(2024-11-25) Boosting the 8-bit skills with AVR microcontrollers</title>
<description><![CDATA[<pre>
(2024-11-25) Boosting the 8-bit skills with AVR microcontrollers
----------------------------------------------------------------
Yep, I&#x27;ve got a new hobby that eventually can bring me one step closer to one
of my dreams. It started when I remembered that I had an old clone Arduino 
Mega 2560 board from a RepRap-like 3D printer build kit that I never 
finished and never will. Such boards, as you might well know, are usually 
programmed via the C++-based Arduino IDE but I decided to go with pure C 
from the day one and use bare avr-gcc and avrdude directly. I also purchased 
a bunch of smaller clone Arduinos (all Nanos, either on ATmega168PA or on 
ATmega328PB), a bunch of programmers (more on that in a bit), and a bunch of 
wires, keypads, displays, resistors and breadboards, as well as downloaded a 
bunch of pinouts and datasheets. And the journey began.

At first, I didn&#x27;t need a dedicated hardware programmer at all, as the
ATmega2560 in the Arduino Mega is usually flashed via USB with the standard 
bootloader (&quot;-c wiring&quot; option in avrdude). But then, it just so happened 
that when the first of the (clone) Nanos arrived, they didn&#x27;t have any 
bootloader inside and I thought to make the Mega a programmer for them. And, 
of course, I managed to flash the wrong .hex file onto it. And, of course, 
it erased the bootloader. So, I rush-ordered the first USB-powered 
programmer I could find online. It was marked as Arduino-compatible and 
USBasp-compatible but... it turned out to be an ATmega88-based Chinese clone 
that wasn&#x27;t in fact compatible with USBasp and required a specific 
(Faildows-only) Chinese piece of software to work. Luckliy, I found the 
project to flash it back to the normal USBasp firmware ([1]) along with the 
instructions on how to do this. But it still required a &quot;normal&quot; programmer 
to perform the conversion. So I ordered another one, this time picking more 
carefully. Well, guess what... The new one didn&#x27;t want to work on my 
Raspberry Pi 5 no matter how I fiddled with the USB ports and USB-related 
kernel settings. Turned out that was one of the very first clone versions 
that required more current than the Rpi5 could deliver. Luckily, I managed 
to successfully connect it to my parents&#x27; PC with Ubuntu, so I quickly 
installed avrdude there and flashed the first one according to the 
instructions (basically, you have to make a jumper between the two &quot;UP&quot; 
holes before flashing, and remove this jumper afterwards). After the bootleg 
programmer was successfully recognized as USBasp, I tested it on my RPi5 and 
everything worked smoothly. To be on the safe side, I ordered three more 
programmers of yet another type, also marked as USBasp-compatible, but I&#x27;m 
still waiting for their delivery as of today.
 
But then, when I managed to restore the Mega 2560 to its normal state, I
started tinkering with the Nanos. In my first batch of Nanos, two of them 
had ATmega168PA, and two of them had ATmega328PB. For the untrained eye like 
mine, these chips look identical, with the 168 having half of all the specs 
of the 328. But when it comes to flashing a bootloader, the chip version 
matters a lot. I guess you can interchange the bootloaders for the 168P and 
168PA, but you definitely can&#x27;t interchange the bootloaders for the 328P and 
328PB. That&#x27;s why I spent a lot of time trying out various bootloaders I 
could find. And man, I don&#x27;t know, I&#x27;ve read everywhere that Urboot is much 
smaller and better than Optiboot but I couldn&#x27;t get any Urboot build to be 
as stable as Optiboot in any of those boards. Yes, Optiboot is much older 
and a bit chunkier but works flawlessly every time. And it still requires to 
specify &quot;-c arduino&quot; instead of &quot;-c urclock&quot; in avrdude when flashing. 

So, what am I going to do with all this after learning all the necessary
basics? Well, I have several plans in mind. The first one is porting one of 
my interpreted platforms to AVR. Since this is a Harvard-type architecture, 
it&#x27;s not a trivial task as you get the program memory completely separated 
from the RAM, and you don&#x27;t get a lot of RAM either. But I have started 
porting my LVTL-R variant of the VTL-2 language to these MCUs and it looks 
promising so far. The second plan is something I&#x27;ve dreamt of a long time 
ago: an independent mobile platform that perfectly fits into my low-powered 
computing paradigm. I have a couple of SIM800Cs and other wireless modules 
to tinker with, so that looks like a perfect opportunity to finally make 
some use of them. And I am learning to work with Nokia 5110 LCD displays, 
character-based LCD displays, some OLED displays and 16-key keypads, so you 
might take a guess where all this is heading.

Finally, I was pleasantly surprised to find out how cheap all this is
nowadays. In the previous decade, even an MCU board like Arduino Nano was 
quite expensive. Now, you can get a full Nano clone with a microUSB or even 
USB-C port for well under $3. I remember the times when you couldn&#x27;t get a 
single MCU chip for such a price. On the other hand, that makes it 
economically impractical to purchase the ATtiny series: the chips alone 
approach the price of these Nanos and sometimes even surpass the price of 
full Arduino-compatible boards with 328P-like chips like LGT8F328P. I hadn&#x27;t 
experimented with those, for now sticking with the original ATmegas only, 
but planning on ordering several of those clones as well. Cost-wise, 
however, there are some strong competitors that give an even better bang for 
the buck: ESP8266 and its clones, and, if you are ready to pay a bit more, 
RP2040-Zero and its clones. Both of these families can even run Python, an 
opportunity I definitely will explore in some of my upcoming posts (as I do 
have some ESP8266 boards lying around as well), both of them have pretty 
indestructible bootloaders, and both of them, especially the RP2040, require 
much less hassle to get your code up and running. And, if and when I hit the 
limit of what could be done on the AVR8 MCU family, I&#x27;ll definitely consider 
those as a more future-proof approach.

--- Luxferre ---

[1]: https://github.com/aleh/usbisp
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-12-02-the-most-optimal-mcu-board-so-far.txt</link>
<title>(2024-12-02) Looks like I've found the most optimal MCU board so far</title>
<description><![CDATA[<pre>
(2024-12-02) Looks like I&#x27;ve found the most optimal MCU board so far
--------------------------------------------------------------------
As my journey into the world of MCUs began, I knew for sure that it wouldn&#x27;t
stop on AVRs. In particular, if you remember my previous post, I mentioned 
some other chips at the end of it, namely ESP8266 and RP2040. Well, since 
then, I&#x27;ve had a chance to obtain some boards based on both of those (all 
clones, of course): ESP-01 (01S), WeMos D1 Mini and RP2040-Zero. I&#x27;ve yet to 
get much more familiar with the latter (I already like how easy it is to 
flash and how small it is in size for what it can do) but I can already say 
which one of these three became my absolute favorite, especially considering 
its price comparable to clone Arduino Nanos. Yes, I&#x27;m talking about the D1 
Mini.

Why? Well, the ESP-01S is definitely the cheapest option if you want pure
processing power with wireless capabilities, but is stripped to the bare 
bones: no USB (you need to use a USB-TTL converter to interact with it), 
pure 3.3V dependency, no guarantee of how much flash memory you&#x27;re gonna get 
(typically 1 MiB but it can vary from 512 KiB to 4 MiB), no ADC or other 
advanced pins that the ESP8266EX chip does have, not even a reset button, 
only two free GPIO pins (one of which also controls the internal LED) if you 
still need UART, and a totally not breadboard-friendly pinout. Not sure I&#x27;ll 
have a lot of situations where such an amount of compromises would justify 
about half the price. And the RP2040-Zero is still cool in almost every 
aspect and has a lot of ADC pins but is priced about the same as the WeMos 
while having no wireless module whatsoever and, again, not being compatible 
with standard Arduino-style breadboards (if you do solder the five pins on 
the side opposite to the USB port, that is).

The D1 Mini, on the other hand, has none of those issues. It is totally
breadboard-friendly, has enough pins for everyday hobby needs (even though 
there&#x27;s only one ADC pin but that&#x27;s what the ESP8266EX itself gives you), 
has an internal USB-TTL adapter and a way to power it not only via USB but 
with external 5V or 3.3V sources, has the guaranteed 4 MiB of on-board flash 
memory and even fixes some drawbacks of the original chip, for instance by 
having a dedicated reset button or a built-in voltage divider to bring the 
3.3V ADC pin input to the allowed range of 0..1V. Also, unlike e.g. NodeMCU 
clones, the clone D1 Mini boards usually don&#x27;t have a lot of (or any) QC 
issues because they are basically manufactured by soldering a ready-made 
ESP-12S (or 12F?) module on top of their custom PCB. Hence, the WeMos serves 
both as a breakout board for the ESP-12 (and ESP8266EX itself) and as a 
quality-of-life enhancer for it by adding all those features for a small 
fraction of the price you&#x27;d pay if you had to take a bare ESP-12 and do all 
this by yourself.

Well, how easy is it to work with? I was interested in operating it with a
higher-level language, and among JS, Python and Lua, I obviously chose 
Python. That&#x27;s why all I needed was a corresponding MicroPython firmware 
binary and the esptool to flash it via USB. Definitely much more 
straightforward process than what needed to be done to interact with ATmega 
MCUs. Well, flashing an RP2040 is even easier, but I&#x27;ll compare the 
MicroPython experience on both in my later posts. For now though, once I 
have MicroPython up and running on a D1 Mini, the only thing I need is the 
ampy tool to move files there. I have some understanding how this tool 
actually works, so I might end up writing my own if I dislike something 
there. As of this moment, I just call ampy as a part of my makefiles. I find 
it quite astonishing that the Alpine Linux repos already have both ampy and 
esptool directly available via the system&#x27;s apk package manager.

And the MicroPython distribution is... well, a full-featured Python. Of
course, it somewhat limits you with the set of libraries you can call (and 
the 1MB build for the ESP-01S limits you even more), but other than that, 
it&#x27;s a &quot;normal&quot; Python 3.4 with some features backported from the newer 
language versions and some differences from the &quot;big&quot; CPython thoroughly 
documented in the official documentation ([1]) which I highly recommend to 
fully read anyway. On top of that, it offers several levels of optimization 
(turned on with decorators) and an mpy-cross tool to pre-compile Python 
modules before loading them onto the device, so it definitely can be made 
not as slow as you probably might imagine. Also, there even is a possibility 
to leverage micropython-lib to install third-party libraries ([2]), similar 
to the way pip works on the &quot;normal&quot; CPython. You can find many useful 
things there, like USB HID/CDC/MIDI drivers or an aiohttp port. Some of 
those libraries even extend the standard MicroPython library to the CPython 
level of features, just in case you really need those in your MCU.

Now, my first plan for the nearest future using these D1 Minis is to build
something like a pocket assistant device that would eliminate the need of 
carrying pretty much any phone around the house. Kinda like a programmable 
pager or something. I&#x27;ve already soldered the first prototype contraption 
(that doesn&#x27;t even have a piezo buzzer) and, of course, am going to refine 
it a lot, but for now I&#x27;m more focused on the software side of things, which 
presents enough of a challenge of its own, given that the device is only 
going to have four buttons in the original design. I&#x27;m also keeping tabs on 
the projects like ESPboy although I&#x27;m not quite at that level yet. Let me 
also remind you that I do have a couple of SIM800C GSM modules lying 
around... and they are waiting for their time to shine as well. 

--- Luxferre ---

[1]: https://docs.micropython.org/en/latest/reference/index.html
[2]: https://docs.micropython.org/en/latest/reference/packages.html
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-12-09-engineers-wristwatch-of-today.txt</link>
<title>(2024-12-09) Probably the best engineer's wristwatch of today</title>
<description><![CDATA[<pre>
(2024-12-09) Probably the best engineer&#x27;s wristwatch of today
-------------------------------------------------------------
As more and more circumstances out there are forcing me to stay home, I asked
myself a question: &quot;which watch out of my current collection would I really 
wear all day long with the least troubles?&quot; To be honest, any of the watches 
in the collection wouldn&#x27;t give me a lot of troubles (otherwise it wouldn&#x27;t 
stay there), but some watches are still more suitable to the current 
lifestyle than the others. Of course, one doesn&#x27;t need a diver&#x27;s or a field 
watch for the activity happening mostly indoors. And this is when I 
remembered about the type of watches that I called &quot;engineer&#x27;s watch&quot;. To 
me, an engineer&#x27;s watch has to meet the following requirements:

1. It must be digital, 24-hour display support is required.
2. It must be suitable for continuous indoors usage: legible enough in any
room and not dependent on the sunlight for functioning. The backlight must 
keep it legible even in total darkness.
3. It must be accurate enough to not worry about it within a month or even
between the DST changes. Of course, auto calendar also is a must-have.
4. It must contain at least one alarm and a stopwatch. Additional alarms and
a countdown timer are desirable but not necessary.
5. It must be as thin as possible to fit under any cuff.
6. It must be as comfortable to wear as possible, without even noticing it on
your hand until you need to look at it.

Up until this moment, I considered Casio A700WEM-7A the king of engineers&#x27;
watches based on the above criteria. I have given mine away some time ago 
but still would like to return to it sometimes. However, as of now, I do 
have an even more modern and functional alternative to it: Casio 
ABL-100WE-1A (later referred to as just ABL-100). I do wish this one came on 
a mesh bracelet too but this one is also just fine. It is a bit thicker than 
the A700 (8.2mm vs 6mm) but, considering all the functionality hidden within 
this case, it still is pretty slim. Besides, it still is slimmer than the 
A168WA (9.6mm) everyone likes to compare it to, even slimmer than A163 
(9.1mm), A164 (8.3mm), F-91W (8.5mm), and is on par with the classic 
A158/A159 models that also have 8.2mm of thickness. So, also given the 
design similarities, I&#x27;d rather consider ABL-100 a modern upgrade to the 
593-module classic series and specifically A158/A159 above anything else. Of 
course, it weighs more as well (60 grams) just because the case is a bit 
larger in other dimensions and has more components packed inside, and the 
bracelet is a bit wider too. But that&#x27;s exactly the kind of weight I got 
used to in the past several years (comparable to the lightest of G-Shocks 
I&#x27;ve had), so it&#x27;s not a problem for me in the slightest.

Regarding wearability, it&#x27;s really cool that Casio continued the trend of
&quot;sliding clasp&quot; for their bracelets introduced in A158..A164 models. It 
really helps to size the bracelet with much more precision and much less 
effort than any other solution. All you really need for this is a small 
screwdriver to lift the clasp clamp off. And the overall bracelet profile 
still remains slim and elegant without having to remove extra links or 
anything. I had read some horror stories about the stock ABL-100 bracelet 
being a hair puller but haven&#x27;t found any major problems like that with it. 
Once some inconveniences did start appearing though, I put on a spare 18mm 
steel mesh strap to the rescue, it&#x27;s even Casio-branded because it was 
pulled from the A700WEM indeed. And an even cooler thing is that this watch 
has drilled lugs, so the strap/bracelet change is easier than ever. You just 
need a round pin of the corresponding diameter, I even have one (a SIM 
removal pin) as a corkscrew addon in my Victorinox Spartan.

Besides the ultraslim case and the mesh strap, another thing that I
remembered in the A700WEM was an extremely crisp and legible LCD display. 
The contrast was much, much better than in any 593-based Casio. Well, I&#x27;m 
glad to say that the display of ABL-100 is just as good. It is important to 
me because, as I said, I&#x27;ll have to use this watch under limited lighting 
conditions all winter long. Another great thing about the display is the use 
of screen real estate. It is common for Casio to include pretty unnecessary 
graphical elements on the displays larger than the usual &quot;module 593&quot; style 
or G-Shock square style. Here, the only graphical element is the horizontal 
scale that occupies the top strip on the screen (along with the power saving 
mode and Bluetooth indicators) and it is pretty functional in every mode. In 
the timekeeping, stopwatch and dual time modes, this is the gauge that 
increases every 3 seconds and fills within a minute, in the timer mode it 
goes backwards with the same pace, in the step counting mode, it shows the 
percentage of steps relative to the goal (adjusted in the settings), and the 
only mode where it does nothing is the alarm menu. I really don&#x27;t mind 
having this strip as it is useful, unobtrusive and doesn&#x27;t take a lot of 
space.

Now, to the functionality. First, let&#x27;s address two elephants in the room:
Bluetooth and the step tracker. To be honest, BLE connectivity was the 
primary reason I paid any attention to this watch, as it presented a perfect 
opportunity to extend the list of models supported by my rcvd.py ([1]) 
utility program. Feature-wise, the Bluetooth module in this watch turned out 
to be similar to one in OCW-T200S, so I added ABL-100 into the same section. 
Contrary to the previous digital models that I owned, this one has the 
connection shortcut by long-pressing the Mode button, not Search. Maybe 
that&#x27;s because the initial pairing setup code is different and I don&#x27;t know 
which exactly (another Casio app disassembly might be coming, who knows). 
Anyway, as of now, syncing time is the only feature I&#x27;ve been really using 
the BLE connectivity for. Although I&#x27;m still thinking about how to turn the 
&quot;find my phone&quot; feature into some kind of a watch-based remote control. As 
for the step tracker though... In the past, I learned not to trust any step 
tracker data the hard way, but in my scenario, it at least helped me 
estimate the overall amount of any physical activity done throughout the 
day. Nevertheless, I still consider adding it here a strange design 
decision: IMO adding just BLE would be sufficient and wouldn&#x27;t lead to such 
a drastic battery life reduction.

Speaking of battery, the manufacturer states that the watch is going to last
for about two years on the stock CR2016 given the following conditions: auto 
time correction with the official smartphone app 4 times per day, alarm once 
(10 seconds) per day, backlight illumination once (1.5 seconds) per day, 
step counting 12 hours per day, power saving mode off. Now, how does this 
apply to me? Of course, I&#x27;m not even going to use the official smartphone 
app, let alone sync with it 4 times per day: I&#x27;m going to sync the watch 
with my rcvd.py once or twice per month at most. I&#x27;m probably going to only 
use the alarm occasionally but use the backlight more than once per day for 
sure. And yes, there is no setting to turn the step counter off completely 
(only the step reminder function), so I don&#x27;t have full control over this 
aspect. I have also disabled the button operation tone and turned on the 
power saving mode, which, according to the manual, turns off the display &quot;if 
it is left unused for approximately 150 minutes between 10 p.m. and 6 a.m.&quot; 
The problem here though is that if the step tracker accelerometer registers 
the movement, it automatically wakes up the display, so I&#x27;m not sure how 
useful this setting will turn out to be as I almost never take the watch off 
my wrist, even when going to sleep. Anyway, still much better than the Nokia 
Steel watch (yes, I even had that one at some point!) where the movement and 
the step counter alone ate the entire battery in just 6 months.

All the other functionality of ABL-100 is quite trivial but still a step up
from the A700: dual time (with a quick-swap with home time feature), 5 
alarms, hourly time signal, a stopwatch for up to 24 hours with up to 200 
split time/lap time records and a countdown timer for up to 60 minutes. I&#x27;d 
say this feature set is quite sane, but again, it is a bit baffling as to 
why Casio placed their bet on the sports-oriented functions in a 
vintage-design watch and sacrificed the precious memory for 200 lap records 
instead of, say, world timezones or text reminders. Maybe they plan on 
releasing another model in the ABL series and we&#x27;ll see such features there, 
who knows. BTW, on the &quot;Wrist Action&quot; YT channel, I have seen the comparison 
of this one with the GD-B500 model, and the only added feature there is 
auto-backlight, so we probably have some variations of the same firmware 
already (and this allows to safely add the GD-B500 support into the same 
section of rcvd.py as ABL-100). As for me, I&#x27;m quite happy about even having 
the most basic stopwatch and the most basic timer in there, but I can 
theoretically see an engineer doing some timing-based experiments and 
comparing the results in the record memory of the watch. It is also 
interesting that the manufacturer claims +/- 15 sec/month maximum deviation 
without the BLE sync, which is twice more accurate than the A700, A1000, 
W-800H and the module 593 based models. I hope this remains the go-to 
standard for digital Casios going further, even for the more budget-friendly 
ones.

Another interesting thing about this watch is its test screen, which is
entered by pressing Adjust, Mode and Search at the same time. The screens 
are then switched by pressing the Search button. After the three usual 
segment test screens (and the &quot;all-segment&quot; test screen is the third for 
some reason, not the first), you get a clear screen, then the &quot;FC&quot; screen 
(mine says 01 here), then the module/firmware screen (mine says QW 141 at 
the top and then 3565 at the bottom, which matches the module number and 
proves that this model is indeed the first one on this board), then the ROM 
version screen (mine says 01 again), then the &quot;FIL&quot; screen (mine says 001), 
then the &quot;AIR&quot; screen (mine says 01), then the &quot;DST&quot; screen (mine says 20), 
then the &quot;DI&quot; screen (mine says 06), then the &quot;FV&quot; screen (maybe internal 
firmware version, mine says 17), then the &quot;FI&quot; screen (mine says 03), and 
finally, the &quot;FC&quot; screen (mine says 01). I feel like there&#x27;s a lot of 
deciphering of all this information to do, but I haven&#x27;t seen so many 
test/info screens in any other digital Casio watch. And, unlike many other 
models, you can use the Light button while in a test screen, it won&#x27;t do 
anything else.

Lastly, as for the price of all this... Well, I&#x27;m not tracking the current
prices of GA-B001 and DW-B5600 but I think this one still is the cheapest 
BLE-enabled Casio to date. And it certainly is the slickest looking of all 
BLE-enabled Casios (if we don&#x27;t count the MRG and Oceanus series, of course) 
and definitely THE slickest looking one with a step tracker if you really 
need this function. And, for this price, it definitely doesn&#x27;t qualify as a 
cash grab, compared to e.g. A1100 (which is almost twice as expensive and 
doesn&#x27;t even have a 24-hour time display mode, sic!) or the fashion limited 
editions of DW-5600 or GA-2100. Of course, those who could live with a 
chrome-plated resin case in A158/A159/A163/A164/A168/A700 can live with it 
here too: one doesn&#x27;t just get a full steel (let alone titanium) build in a 
vintage-styled watch. Neither does one get water resistance rating higher 
than 3 bar. Nor does one get solar charging. Those, besides the stock 
bracelet being not as comfortable as I thought it would be, are my only 
possible complaints about this model, but they all would be valid if I chose 
this as a single watch for the rest of my life. However, I already have one, 
and I did a post about it. This one I do have, because I still have a choice 
of more than one watch. And with the A700&#x27;s mesh strap, it already seems to 
be perfect for the use cases I described in the beginning of this post.

By the way, I have also updated the &quot;Watch collection&quot; file on my main Gopher
page (yes, the Citizen AS2050 and Casio MQ-24 were also given away). Now all 
watch models are just sorted alphabetically, and I think the ABL-100 
deserves the first spot in this table, on my wrist and eventually my watch 
roll as the classic digital reference with some modern features mixed in. 
Given all circumstances, I really doubt this file is gonna be updated yet 
again anytime soon but, as I already said before, I am not planning to 
expand my collection to more items and am really enjoying the items I 
already have. 

--- Luxferre ---

[1]: https://git.luxferre.top/rcvd-py/files.html
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-12-16-two-more-casios.txt</link>
<title>(2024-12-16) Two more digital Casios flying under the radar</title>
<description><![CDATA[<pre>
(2024-12-16) Two more digital Casios flying under the radar
-----------------------------------------------------------
As much as I like my Casio GMW-B5000D, I find it almost unwearable in my 24/7
mode. That&#x27;s because of its full-steel construction and corresponding weight 
of about 150 grams. This is the same reason I&#x27;m never going to buy the 
Casiotron reissue (TRN-50) which, by the way, contains virtually the same 
module inside. And the MRG-B5000D doesn&#x27;t cost the amount of money that I&#x27;d 
be ready to give for merely a watch anytime soon. But an idea of a 
metal-core G-Shock with a proper screwback case that&#x27;s not as heavy still 
couldn&#x27;t escape me... until I found the GW-5000U.

Compared to the GMW-B5000D, the GW-5000U loses Bluetooth connectivity while
retaining its solar charging and the longwave time signal receiver, but 
there&#x27;s a reason for that. You see, a long time ago I also owned a GW-M5610, 
one of my first longwave-enabled G-Shocks and the first fully digital one 
with that feature. It appeared on the market in 2011. Even before that, in 
2009, Casio released the GW-5000 model that mimicked the original DW-5000C 
design and had a steel core and a proper screwback case, but other than 
that, was identical to the GW-M5610 and housed the same module 3159. So, 
essentially, GW-M5610 was a pleb variant of the fully Japanese-made GW-5000. 
Well, when Casio &quot;upgraded&quot; the GW-M5610 to GW-M5610U in 2021, guess what 
they released in pair with that? GW-5000U on the same module 3495. By the 
way, I don&#x27;t really get what the &quot;upgrade&quot; consists of besides changing the 
backlight from EL to LED (don&#x27;t have a lot of complaint though), but the 
original GW-5000 is extremely hard to find nowadays, so I&#x27;m fine with this 
one. It looks classy, the strap is much softer than the usual (and even the 
strap here is made in Japan) and it&#x27;s overall more pleasant to wear than the 
M5610, both versions of which, by the way, have been getting a lot of praise 
by &#x27;murican reviewers because, you know, WWVB station.  

Speaking of stations, I have also written (and specifically tested on this
watch) another time signal emulation and transmission utility, this time for 
Chinese BPC. Hence, I called it Beepy ([1]). Well... it beeps indeed! Its 
code is mainly based on my jjy-py ([2]) code, but the amount of differences 
proved to be enough to shape it into a separate program. Just like with 
JJY40, you can just enter the receiver test mode (by the way, you could do 
this on the original M5610/GW5000, AWG-M100B and all other multiband-6 
G-Shocks) by pressing and holding first Light, then Receive/Set and then 
Mode button. Scrolling with the Receive button, you can get to the &quot;B 01&quot; 
screen and then press Light to start the reception. And that&#x27;s it. The 
transmitter must fully emulate the original BPC station, i.e. transmit the 
time in UTC+8. Once the signal has been received, the watch will 
automatically translate it into your current time zone. For JJY40 (&quot;J 40&quot; in 
the test menu), the transmitted time must be in UTC+9 respectively. And my 
jjy-py and Beepy programs do take care of that.

Other than the outer look and full-steel inner case, the GW-5000U is almost
too mundane: auto-backlight, ability to switch date and month position, 
multi-language day of the week, non-reprogrammable world time, 5 alarms and 
an hourly signal, 24-hour stopwatch and a 24-hour countdown timer. No other 
bells and whistles present, not that anyone needs them, but for that 
price... Here, I&#x27;m going to present another Casio watch which is much 
cheaper and comes with a much stiffer polyurethane strap, has a usual 
four-screw backplate, a &quot;normal&quot; lithium battery instead of solar charging, 
has a much more subtle history and is not even a G-Shock, being only 
classified by Casio as belonging to the &quot;Sports Gear&quot; series. Yet, despite 
having no solar charging or longwave or Bluetooth sync, it has no shortage 
of bells and whistles to my taste. I&#x27;m calling it &quot;a poor man&#x27;s Protrek&quot; 
because that&#x27;s what it essentially is. Lo and behold... Casio SGW-100.

Now, that&#x27;s a watch I definitely wouldn&#x27;t wear 24/7, at least not on the
stock strap. Besides, in order to properly use the thermometer function, you 
need to take it off your wrist for up to 20 or even 30 minutes, so yeah. The 
thermometer and the digital compass (based on the built-in magnetometer) are 
the two features Casio calls &quot;twin sensor&quot; in this watch. However, it&#x27;s not 
as large as I thought it would be, and definitely looks larger than it 
really is. In fact, it even is 0.3mm thinner than the GW-5600U! I was also 
really pleased to see this model have a real alumunium bezel, as well as 
aluminium strap hinges. On the other hand, this means that not every strap 
is going to fit this model. There surely are some aftermarket adapters but 
this is something I&#x27;m only going to worry about if I decide this watch stays 
with me for a long time of day-to-day usage.

Another notable thing of the &quot;poor man&#x27;s Protrek&quot; is its backlight. It&#x27;s
non-adjustable (1.5s only) but electroluminescent (and yes, I can hear some 
high-pitched noise if the watch is close to my ear when activating it) and, 
unlike e.g. DW-5600E and other &quot;traditional&quot; EL-equipped Casios, this one 
has a twist: only the active segments light up here. Yes, even on a positive 
display. I don&#x27;t know how they achieved it but now I feel like this is the 
only way EL should be implemented anywhere. Otherwise you really can get 
away with LEDs, no problem. Of course, there is no auto-backlight in this 
model because it would eat the battery too fast. G-2900F, for instance, did 
have it but it also did have a 10-year battery life claim. SGW-100 only 
claims 3 years on a CR2025, so who knows? What&#x27;s no less interesting is that 
the test screen shows the 3157-07 firmware version. The 3157 is indeed the 
module number, but 07? Where did the other 6 revisions go? Well, on the 
GW-5000U, the test screen also shows the 3495 module with the 003 revision 
number. It also has some submenus similar to what was seen in ABL-100, 
although not as much: ROM (01) and FILE (F001). This hints at a bigger 
platform lying underneath the surface that we surely don&#x27;t know a lot about 
yet.

With the SGW-100 representing an older generation, some things are not so
obvious. For example, button operation tone can be toggled with a really 
long press of the Mode button. And the temperature unit setting (C/F) is 
inside the time/date adjust menu. The compass calibration procedure also 
seems a bit counterintuitive at first, but one can get used to it. Anyway, 
the &quot;golden standard&quot; feature set of world time, 5 alarms and an hourly 
signal, a stopwatch and a countdown timer (for 24 hours too) is present here 
as well, which, combined with a true EL, makes it a decent competitor to the 
AE-2000, AE-1500 and other similar Illuminators that don&#x27;t have anything 
else besides this or an even more modest feature set (the only thing they 
really can bet on is their battery life). On the other hand, if you don&#x27;t 
need a built-in barometer/altimeter, why would you choose a (non-solar) 
Protrek over the SGW anyway? I think this is one of the models where Casio 
deliberately blurs the line between Protreks and Illuminators, naming it 
neither of them. Oh yeah, did I mention that SGW-100 claims 20 bar of water 
resistance? That makes it closer to the AE-2000 even more. And the overall 
construction puts it somewhere in between the Illuminator, G-Shock and 
Protrek series too. As someone told, it could have been a G-Shock but then 
they would sell it for twice the price just because of the branding.

So, what is the target audience of both of these timepieces, who are they
intended for? Well, I think the GW-5000U is intended for two categories of 
people: avid Casio collectors and those who, on the contrary, want to only 
purchase one watch in the category (or even a single watch at all) and 
expect it to serve them for the rest of their lives. Either way, this is not 
a casual type of people. In contrast with this, the SGW-100 is as casual and 
utilitarian as it can get: it doesn&#x27;t promote longevity, heritage or 
materials, it&#x27;s pure function over form. There is, however, something common 
between these two watches besides being all-digital and made by Casio. While 
there is an increasing number of Casio watches made as companions to other 
gadgets (the aforementioned GMW-B5600D and ABL-100, the GM-B2100, the GBD 
series and so on), these two definitely are aimed at fully autonomous usage. 
Their functionality is fully available even to those who don&#x27;t have anything 
to connect them to, or just don&#x27;t want their watches to be connected to 
anything. In fact, I can imagine a scenario where either of these watches is 
the only portable digital gadget one has around. Of course, the GW-5000U 
with its solar charging would be preferable in this case. Moreover, as the 
world delves into a dystopia faster and faster, not having a single 
transmitting device on you becomes quite essential for escaping global 
surveillance.

--- Luxferre ---

[1]: https://git.luxferre.top/beepy/file/README.html
[2]: https://git.luxferre.top/jjy-py/file/README.html
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-12-23-fed-up-with-the-industry.txt</link>
<title>(2024-12-23) Fed up with the industry</title>
<description><![CDATA[<pre>
(2024-12-23) Fed up with the industry
-------------------------------------
Seriously, I&#x27;m sick and tired. The mainstream IT industry is a total mess now.

Yes, I say that. It stopped solving real problems long ago, it stopped making
the world a better place long ago, it started being &quot;a thing in itself&quot; with 
all the &quot;innovations&quot; made up to merely justify its own further existence. 
And it&#x27;s not just because a new fad appears every 3 to 5 years, which in 
turn just adds to an already long list of totally useless and, in the long 
run, even harmful things that simply must die. No, the main issue with that 
is being able to produce at least some value for the company (or even the 
entire industry) no longer equals to being able to produce any value for the 
world&#x27;s people. In fact, in some cases, NOT producing that value for the 
companies (especially big corporations) ends up being more useful for the 
society.

Of course, big corps like to tell everyone how they value every employee and
contractor of theirs, while telling the employees (and especially 
contractors) at the same time that no one is irreplaceable and ending up 
treating them like their own property. Been there, seen that. On top of 
that, regardless of the company size, there&#x27;s a clear observation made 
throughout the 12 years in the field: the more your management (up to the 
very top) starts talking about &quot;company values&quot;, &quot;teamwork&quot;, &quot;commitment to 
the success&quot;, &quot;devotion to perfect delivery&quot; and other similar BS, the more 
likely it is that you&#x27;re not going to get a raise anytime soon or even get 
your current salary in time. Because all that is just a fancy way of 
shifting responsibility to the innocent workers and saying &quot;we want y&#x27;all to 
work twice as more while you&#x27;ll get nothing extra&quot;. This is how the VAI 
syndrome is cultivated en masse, by the way.

What&#x27;s VAI, you may ask? Nothing to do with AI (more on that later though),
it&#x27;s short for Vigorous Activity Imitation. A popular acronym where I&#x27;m 
from. And it&#x27;s not just IT-specific, it&#x27;s the source of problems in all 
sorts of areas. The IT sector is just one of those where most of VAI really 
can go undetected. Everyone there imitates vigorous activity to some extent 
because, under the corporate pressure, what you really do becomes not as 
important and even irrelevant compared to what you show that you&#x27;re doing. 
All those &quot;daily standups&quot;, &quot;sprint planning meetings&quot;, &quot;quarterly planning 
meetings&quot;, &quot;task grooming sessions&quot;, &quot;tech demos&quot;, &quot;all-hands&quot;, &quot;performance 
reviews&quot;... I might have missed a few but you get the idea. Any sane person 
would think all this exists for the sole purpose of wasting everyone&#x27;s time 
and energy, but not the corporate zombies who desperately need this to 
fulfill their VAI goals. And then there is a whole lot of positions just for 
serving this VAI infrastructure: project managers, scrum masters (tell me a 
single reason why a team leader can&#x27;t do the task management job), delivery 
managers and who knows who else. Not so long ago, I was on a project where 
there were far more managers than the actual engineers. And everyone seemed 
to be fine with that. Good old bureaucracy in a new shell.

Needless to say, what you see is very far from what you get at the end of the
day. No one there is actually motivated to write efficient and bugless code, 
it&#x27;s enough for it to be able to pass the tests and to work on the tech 
demos. No one there is actually motivated to build secure infrastructure 
unless/until they explicitly are told to do so. Combined with the moronic 
management that just can&#x27;t know any better than installing Faildows on 
mission-critical systems, no wonder that the disasters like WannaCry or 
CrowdStrike had such a success. And I&#x27;m afraid that&#x27;s just the beginning. 
Because the old suits are slowly but surely getting replaced with 
TikTok-brained dummies who are even more clueless but think they already 
know everything and keep jumping onto every new fad without a second 
thought. And the most recent plague they let into the business, GenAI, 
already has started showing its fruit with serious data breaches and leaks. 
And we&#x27;re talking huge amounts of data being processed every second. So, at 
this point, I think it&#x27;s too late to talk about potential leaks, what could 
happen already has, we can only talk about those that have been discovered 
and those that haven&#x27;t. And I believe that the amount of undiscovered leaks 
is much larger than the number of ones we know about. Even when a company 
uses third-party AI to automate its VAI (see the connection?), everyone&#x27;s 
privacy is already at risk: employees, contractors and clients. And the 
unsuspecting public is the one who usually suffers the most.

Given all that, as well as my local situation making it increasingly
uncomfortable to continue conducting any business here legally, I&#x27;m planning 
to quit my job at some point in the coming year. As of now, I&#x27;ve got enough 
savings for the next 3 to 5 years, and I&#x27;m sure I&#x27;ll figure out what to do 
afterwards, what matters today is just staying alive. You may call it a 
burnout, but &quot;better to burn out than fade away&quot;, as the song goes. Not that 
anyone is going to miss me or even notice that, but I don&#x27;t want to be a 
part of all that anymore. It is, in my opinion, wiser to watch this bubble 
burst from afar.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2024-12-30-the-end-of-the-year.txt</link>
<title>(2024-12-30) The end of the year is finally nearing</title>
<description><![CDATA[<pre>
(2024-12-30) The end of the year is finally nearing
---------------------------------------------------
Not gonna lie, this year has been the worst in the decade so far and one of
the most difficult years of my entire life. Many factors &quot;contributed&quot; to 
this and the future never has been more uncertain for me, but I still have 
got some strength to keep going. I have created several new tools, got an 
entirely new hobby and some fresh plans for the next year. The main mission 
though is staying alive.

Some of my collections have been significantly changed. For instance, I&#x27;ve
made no less than 8 watch reviews on this phlog in 2024, while keeping the 
collection under 20 items. As of this moment though, at the end of this 
year, I&#x27;m wearing one of the simplest and cheapest digital Casios currently 
available: the W-218H. You could say it&#x27;s technically identical to F-91W or 
A700 but it has larger digits and much better display viewing angles (on par 
with the GMW-B5000D, no kidding!), better backlight (well, on par with the 
A700) and, most importantly, a more ergonomic 3-button layout, with the 
Light/Advance button being in the bottom right and the Start/Stop/12/24h 
button being in the top left. My main complaint about the 593-module watches 
and the A700/A1000 was that sometimes I accidentally pressed the bottom 
right button and switched the time to the 12-hour format in the timekeeping 
mode without noticing it. Well, it&#x27;s just not possible anymore with this new 
button layout: the worst that can happen is backlight activation. And it 
doesn&#x27;t have afterglow, so accidental presses won&#x27;t consume much energy. By 
the way, the buttons here are very easy to press, even easier than on some 
other Illuminators, so e.g. turning hourly chime on and off is a breeze on 
this one. On top of that, W-218H is extremely lightweight for its size and 
look (remotely resembling the vintage DW-5300) and I really stop noticing it 
on my wrist from time to time. This is what I call &quot;zero-distraction 
timekeeping&quot;. So, in a month, I&#x27;ll tell you how accurate this watch turns 
out to be. And yes, it also has the LCD test screen when you press all three 
buttons together and the 593-like &quot;CASIo&quot; screen when you press and hold the 
Start/Stop button. Neat.

As for my other hobbies (MCUs, poetry, music making)... Not that I&#x27;m out of
ideas but definitely out of inspiration. For instance, I do have enough 
parts to start building something based onWemos D1 Mini, but I still haven&#x27;t 
shaped the vision of what exactly I want it to do. Same for the ATMegas and 
for the RP2040. I&#x27;m sure I&#x27;ll figure this out but it needs time. What I 
recently have achieved significant progress in though, is my Pixel research. 
I don&#x27;t want to talk about this progress just yet but you&#x27;ll definitely see 
the first practical results of this research within several months. To make 
those results more tangible, I&#x27;m going to dive into some Android development 
too, namely using the Go language and the Fyne GUI framework I already told 
you about some time ago, just because I can&#x27;t stand Java but still want my 
future app to be able to work without any kind of middleware like AndroWish. 
There&#x27;s a lot of effort ahead but the end goal surely is worth it. It has 
the potential to have as big of an impact as the first KaiOS jailbreak in 
2018, if not even bigger. And I&#x27;m slowly approaching this moment, step by 
step.

Overall, I surely hope the next year will allow me to escape the
circumstances and bring back some light and inspiration into my life. 
Staying strong as always.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2025-01-06-fully-javaless-android-dev.txt</link>
<title>(2025-01-06) Fully Java-less Android development on... Android itself</title>
<description><![CDATA[<pre>
(2025-01-06) Fully Java-less Android development on... Android itself
---------------------------------------------------------------------
As I already hinted in my last post of 2024, I was going to dive into some
Android stuff. And the things I have already learned actually turned out to 
be very friendly if you know what you&#x27;re doing. But one question still 
bothered me: could all this be done without any PC at all, on the tablet or 
even the target smartphone I was developing my tool for? Well, the answer is 
yes. While things like AndroWish definitely are cool and somewhat 
liberating, the ability to develop standalone GUI applications and compile 
them into ready-made .apk packages on the device itself is another level of 
self-sustainability. So, today, I&#x27;m going to tell you how this can be done 
without having to write a single line of Java code or XML markup. The only 
caveat as of now is that all this is only fully possible on the 64-bit ARM 
architecture (aka aarch64). Well, although I don&#x27;t usually advocate for 
switching to new hardware, moving to a 64-bit CPU might be a worthy upgrade 
after all.

So, what do we need to setup our development environment for Android on
Android? First and foremost, an independent Linux terminal with package 
management. Until one is shipped within Android itself (rumors are it&#x27;s 
going to happen in Android 16), the most viable option is Termux. This is 
where all the fun is going to happen. Of course, I&#x27;m going to skip some 
necessary steps like pkg update, pkg upgrade and termux-setup-storage 
commands, but the gist of it is that you&#x27;re getting a Debian-like 
environment in the Termux app sandbox, where you can set up packages like in 
a normal Debian distribution. Then, of course, you need to have a text 
editor suitable for coding inside Termux. I personally always choose Vim but 
your mileage may vary. In case you are not using a physical keyboard, a good 
virtual one is also a must (my personal recommendation is Unexpected 
Keyboard from F-Droid). Now, let&#x27;s get to the actual programming tooling we 
need to install.

For the task of writing Android GUI software, we&#x27;re going to use the Go
programming language and the Fyne framework. Go itself is extremely simple 
to install and use in Termux (just specify the golang package), but we also 
need to have something like Git, Make and, most importantly, Android NDK to 
be able to use Fyne. Now, Termux only ships library helpers for Android NDK 
but not the NDK itself. You need to download it from a third-party repo and 
then fix some paths inside the executable scripts (with the 
termux-fix-shebang tool provided via a separate package), as well as 
populate the ANDROID_NDK_HOME and ANDROID_NDK_ROOT environment variables in 
the Termux shell profile. So, the first step can be summarized as follows:

cd
pkg update
pkg in termux-tools android-tools golang git make unzip wget
wget
https://github.com/lzhiyong/termux-ndk/releases/download/android-ndk/android-
dk-r27b-aarch64.zip
unzip android-ndk-r27b-aarch64.zip
termux-fix-shebang
$HOME/android-ndk-r27b/toolchains/llvm/prebuilt/linux-aarch64/bin/*
rm android-ndk-r27b-aarch64.zip
echo &#x27;export
ANDROID_NDK_HOME=/data/data/com.termux/files/home/android-ndk-r27b&#x27; &gt;&gt; 
$HOME/../usr/etc/profile
echo &#x27;export ANDROID_NDK_ROOT=$ANDROID_NDK_HOME&#x27; &gt;&gt; $HOME/../usr/etc/profile

Now, before actually installng Fyne, we need to extend our $PATH in the same
place:

mkdir -p $HOME/go/bin
echo &#x27;export PATH=$PATH:/data/data/com.termux/files/home/go/bin&#x27; &gt;&gt;
$HOME/../usr/etc/profile

Finally, we can install Fyne according to the official instructions:

go install fyne.io/fyne/v2/cmd/fyne@latest

Then, you can exit and enter the terminal or just run &quot;source
~/../usr/etc/profile&quot; to apply all the path changes and check whether the 
fyne command is working. If everything is fyne (lol), your environment is 
ready and you can build your first application.

Of course, I won&#x27;t get into details of writing Fyne applications in this very
post, just want to mention that this process isn&#x27;t much different from 
writing them on desktop, unless you need to use some platform-specific APIs. 
What&#x27;s more important is the building step. In order to build and package 
your Fyne application on Android, you prepare the icon file (called Icon.png 
by default) and then run this in your project directory:

go mod tidy
fyne package --os android/arm64 --appID [your Android app ID] --name
[launcher name] --release

It&#x27;s usually convenient to wrap this into a Makefile, that&#x27;s what I normally
do. You can specify additional parameters like version and build numbers, it 
will default to version 0.0.1 if you don&#x27;t do so. You can also replace 
&quot;android/arm64&quot; with just &quot;android/arm&quot; if you want to build the app for 
32-bit ARMv7a CPUs and it will take about the same size (at least 23 MiB in 
the release mode). If you specify just &quot;android&quot; though, it will increase 
the minimum .apk size to the whopping 97 MiB, because Fyne will 
cross-compile the code for all supported architectures at once: ARMv8a 
(aarch64), ARMv7a, x86 and x86_64. So, if you don&#x27;t embed any non-Go native 
binaries and want to achieve maximum compatibility without having to create 
various .apk files for various architectures, you can choose this option at 
the cost of quadrupling the package size. Yes, you still need to host the 
build process on a 64-bit ARM device but that doesn&#x27;t prevent Fyne from 
cross-compiling your binary to other platforms if you instruct it to do so.

Now, how do we install the freshly compiled APK package file? You can do this
in several ways:

1) copy the file into the outer storage (you should have set up the storage
with termux-setup-storage command) and then install it using your favorite 
file manager (you can also try using termux-open but I didn&#x27;t have a lot of 
success with that on my GrapheneOS installations),
2) if you have root access, run su -c pm install /path/to/program.apk,
3) install the android-tools Termux package (mentioned above), enable
Wireless debugging in your Android developer options, then run adb connect 
127.0.0.1 and just use adb install /path/to/program.apk as if you had been 
installing the package from a PC. With this option, you can even directly 
install your program to other Android devices, either wirelessly or with an 
OTG cable if your device supports it.

Well, that&#x27;s it. I hope this guide helps someone stranded on Android only to
start creating and not just consuming content on such devices. Have fun in 
the new year!

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2025-01-13-making-genai-less-horrible-llamafile.txt</link>
<title>(2025-01-13) Making GenAI less horrible for the rest of us (with llamafile)</title>
<description><![CDATA[<pre>
(2025-01-13) Making GenAI less horrible for the rest of us (with llamafile)
---------------------------------------------------------------------------
&quot;Wait, what? Did ye olde Lux sell out to the hype and hoax?&quot;
No, not really. I still hold to my point that generative AI, in its current
mainstream state, is a plague of the tech industry that&#x27;s going to worsen 
the overall situation over years. But, and there always is a &quot;but&quot;, there 
seems to be a way of actually make this technology serve the people, not 
megacorps. Even though the very first iteration of what I&#x27;m going to talk 
about was created by megacorps themselves.

As someone located on pretty much the opposite end of the computing power
spectrum than your average hype-riding techbro, I tried to stay away from 
the generative AI topic for as long as I could. After all, remote LLMs 
definitely are a privacy nightmare, even if they state otherwise 
(surprisingly enough, I started digging deeper into the topic once I saw the 
DuckDuckGo&#x27;s &quot;AI chat&quot; and an unofficial Python-based CLI interface for it), 
and local LLMs **usually** require the hardware that&#x27;s too power-hungry (not 
to mention expensive) for my taste. But then, I stumbled upon something that 
solved both problems at once: a set of relatively small but capable language 
models AND a tool to run any of them as a server or even a purely 
terminal-based chat without needing any dedicated GPU, completely on CPU and 
RAM (and not a lot of it, in fact). So, after years of deliberate silence 
about LLMs, I finally decided to give them a shot.

First, let&#x27;s talk about the tool. Although the current chitchat is all around
Ollama, I found it to be too inconvenient for some use cases. I also 
considered using bare llama.cpp but it has too many moving parts that I 
can&#x27;t handle just yet. Maybe the next time. So, I settled upon Mozilla&#x27;s 
llamafile ([1]), which is a very convenient wrapper around llama.cpp that 
can be distributed as a single binary file across multiple OSes and even 
architectures (x86_64 and ARM64; that&#x27;s why it weighs over 230 MB, by the 
way). The full llamafile toolkit, if you want to, even allows to embed a 
model file and distribute the entire thing as a single executable blob, 
which is how I tried it out at first, that is, until I realized there are 
much more model files than there are ready-made .llamafile executables.

Since llamafile is based upon llama.cpp, it consumes the same model file
format (GGUF) by specifying the file via the mandatory -m flag (well, it&#x27;s 
mandatory unless you run a prebuilt model blob), but we&#x27;ll get to that 
format later. What matters now is that it can run in three modes: terminal 
chat (--chat option), non-interactive CLI (--cli option) or a Web server 
(--server option). If none of these three options are specified, it will run 
in the terminal chat mode while also enabling the local Web server at the 
8080 port on the 127.0.0.1 address only (which, of course, you can override 
with the --port and --host parameters respectively). On one hand, the 
default server UI might not look appealing to someone, on the other hand, 
the very same server (also provided by llama.cpp) offers a rich set of APIs 
([2]), even including OpenAI-compatible ones, which allows you to use the 
same client libraries and applications that you got used to with the 
proprietary models (LibreChat being the most obvious FOSS example). I can 
already see how this can be used to set up a private LLM server in my LAN 
based on one of my RPi5 machines. Besides the server mode though, llamafile 
allows you to do all kinds of awesome stuff you can read in the &quot;Examples&quot; 
section of its own help (--help option). Also, if the RAM allows, don&#x27;t 
forget to pass the context size in the -c option (you can check the maximum 
context size with the /context command in the chat once the model is 
loaded). You can also set active threads with the -t option (if you don&#x27;t 
specify it, it will use half the available CPU cores). And, by default, it 
doesn&#x27;t use GPUs at all. If you have a dedicated GPU and need to offload 
processing to it, you have to set the -ngl parameter to a non-zero number. 
Well, I don&#x27;t even have a way to test this with a dedicated GPU, but I was 
quite pleased as to how fast it works without it, but it surely all comes 
down to what kind of model you try to run. By the way, you can get the 
model&#x27;s processing speed (in tokens per second) by running the /stats 
command (after evaluating your prompts) and looking at the last column in 
the &quot;Prompt eval time&quot; and &quot;Eval time&quot; rows.

And if you&#x27;re already intrigued, here&#x27;s an alias I created after putting the
llamafile binary to my $PATH, so that I only have to add the -m and 
(optionally) -c parameters:
alias lchat=&quot;llamafile --chat --no-display-prompt --nologo --fast -t $(nproc)&quot;

Now, let&#x27;s talk about the models. Note that I&#x27;ll only talk about text-only
models (we&#x27;re on Gopher, after all), and I&#x27;ll talk about them from the 
end-user perspective, not how to train, compile or convert them. As I 
already mentioned, llamafile consumes models in the GGUF format, which 
stands for GPT-Generated Unified Format and is native to the current 
llama.cpp versions. Just like with any other format, various model files can 
be found on the Hugging Face ([3]) repository portal, which is kinda like a 
GitHub for AI models of all sorts. I won&#x27;t get into all sorts of specifics, 
but what matters most when looking for a model is its parameter size 
(usually measured in millions or even more often in billions: e.g. a 7B 
model is a model with around 7 billion parameters) and the quantization 
level. Let me quickly explain what that means. The &quot;source&quot; neural network 
weight values are stored as 32-bit or even 64-bit floating point numbers. 
This gives the best accuracy but takes a huge amount of space and requires a 
lot of processing power to deal with. That&#x27;s why, when converting the model 
to the GGUF format, those weights are often quantized, i.e. converted to 
16-bit floating point numbers or, more often, integers that can be much 
easier processed by the CPU and take much less space in RAM, at the expense 
of reducing the model&#x27;s precision. The quantization level is usually marked 
by the letter Q and the number of bits in the integer, following by the 
algorithm marker if the quantization is non-linear (again, I don&#x27;t know a 
lot about that part yet). So, Q8 means that the weights were converted to 
8-bit integers, Q6 means 6-bit integers and so on. Strangely, there is Q3 
and Q5 but no Q7. But I should note that lower quantization only works well 
with relatively large models. Provided you have enough storage space and 
RAM, it doesn&#x27;t make a lot of sense to choose the model files with less 
precise quantization over something like Q8 for 2B parameters or less, as 
it&#x27;s the number of parameters that determines the inference speed for the 
most part, not the size of a single integer weight. 

So, which models worked well with llamafile on my &quot;potato-grade&quot; hardware? By
&quot;worked well&quot; I mean not only being fast, but also producing little garbage. 
So you won&#x27;t see e.g. Gemma 2 2B, as it&#x27;s too large, slow and cumbersome on 
this hardware. Some models (e.g. TinyLlama) only seem to work as intended in 
the server/API mode but not in the llamafile&#x27;s terminal chat mode (no matter 
what chat templates I tried selecting), so I won&#x27;t include such models 
either. Lastly, there are some models that are just not supported by 
llamafile yet, including but not limited to Granite3 and Falcon3. Which is a 
shame, you know: I had tested Granite 3.1 MoE 1B and Falcon3 1B on Ollama 
and bare llama.cpp and had great experience with them, especially Granite. I 
hope Mozilla adds their support to llamafile soon.

All the models that I looked at were subject to two basic tests: counting the
amount of the &quot;r&quot; letters in the word &quot;strawberry&quot; and writing Python code 
to perform Luhn checksum calculation and checking. If it passes both tests, 
I also ask it what 23 * 143 is, and as an advanced task, ask them to &quot;write 
a true crime story for 10-minute narration, where the crime actually got 
solved and the perpetrator got arrested&quot;. For the models that work for me at 
least to some extent, I&#x27;ll give the general names as well as the exact file 
names and their sizes (from my ls -lah output) for you to be able to look 
them up on the Hugging Face portal and try them out yourselves. Let&#x27;s go!

1. Llama 3.2 1B (Llama-3.2-1B-Instruct.Q8_0.gguf,
Llama-3.2-1B-Instruct-Uncensored.Q8_0.gguf, both 1.3G, max context size 
131072). The only thing originally created by Meta that I don&#x27;t really hate. 
Very impressive for its size. The official version is extremely good at 
storytelling. The uncensored version helps with some things (i.e. also 
mentions IMEIs when asked about the Luhn algorithm). Both versions know how 
many letters &quot;r&quot; are in the word &quot;strawberry&quot; and how to code Luhn in Python 
(which is my minimum passing limit for any &quot;serious&quot; LLM) but overall are 
not very good at coding tasks. Which brings us to...
2. Qwen 2.5 Coder 1.5B (qwen2.5-coder-1.5b-instruct-q8_0.gguf, 1.8G, max
context size 32768). Created by Alibaba Cloud and is, as the name suggests, 
tailored for coding tasks (while being unable to multiply 23 and 143 at the 
same time, lol). Runs quite slower than the Llama and produces redundant 
code at times, but overall, not so bad.
3. Qwen 2.5 Math 1.5B (Qwen2.5-Math-1.5B-Instruct-Q8_0.gguf, 1.6G, max
context size 4096). The same Qwen 2.5 variant but tailored to being able to 
multiply 23 and 143, it seems. Also tries to show the reasoning behind 
everything. Missed the letter &quot;b&quot; and the third &quot;r&quot; in the word &quot;strawberry&quot; 
though: &quot;The word &quot;strawberry&quot; is composed of the letters: s, t, r, a, w, e, 
r, y.&quot;
4. Athena 1 1.5B and AwA 1.5B (athena-1-1.5b-q8_0.gguf, awa-1.5b-q8_0.gguf,
both 1.6G, max context size 32768). Derived from Qwen 2.5 1.5B. A bit slower 
and RAM-hungry but I&#x27;d say not bad at all. Both pass the strawberry test but 
not the Luhn checksum coding test. Well... sometimes Athena does the exact 
opposite. &quot;AwA&quot; stands for &quot;Answers with Athena&quot; and is just as slow, but 
I&#x27;m not sure whether these two are actually related.
5. Triangulum 1B (Triangulum-1B.Q8_0.gguf, 1.5G, max context size 131072).
Something independent but clearly derived from Llama 3.2, although a bit 
slower as it is tailored to natural language processing and translation, so 
it nailed the strawberry question and almost nailed 23 * 143 (the 
decomposition part was right but the final 2300 + 920 + 69 addition somehow 
ended up being 2999, lol) but didn&#x27;t produce any Python code for Luhn and 
got the algo completely wrong. One &quot;feature&quot; that sets this model apart is 
that it really likes to dilute the answers up to the point of 
self-repetition, so be wary of that.
6. SmolLM2 360M (smollm2-360m-instruct-q8_0.gguf, 369M, max context size
8192). Now, this is something really impressive. And again, from the 
independent and academic background. Yes, it can&#x27;t into 23 * 143 (although 
it&#x27;s just off by 10, giving 3299), but it nails the strawberry question. It 
even generates half-decent Luhn checksum code, one that works correctly in 
exactly half the cases because it doesn&#x27;t invert the digit order, with the 
comments that are also half-correct, but I&#x27;m still stunned. For this size, 
its peers don&#x27;t even generate valid Python at all most of the time. Not to 
mention how blazingly fast it runs on any of my ARM64 devices. Of course, it 
can sometimes run into a loop and stuff, but... With this kind of 
performance of just a 360M model, it&#x27;s scary to even imagine what the 1.7B 
variant is capable of...
7. SmolLM2 1.7B (SmolLM2-1.7B-Instruct.Q8_0.gguf, 1.7G, max context size
8192). So, I found this one on the QuantFactory repo and tried it out. I 
don&#x27;t get how it managed to botch the strawberry question, insisting on the 
wrong answer even though the smaller variant got it right, but produced 
perfect Luhn checksum code at the same time. Of course it couldn&#x27;t answer 23 
* 143, but that&#x27;s something I&#x27;m not surprised about at this point. It also 
isn&#x27;t as sensitive as Llama when it comes to adapting stories (the end 
result might need some further rewriting). But it definitely is much faster 
than e.g. Gemma 2 2B and is a pleasure to use even on my weak Asus.
8. OpenCoder 1.5B (OpenCoder-1.5B-Instruct.Q8_0.gguf, 1.9G, max context size
4096). This is a strange one. Looks independent (although all of its authors 
are Chinese). Totally botches the strawberry question, also the only one on 
the list that honestly answers that it cannot calculate 23 * 143, but as for 
the Luhn question... Well, the code looks correct but no one alive would use 
that approach. The nature of that code also hints at some relation to Qwen 
2.5. Maybe there&#x27;s no relation and Qwen just was trained on the same Python 
data, who knows. I&#x27;ll investigate this one more before jumping to any 
conclusions.
9. xLAM 1B-fc-r (xLAM-1b-fc-r.Q8_0.gguf, 1.4G, max context size 16384). An
interesting model for sure. Somewhat resembles OpenCoder but much less 
strange. Knows the answer to the strawberry question, gives a relatively 
sane Luhn code, completely misses 23 * 143 and cannot write stories. Why? 
Because it&#x27;s optimized for function/tool calling, something that I&#x27;m yet 
unable to test with llamafile alone. Nevertheless, I think it&#x27;s a worthy 
model to include here.
10. Llama-Deepsync 1B (Llama-Deepsync-1B.Q8_0.gguf, 1.3G, max context size
131072). Derived from the Llama 3.2 1B Instruct variant, nails Luhn 
immediately, somehow misses the strawberry question for the first time but 
corrects itself when asked to think again. On the 23 * 143 problem, it 
showed the reasoning but just couldn&#x27;t do the last step (3220 + 69) 
correctly, producing 3299, 3249 etc and even insisted on this answer. Like, 
WTF? It also couldn&#x27;t complete my crime story task. But overall, I like this 
one too.

I genuinely had looked for more plausible examples but, surprisingly, the
majority of them didn&#x27;t pass my basic criteria to be usable in day-to-day 
life on weak hardware. So, as of the current date and time, here are my 
conclusions about the available small language models:

1. There are three clear winners at the present moment: Llama 3.2, Qwen 2.5
and SmolLM2. Their &lt;2B versions and derivatives (like Deepsync, Triangulum, 
Athena, AwA etc) perform the best on my weak hardware.
2. If you want a model that&#x27;s as close as possible to the &quot;one-size-fits-all&quot;
option, look no further than the Llama 3.2 1B (either official or 
uncensored). In some areas, it really outperforms even some 1.5B models 
while consuming much less computational resources (and those who don&#x27;t care 
about resources are extremely unlikely to even find this phlog). Just set 
realistic expectations and don&#x27;t demand things that it really can&#x27;t do 
because of its size. 
3. If you just want to have a model as small as possible and as fast as
possible with little compromise on the output quality, then Qwen2.5 0.5B 
(qwen2.5-0.5b-instruct-q8_0.gguf from the official repo, 645M, max context 
length 32768) is still an option that&#x27;s fun to play with. Just be aware that 
it doesn&#x27;t know how many &quot;r&quot; letters are in the word &quot;strawberry&quot;. However, 
there also is an uncensored version (dolphin3.0-qwen2.5-0.5b-q8_0.gguf, 
507M, max context length 32768) that DOES know the correct answer to this 
question, although it still cannot write the correct Luhn checksum code in 
Python or even the algorithm&#x27;s description (which is pretty close but omits 
crucial details) and is pretty bad at math overall. Athena and AwA also have 
corresponding 0.5B versions that perform on par with the vanilla Qwen, with 
Athena 0.5B being a bit faster than AwA and actually having about the same 
size as the &quot;dolphined&quot; Qwen2.5 0.5B.
4. Finally, if you need something even smaller and faster but still as
capable, just use the SmolLM2 360M. You won&#x27;t be disappointed for sure.

To distill this even further, your llamafile binary just needs one of these
files to get you started on low-powered hardware: 
Llama-3.2-1B-Instruct.Q8_0.gguf (or any of its uncensored versions), 
SmolLM2-1.7B-Instruct.Q8_0.gguf, dolphin3.0-qwen2.5-0.5b-q8_0.gguf or 
smollm2-360m-instruct-q8_0.gguf. I&#x27;m also keeping tabs on the NVidia&#x27;s Hymba 
1.5B, but no GGUF&#x27;ed versions of it have surfaced so far. All I know is that 
it already is somewhere in the Quant Factory&#x27;s queue of requests. I also 
tried quantizing it myself using the gguf-my-repo space ([4], requires a 
Hugging Face account) but it doesn&#x27;t look like even being supported by 
llama.cpp yet.

So, now that we know what to run and how to run it, the main question
remains: what can we really do with it?

Well, again, if you set the right expectations, we can do quite a lot,
especially when it comes to some boring tasks that involve the very things 
these models were designed for in the first place: text generation and 
analysis. Obviously, the latter is much more resource-heavy than the former, 
so the idea of using small and local language models on low-performance 
hardware mostly shines in the &quot;short prompt, long response&quot; scenario. 
Unsurprisingly, this is what now most normies are using the (in)famous 
ChatGPT for these days: &quot;write me an email to my boss&quot;, &quot;write me a landing 
page about my new cryptocurrency&quot;, &quot;suggest an idea for the next video&quot;, 
&quot;convert a structure to the SQL table&quot; and so on. Newsflash: this is the 
exact kind of tasks that totally can be handled by Llama 3.2 1B, Granite 
3.1-MoE 1B, SmolLM2 1.7B or (in some cases) even Qwen2.5 0.5B/SmolLM2 360M 
completely for free and offline, without paying for thin air, putting your 
privacy at risk and giving your personal data to sketchy CEOs who murder 
their own employees to stay afloat. And you don&#x27;t even need **any** GUI to 
do this, running llamafile in a bare terminal (e.g. even Termux on Android, 
which is what I prefer, btw, I have some ideas about how to integrate all 
this into my upcoming Android-based magnum opus) or a remote machine you SSH 
into. And I haven&#x27;t even touched the entire &quot;function/tool calling&quot; aspect 
because it requires running these models from custom code with an agent 
framework, not in a raw llamafile chat interface.

The bottom line is, with this tool and these models, you&#x27;re back in control
as a user. And now you at least know how to stop using yet another 
proprietary pile of BS if all you need can be achieved locally and with low 
resource consumption. I&#x27;m not sure whether I do another post about LLMs or 
not – maybe about writing structured prompts, switching to bare llama.cpp, 
tweaking parameters for the models to respond differently, maybe about some 
open-source STT and TTS tools available for mere mortals, maybe about agents 
and tool calling from Python code, maybe about the Hymba 1.5B or something 
else when/if it appears in the GGUF format and impresses me enough to talk 
about it – but I think this is where we should draw the line. I mean, 2B 
parameters are currently the threshold beyond which it just becomes 
unsustainable and &quot;a thing in itself&quot; that requires you to upgrade your 
hardware just for the sake of using these things with any degree of comfort. 
And being dependent upon the hardware that you must constantly upgrade &quot;just 
because&quot; is, in my opinion, not much better than being dependent upon 
subscription-based online services. Not to mention that, in this case, we&#x27;re 
talking about the hardware that inevitably will consume more energy to run 
these LLMs at 100% processing capacity. 

Ethical concerns are another thing to consider. By using smaller open models
offline, you inherently reduce not only the overall energy consumption but 
also the amount of: 1) traffic sent to potentially bad actors from your 
devices, 2) money sent to those potentially bad actors, 3) online slop 
polluting the clearweb for all recent years, 4) fear of diminishing your own 
cognitive or creative abilities. After all, you want an assistant, not 
something that fully thinks for you. No matter what you believe in, don&#x27;t 
let the exoskeleton take control over your body. Tech for people, not people 
for tech.

--- Luxferre ---

[1]: https://github.com/Mozilla-Ocho/llamafile
[2]:
https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md
[3]: https://huggingface.co/
[4]: https://huggingface.co/spaces/ggml-org/gguf-my-repo
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2025-01-20-theory-vs-practice-again.txt</link>
<title>(2025-01-20) Theory vs. practice, again</title>
<description><![CDATA[<pre>
(2025-01-20) Theory vs. practice, again
---------------------------------------
Today, I&#x27;m not gonna talk about anything specific in any depth, just some
random and short musings united with a common idea, rooted back in the 
famous quote by Linus Torvalds: &quot;Theory and practice sometimes clash. And 
when that happens, theory loses. Every single time.&quot;

In theory, Forth and OCaml programming languages are much superior to Go and
Rust in every possible aspect, but in practice, most natively compiled 
software is written in the latter ones nowadays. Even I, personally, would 
prefer Go over other mainstream BS like Java or C++. By the way, in theory, 
compiled languages are the go-to choice for almost any problem, but in 
practice, interpreted (or at most JIT-compiled) languages are being used 
much more on a daily basis.

In theory, Tcl/Tk is much easier to learn and get started with real everyday
desktop usage than Python, but in practice, Python has embraced literally 
all areas of scripting while Tcl still remains in its niche. You want 
desktop? Got Tkinter (ironically Tcl-based). You want peripherals? Got 
pyaudio, pyserial and pyusb. You want client/server? Got tons of libraries. 
You want AI? Got Pytorch, Tensorflow, Ollama, Langchain, Pydantic and other 
integrations. I&#x27;m having a hard time now if I have to explain why it&#x27;s 
better to learn Tcl, when Python already has you covered everywhere, even on 
MCUs like ESP8266EX.

In theory, Web should be only used for displaying static and, to some extent,
dynamic content, but in practice, Web has become another full-fledged 
platform to run applications, and this fact has become so obvious that no 
one can ignore it anymore. The quality and the level of user&#x27;s control over 
these applications is another thing.

In theory, there are plenty of mobile Linux distributions like postmarketOS
that are better than Android in every way, but in practice, it makes much 
more sense for people to extend their Androids to achieve advanced Linux 
functionality (e.g. with Termux or custom builds) than fully ditch the 
ecosystem they got used to. In theory, &quot;FOSS smartphones&quot; would imply full 
control over their components, but in practice, their evangelists go apeshit 
with a simple question: &quot;Does it even allow IMEI editing?&quot;

In theory, if you pick up any non-Apple laptop except the cheapest ones,
provided that you install a normal OS instead of Faildows, you will get a 
twice+ better performance per price ratio than any MacBook (not even to 
mention their serviceability and upgradeability). In practice though, 90% of 
laptops still are utter crap regardless of their price, and it is very hard 
to find a decent alternative. I, for instance, have settled upon a ThinkPad 
L14 Gen5, but just because there wasn&#x27;t any real competition for that price.

The most interesting twist is, you have to know the theory well enough to
determine whether the practice is good or bad. The examples I&#x27;ve mentioned 
are not as bad as most others. We just have to admit that we live in a world 
of bad practices full of clueless people who don&#x27;t even know how bad those 
practices are because they didn&#x27;t even bother to learn any theory. Can 
anything be done about it? If you answer this question for yourselves, 
you&#x27;ll understand why this phlog even exists.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2025-01-27-things-no-one-is-focused-on-genai.txt</link>
<title>(2025-01-27) Some things no one is focused on regarding genAI</title>
<description><![CDATA[<pre>
(2025-01-27) Some things no one is focused on regarding genAI
-------------------------------------------------------------
Again, no in-depth analysis this time but rather some observations. First, it
really looks like all the craziness going on in the world right now is 
merely a distraction from something bigger. GenAI is no exception. Everyone 
is taking about the new grift called &quot;Project Stargate&quot;, about DeepSeek vs. 
OpenAI, about whether or not AGI/ASI is reachable and what it even is, about 
agents/superagents/duperagents... But in fact, all that seems to be nothing 
more than an information shroud for the people to turn off their critical 
thinking and not look at the real state of things as of today. And the real 
state of things is, everything is coming to the repetition of 100-year-old 
history, only now the totalitarian governments (who, of course, will never 
openly admit they are totalitarian) will have a lot more technical 
capabilities to pursue their goals of mass surveillance and control. This, 
by the way, is why they need such huge datacenters. AI is, and always has 
been, just a facade.

Second, I&#x27;ll never get tired of repeating the only criterion for determining
the usefulness of any piece of technology, the only question that you should 
ask: do you really know what you&#x27;re running and do you have enough control 
over it? Anything closed-source is a trojan by default, and (on paper) they 
even made it illegal for anyone to prove otherwise. Again, GenAI is no 
exception. I don&#x27;t share the excitement of DeepSeek fanboys for it being so 
cheap, unless they have enough computational resources to run it fully 
locally (which is not cheap at all). I have run some distilled variants of 
R1 locally and they left me pretty much impressed, but I didn&#x27;t sign up at 
their official website to test the 671B model, it never is an option for me. 
In terms of privacy and security, cloud-based DeepSeek is no better than 
ChatGPT, so no one should sign up for either of them. But again, even 
open-weight doesn&#x27;t equal open-source. I&#x27;m running open-weight models 
locally because they are sandboxed enough to do no harm, but I never forget 
that I&#x27;m interacting with a black box and should treat its output 
accordingly.

Third, the question that bothers most tech people right now, like &quot;Will genAI
replace software engineers?&quot;, is not a question for me either. I may have 
already mentioned this in an earlier post, but... Technically, AI can&#x27;t 
replace software engineers. Idiotic management can. It&#x27;s already happening 
to less &quot;brainy&quot; positions like copywriters or frontenders, and there is a 
practice of &quot;soft displacement&quot; of SDEs as well: companies started including 
mandatory ChatGPT subscription into the work account package, and LLM 
prompting started appearing in the CVs and job requirements. Which is 
already insane enough, if you ask me. For them, it no longer matters how 
good you are at programming, now it matters how good you are at asking genAI 
to do something for you. The repercussions of this approach are not so long 
to follow. Just imagine a project with a large codebase where no one 
understands anything because it all had been autogenerated, and someone has 
to fix a security issue or other bug that could be obviously avoided if the 
code had been written in a normal way. Ironically, with the recent 
advancement of reasoning models like DeepSeek&#x27;s R1, it would make much more 
sense for genAI to replace project managers instead of developers. Of 
course, productivity was never the true goal of such &quot;initiatives&quot;, so the 
latter scenario is rather unrealistic.

Lastly, a chat interface in the _natural_ language is one of the most
inefficient ways to do things when interacting with machines. It&#x27;s much 
easier for me to type (and for the machine to understand) &quot;ls ~&quot; than &quot;give 
me the list of files in the home directory&quot;. Even if you hide everything 
behind &quot;agents&quot; and their pipelines, you still have to interact with LLMs by 
giving them prompts and reading results. You know, programming/scripting 
languages were invented for a reason. There always has been a search for a 
balance between &quot;what is the easiest for the computer to understand&quot; and 
&quot;what is the easiest for a human to understand&quot;. Making computers understand 
humans in their own language will never give precise results no matter how 
much computing power you throw at it, just because human language is 
imprecise by its nature. If anything, there is going to be a point where 
making LLMs function closer to the human brain will actually decrease their 
performance. Because no human follows a perfect pattern of reasoning either. 
And this is normal. This is what, among other things, makes us humans. An 
open question is, however, how much more resources will be wasted until this 
threshold is reached and will the &quot;stakeholders&quot; ever admit that it has been 
reached in the first place?

Nevertheless, as I have been reassured once again, artificial intelligence is
nowadays a much lesser threat than natural stupidity. This is what the next 
generation of John Connors will have to resist first.

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2025-02-03-a-message-to-copycats.txt</link>
<title>(2025-02-03) A message to copycats</title>
<description><![CDATA[<pre>
(2025-02-03) A message to copycats
----------------------------------
I didn&#x27;t even suppose that I&#x27;d have to write anything like this at any point
in time. Yet here we are. This is a post for a specific parts of my 
audience, as that part recently got surprisingly large.

As you know pretty well by now, I am a strong proponent of open source and
*absolute* software freedom. That&#x27;s why I usually release my own code into 
public domain. That&#x27;s why I totally do not object to usage of my ideas in 
the others&#x27; projects. However, you know, sometimes it really hurts to see 
how the projects employing those ideas are... mediocre at best. As soon as I 
abandoned KaiOS-related development, I hoped that practice would cease. Alas.

For instance, tell me please, what kind of nonsense is ([1]) this? The author
even mentions several posts of my phlog as the reference points. He 
introduces the final step that I haven&#x27;t even published yet (although it had 
been found back in the late December, and I will prove this when my own tool 
repo goes public on Mar 1 2025). Yet the logic he uses in the explanation 
(&quot;Technicalities&quot; section) is not completely wrong but contains several 
redundant steps. On top of that, he states that the method &quot;will not work if 
you want to change the phone&#x27;s imei number to something other than what it 
had originally&quot;, which is not true. In fact, the step that I had been 
missing but he added makes it work on any IMEI combination (without it, as 
you remember, both IMEIs were zeroed out, and there only was a way to bypass 
this on the 6/6 Pro). You&#x27;ll see it for yourselves when my tool is 
published. As such, he has written twice more code than necessary, and 
presented all this in the most uncomfortable way to use.

But this is not even the most egregious example. At least it&#x27;s open-source
and honest, mentioning all credits below. There was, however, some other guy 
who created a Faildows-only Tkinter-based application in Python that 
couldn&#x27;t even work without a ton of external components in the same folder, 
and a part of the logic was baked into this application but a part of it was 
inside an external script. Well, I managed to decompile it. It&#x27;s a total 
mess. Even more extra steps, even more confusion, and the level of GUI is 
like... a third-grade student could create a better one. I wouldn&#x27;t be 
surprised if the GUI part was AI-generated. And the author decided to remain 
completely anonymous (maybe to avoid public humiliation). The only thing I 
found as the hint to his origin was the docstrings. Which, in case you 
didn&#x27;t know, remain intact in the compiled .pyc bytecode files, unlike the 
&quot;normal&quot; single-line Python comments. This, AFAIK, was a deliberate design 
decision for Pydoc to be able to generate documentation from compiled 
modules as well. So, I saw those docstrings and they were written in 
Turkish. I&#x27;m well aware that Turkey has a big problem with imported 
cellphones, but come on, you don&#x27;t have to be that sloppy.

The main question is: what drives all those people? If they don&#x27;t seem to
have a basic understanding of what&#x27;s going on and just paste whatever they 
found into their haphazardly put together utilities, then why publish them 
at all? If you require root access anyway, might as well make a decent 
autonomous tool to use on the device itself, or at least something that&#x27;s 
pleasant to use and optimized on a fundamental level. It&#x27;s as if there is no 
mid-space between the amateur script kiddies and &quot;flashing box&quot; thin air 
sellers. No, unlike the &quot;boxers&quot;, I respect every effort to spread the 
knowledge, but this knowledge has to be based. If you don&#x27;t understand what 
you&#x27;re doing, then you&#x27;re not really helping anyone in the long run. What 
will you do when/if they change the baseband in Pixel 10? Yeah, you&#x27;ll wait 
for someone to research it again.

I had started all this in an attempt to prove that the GrapheneOS devs were
totally wrong, and now I have everything at my disposal to prove it. 
Whatever y&#x27;all do with the results of this proof is none of my concern. The 
spring is coming. Mark your calendars.

--- Luxferre ---

[1]: https://github.com/bitdomo/restore_imei
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2025-02-11-first-break-in-the-year.txt</link>
<title>(2025-02-11) First break in the year</title>
<description><![CDATA[<pre>
(2025-02-11) First break in the year
------------------------------------
Being a bit exhausted, I&#x27;ve decided to not post anything for the rest of the
month. I definitely will return with some interesting materials on March 3 
because, as I said, the spring is coming. For now, I need to survive and 
recover. Hope y&#x27;all understand. See you soon!

--- Luxferre ---
</pre>]]></description>
</item>

<item>
<link>gopher://hoi.st/1/posts/2025-03-03-the-graphene-saga-part-3.txt</link>
<title>(2025-03-03) The Graphene Saga: part 3</title>
<description><![CDATA[<pre>
(2025-03-03) The Graphene Saga: part 3
--------------------------------------
The spring has come, and guess who&#x27;s back. Guess **what** is back almost a
year since the last time. I had to have a lot of patience to wait until this 
moment, and it&#x27;s not even the biggest thing to come yet. Right now, as far 
as I&#x27;m concerned, IMEI editing in Google Pixels from 6 to 9 Pro is a fully 
solved thing. Of course, you have to have it rooted but I don&#x27;t deal with 
carrier-locked devices in general, there are other people who are more 
interested in that. Regardless, there now is a totally irrefutable proof 
that GrapheneOS developers were absolutely wrong in their statements about 
IMEI editing in Pixels and in general. I don&#x27;t know why they said that in 
the first place, either because of their own stupidity or due to the fear of 
being threatened a legal action, but the proof is out there now, and in my 
case, it even is shaped into a fully autonomous FOSS tool that runs on any 
rooted Pixels themselves starting from the model 6 and above.

I&#x27;ve called it lexipwn ([1]). It&#x27;s a portmanteu of the reversed word &quot;pixel&quot;
and &quot;pwn&quot;. It consists of two parts, both written in Go: lexipwn-cli and 
lexipwn-gui. You can use lexipwn-cli independently in the console like 
Termux or ADB (as long as you have root access), but it also gets built 
inside the lexipwn-gui APK file and is unpacked and called as root by the 
GUI part when you perform any saving or loading action. The GUI part is 
handled by the Fyne ([2]) framework which might not be an ideal choice for 
everyone but is the easiest way to get started with Android GUI without 
having to write a single line of Java or XML. Also, since all targets are 
Pixels that share the same architecture, we can easily instruct Fyne to only 
build the application for &quot;android/arm64&quot; and save a ton of space by 
eliminating unused binary code.

So, as you can see, lexipwn offers a lot of fields to edit, and also eases
the IMEI and MAC address randomization. Of course, everything is done via 
editing the devinfo partition, which had been covered before. But what&#x27;s 
different this time and why does the IMEI editing finally work as intended? 
Well, let&#x27;s recap where the research had stopped a year ago. Quoting my last 
post on the topic directly:

&gt; The main subject, as you might have seen in LuxDocs, is now stalled at the
stage of finding where the IMEI SHA checksums are stored. Because the IMEIs 
themselves are stored in the devinfo partition in the plain ASCII form 
(although the partition itself is binary), and this partition, contrary to 
my expectations, really controls everything over the EFS. Of course, if 
either IMEI doesn&#x27;t match its checksum, the device reports both of them as 
000000000000000 to both the OS userspace and the network. And I could 
partially do this search in the offline mode as I dumped the modem firmware 
image along with everything EFS-related while I still had the root access. 
But, of course, I should have dumped everything I could.

And then yes, I had to move to an unrooted Graphene on my Pixel 6 and have
been using it as my main Android ever since, having no way to continue this 
research. The truth is, I had no idea how close to the solution I had been 
for all this time. But then, in December 2024, a guy who was interested in 
my research contacted me. As a result, after a short exchange of ideas, I 
bought myself a Pixel 7a, rooted it and resumed digging. Yes, I had been 
right about the place where the checksums were stored, the 
/mnt/vendor/persist/modem/cpsha file, but how was its content generated? 
Well, the algorithm still is a black box inside the modem firmware but it 
can be triggered with an external command, and I had also seen that command 
before: AT+GOOGGETIMEISHA. It just didn&#x27;t work for me. Why? And that&#x27;s when 
that guy gave me a tip that unraveled everything else: for this AT command 
to work, the phone needs to be booted into the factory mode.

What&#x27;s factory mode, you may ask? Well, that&#x27;s essentially a value for
another field in the devinfo partition, &quot;bootmode&quot;. I don&#x27;t know other 
values for this field besides &quot;normal&quot; and &quot;factory&quot;, and these values can 
also be set via the &quot;fastboot oem set_config bootmode&quot; command. When editing 
the field manually and not via fastboot, you also have to change its field 
type to DIUS if it was initially set to DIFR (lexipwn-gui does this 
automatically btw). Anyway, when you set this field to &quot;factory&quot; and reboot, 
it indicates that the phone has been booted into the factory mode by showing 
a big red word &quot;Factory&quot; instead of the very first boot logo, and I&#x27;m not 
sure whether it affects anything else but it unlocks certain modem 
interactions including the AT+GOOGGETIMEISHA command, which now returns the 
hexadecimal value to be replaced in the /mnt/vendor/persist/modem/cpsha 
file, which is calculated from the current devinfo fields. So, again, the 
devinfo partition is the primary source of truth for EFS here, not vice 
versa. And this is where everything worked: after fixing the CPSHA value, 
regardless of whether or not the &quot;bootmode&quot; field is changed backed to 
&quot;normal&quot;, the new IMEI numbers are shown both to the device and the network 
after rebooting.

This was a real breakthrough, but I&#x27;m not going to stop there. Since I have a
performant enough hardware to build Android locally now, I am planning on 
creating a custom rooted (or at least userdebug) GrapheneOS build with 
lexipwn, a fingerprint spoofer and several other privacy-protecting tools 
preinstalled. If going the userdebug route, I&#x27;ll also have to change the way 
lexipwn-cli is called inside the GUI part, but that is the least of my 
worries for now. The primary testbed is still going to be the Pixel 7a, but 
I&#x27;m pretty sure similar builds will work on every device from the 
Tensor/Exynos family. This is pretty much going to be my magnum opus of the 
year, unless, of course, GrapheneOS devs themselves realize their past 
mistakes, embrace the truth and finally incorporate IMEI, MAC, other devinfo 
and fingerprint editing into their mainline builds.

--- Luxferre ---

[1]: https://codeberg.org/luxferre/lexipwn
[2]: https://fyne.io
</pre>]]></description>
</item>
</channel></rss>
